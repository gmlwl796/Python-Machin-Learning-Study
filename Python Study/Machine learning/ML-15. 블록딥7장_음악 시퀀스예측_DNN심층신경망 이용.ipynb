{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'color'>블록 딥러닝: 7장.음악시퀀스 예측_심층신경망(DNN) 이용</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>문제 정의</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나비야 노래음표 데이터셋(Naviya Musical Notes Dataset)\n",
    "* 다중분류(multi-class classification) 문제\n",
    "* 노래를 구성하고 있는 음표 구성 특성에 따라 이어지는 노래 음표시퀀스를 분류예측하는 문제\n",
    "* 기존 로이터뉴스 예제는 텍스트 시퀀스 데이터를 분석해서 최종적인 뉴스기사의 주제를 분류예측을 해보는 문제였음: n to 1 예측문제\n",
    "* 이번 문제는 음표 시퀀스 데이터를 분석해서 일정 윈도우 길이마다 시퀀스로 분류예측을 하는 문제임: n to n 예측문제\n",
    "<img src = './../../images/naviya.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 피처(입력/예측/독립)변수\n",
    "    * 한 편의 노래를 구성하고 있는 음표코드별 인덱스번호를 4개 단위로 만들어 순서대로 나열한 데이터 <pre>\n",
    "1. 타깃(출력/반응/종속)변수\n",
    "    * 4개 음표 인덱스 번호가 입력되었을 때 예측될 음표 인덱스 번호\n",
    "<img src = './../../images/naviya_prediction.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4개씩 학습한 다음에 5번째 데이터를 target변수(= y값)로 둬서 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>데이터 준비</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화 모듈로딩 및 한글폰트 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 작업을 위한 모듈 로딩작업\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malgun Gothic\n",
      "NanumGothic\n"
     ]
    }
   ],
   "source": [
    "# 사용을 원하는 한글폰트에 대한 공식이름 확인\n",
    "font_name = font_manager.FontProperties(fname=\"./fonts/malgun.ttf\").get_name()\n",
    "print(font_name)\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"./fonts/NanumGothic.ttf\").get_name()\n",
    "print(font_name)\n",
    "\n",
    "# 해당 한글폰트를 파이선 그래프에서 사용가능하도록 설정\n",
    "plt.rcParams[\"font.family\"] = font_name\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워닝메시지 출력관련 모듈로딩\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "# - 출력되는 경고메시지를 무시하고 숨김\n",
    "\n",
    "# warnings.filterwarnings(action = 'default')\n",
    "# - 다시 경고메시지가 나타나도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 범용라이브러리 로딩\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>데이터 준비</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석대상 음표 raw 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'd8', 'e8', 'f8', 'g8', 'g8', 'g4', 'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4', 'd8', 'd8', 'd8', 'd8', 'd8', 'e8', 'f4', 'e8', 'e8', 'e8', 'e8', 'e8', 'f8', 'g4', 'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4']\n"
     ]
    }
   ],
   "source": [
    "seq = [\"g8\", \"e8\", \"e4\", \"f8\", \"d8\", \"d4\", \"c8\", \"d8\", \"e8\", \"f8\", \"g8\", \"g8\", \"g4\", \n",
    "       \"g8\", \"e8\", \"e8\", \"e8\", \"f8\", \"d8\", \"d4\", \"c8\", \"e8\", \"g8\", \"g8\", \"e8\", \"e8\", \"e4\", \n",
    "       \"d8\", \"d8\", \"d8\", \"d8\", \"d8\", \"e8\", \"f4\", \"e8\", \"e8\", \"e8\", \"e8\", \"e8\", \"f8\", \"g4\", \n",
    "       \"g8\", \"e8\", \"e4\", \"f8\", \"d8\", \"d4\", \"c8\", \"e8\", \"g8\", \"g8\", \"e8\", \"e8\", \"e4\"] \n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음표사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c4': 0,\n",
       " 'd4': 1,\n",
       " 'e4': 2,\n",
       " 'f4': 3,\n",
       " 'g4': 4,\n",
       " 'a4': 5,\n",
       " 'b4': 6,\n",
       " 'c8': 7,\n",
       " 'd8': 8,\n",
       " 'e8': 9,\n",
       " 'f8': 10,\n",
       " 'g8': 11,\n",
       " 'a8': 12,\n",
       " 'b8': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음표코드to음표번호 사전(code2idx)\n",
    "code2idx = {'c4':0, 'd4':1, 'e4':2, 'f4':3, 'g4':4, 'a4':5, 'b4':6, \n",
    "            'c8':7, 'd8':8, 'e8':9, 'f8':10, 'g8':11, 'a8':12, 'b8':13}\n",
    "code2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'c4',\n",
       " 1: 'd4',\n",
       " 2: 'e4',\n",
       " 3: 'f4',\n",
       " 4: 'g4',\n",
       " 5: 'a4',\n",
       " 6: 'b4',\n",
       " 7: 'c8',\n",
       " 8: 'd8',\n",
       " 9: 'e8',\n",
       " 10: 'f8',\n",
       " 11: 'g8',\n",
       " 12: 'a8',\n",
       " 13: 'b8'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음표번호to음표코드 사전(idx2code) -> 디코딩 목적(컴퓨터의 숫자를 우리가 알아볼 수 있도록)\n",
    "# 하나하나 입력하기보다는 dictionary reversing 을 하면됨.  for문 돌려서 key를 value에 value를 key에 넣어주면 됨\n",
    "idx2code = {0:'c4', 1:'d4', 2:'e4', 3:'f4', 4:'g4', 5:'a4', 6:'b4', \n",
    "            7:'c8', 8:'d8', 9:'e8', 10:'f8', 11:'g8', 12:'a8', 13:'b8'}\n",
    "idx2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 9, 2, 10, 8, 1, 7, 8, 9, 10, 11, 11, 4, 11, 9, 9, 9, 10, 8, 1, 7, 9, 11, 11, 9, 9, 2, 8, 8, 8, 8, 8, 9, 3, 9, 9, 9, 9, 9, 10, 4, 11, 9, 2, 10, 8, 1, 7, 9, 11, 11, 9, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "# 개별 뉴스의 단어인덱스번호를 이용한 실제 단어명 구성내용 확인\n",
    "# - for 구문에 print() 함수이용\n",
    "# - 딕셔너리.get(key값) => values값이 나옴\n",
    "\n",
    "seq_num = []\n",
    "\n",
    "for i in seq:\n",
    "    seq_num.append(code2idx.get(i)) # 음표기호를 숫자기호로 변형되게 seq_num리스트에 하나씩 추가\n",
    "\n",
    "print(seq_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N2N예측 입력도 다중, 출력도 다중\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원하는 음표길이(갯수)만큼 시퀀스 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표시퀀스 전체데이터를 4개 음표단위로 분할해 시퀀스 데이터셋을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음표시퀀스 전체 데이터 길이: 54\n",
      "음표시퀀스 추출 윈도우 길이: 4\n"
     ]
    }
   ],
   "source": [
    "print('음표시퀀스 전체 데이터 길이:', len(seq))\n",
    "\n",
    "window_size = 4\n",
    "print('음표시퀀스 추출 윈도우 길이:', window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4개 윈도우를 슬라이드 기법으로 음표를 옮기는 것. (자르는게 아님) <br>\n",
    "1,2,3,4 -> 5 예측 <br>\n",
    "2,3,4,5 -> 6 예측 <br>\n",
    "3,4,5,6 -> 7 예측 이런식으로 음표를 한칸씩 옮겨가면서 촘촘히 예측함 <br>\n",
    "4개 데이터는 feature set, 5번째 데이터는 target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음표시퀀스 데이터셋 저장용 리스트객체 생성 \n",
    "dataset = []\n",
    "\n",
    "# for반복구문을 이용한 시퀀스 데이터셋을 분리추출\n",
    "for i in range(len(seq) - window_size): # 5부터 54까지 50번의 for문을 돌겠지\n",
    "    \n",
    "    subset = seq[i:(i + window_size + 1)]\n",
    "    # - 전체 음표시퀀스 데이터에서 원하는 윈도우 길이 + 1 만큼 부분추출\n",
    "    # - 윈도우 길이만큼의 음표로 학습을 하고, 이어진 + 1에 해당하는 음표를 분류예측하는 학습을 진행하게 됨\n",
    "    \n",
    "    dataset.append([code2idx[item] for item in subset])\n",
    "    # - 음표코드2인덱스번호 사전을 이용해 분리추출한 \n",
    "    #   음표 하나하나를 인덱스번호로 변환해 데이터셋을 만듬\n",
    "    \n",
    "dataset = np.asarray(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음표시퀀스 데이터셋 객체유형: <class 'numpy.ndarray'>\n",
      "음표시퀀스 데이터셋 객체구조: (50, 5)\n",
      "\n",
      "음표시퀀스 데이터셋 내용:\n",
      " [[11  9  2 10  8]\n",
      " [ 9  2 10  8  1]\n",
      " [ 2 10  8  1  7]\n",
      " [10  8  1  7  8]\n",
      " [ 8  1  7  8  9]\n",
      " [ 1  7  8  9 10]\n",
      " [ 7  8  9 10 11]\n",
      " [ 8  9 10 11 11]\n",
      " [ 9 10 11 11  4]\n",
      " [10 11 11  4 11]\n",
      " [11 11  4 11  9]\n",
      " [11  4 11  9  9]\n",
      " [ 4 11  9  9  9]\n",
      " [11  9  9  9 10]\n",
      " [ 9  9  9 10  8]\n",
      " [ 9  9 10  8  1]\n",
      " [ 9 10  8  1  7]\n",
      " [10  8  1  7  9]\n",
      " [ 8  1  7  9 11]\n",
      " [ 1  7  9 11 11]\n",
      " [ 7  9 11 11  9]\n",
      " [ 9 11 11  9  9]\n",
      " [11 11  9  9  2]\n",
      " [11  9  9  2  8]\n",
      " [ 9  9  2  8  8]\n",
      " [ 9  2  8  8  8]\n",
      " [ 2  8  8  8  8]\n",
      " [ 8  8  8  8  8]\n",
      " [ 8  8  8  8  9]\n",
      " [ 8  8  8  9  3]\n",
      " [ 8  8  9  3  9]\n",
      " [ 8  9  3  9  9]\n",
      " [ 9  3  9  9  9]\n",
      " [ 3  9  9  9  9]\n",
      " [ 9  9  9  9  9]\n",
      " [ 9  9  9  9 10]\n",
      " [ 9  9  9 10  4]\n",
      " [ 9  9 10  4 11]\n",
      " [ 9 10  4 11  9]\n",
      " [10  4 11  9  2]\n",
      " [ 4 11  9  2 10]\n",
      " [11  9  2 10  8]\n",
      " [ 9  2 10  8  1]\n",
      " [ 2 10  8  1  7]\n",
      " [10  8  1  7  9]\n",
      " [ 8  1  7  9 11]\n",
      " [ 1  7  9 11 11]\n",
      " [ 7  9 11 11  9]\n",
      " [ 9 11 11  9  9]\n",
      " [11 11  9  9  2]]\n"
     ]
    }
   ],
   "source": [
    "# 윈도우 길이만큼 분리 추출된 음표시퀀스 데이터셋 객체유형과 구조\n",
    "print('음표시퀀스 데이터셋 객체유형:', type(dataset))\n",
    "print('음표시퀀스 데이터셋 객체구조:', dataset.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# 윈도우 길이만큼 분리추출된 음표 데이터셋\n",
    "print('음표시퀀스 데이터셋 내용:\\n', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 데이터를 잘 보면, <br>\n",
    "[1,2,3,4,5] <br> \n",
    "[2,3,4,5,6] <br>\n",
    "[3,4,5,6,7] <br>\n",
    "[4,5,6,7,8] <br>\n",
    "\n",
    "이런식으로 음표를 한칸씩 옮겨가면서 데이터셋을 저장해놓은 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표시퀀스 데이터셋 생성을 위한 사용자정의함수 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 윈도우 길이만큼 부분추출 음표 데이터셋을 만들어 주는 사용자정의함수 생성\n",
    "def seq2dataset(seq, window_size):\n",
    "\n",
    "    # 음표시퀀스 데이터셋 저장용 리스트객체 생성 \n",
    "    dataset = []\n",
    "\n",
    "    # for반복구문을 이용한 시퀀스 데이터셋을 분리추출\n",
    "    for i in range(len(seq) - window_size):\n",
    "\n",
    "        subset = seq[i:(i + window_size + 1)]\n",
    "        # - 전체 음표시퀀스 데이터에서 원하는 윈도우 길이 + 1 만큼 부분추출\n",
    "        # - 윈도우 길이만큼의 음표로 학습을 하고, 이어진 + 1에 해당하는 음표를 분류예측하는 학습을 진행하게 됨\n",
    "\n",
    "        dataset.append([code2idx[item] for item in subset])\n",
    "        # - 음표코드2인덱스번호 사전을 이용해 분리추출한 \n",
    "        #   음표 하나하나를 인덱스번호로 변환해 데이터셋을 만듬\n",
    "     \n",
    "    # 사용자정의함수 반환값 지정\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable       Type        Data/Info\n",
      "------------------------------------\n",
      "code2idx       dict        n=14\n",
      "dataset        ndarray     50x5: 250 elems, type `int32`, 1000 bytes\n",
      "font_manager   module      <module 'matplotlib.font_<...>lotlib\\\\font_manager.py'>\n",
      "font_name      str         NanumGothic\n",
      "i              int         49\n",
      "idx2code       dict        n=14\n",
      "np             module      <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os             module      <module 'os' from 'C:\\\\Anaconda3\\\\lib\\\\os.py'>\n",
      "pd             module      <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt            module      <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "seq            list        n=54\n",
      "seq2dataset    function    <function seq2dataset at 0x0000018595DF6598>\n",
      "seq_num        list        n=54\n",
      "shutil         module      <module 'shutil' from 'C:<...>aconda3\\\\lib\\\\shutil.py'>\n",
      "sns            module      <module 'seaborn' from 'C<...>s\\\\seaborn\\\\__init__.py'>\n",
      "subset         list        n=5\n",
      "warnings       module      <module 'warnings' from '<...>onda3\\\\lib\\\\warnings.py'>\n",
      "window_size    int         4\n"
     ]
    }
   ],
   "source": [
    "# 메모리상에서 사용자정의함수 생성여부 확인 \n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seq2dataset(seq, window_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음표시퀀스 데이터셋 객체유형: <class 'numpy.ndarray'>\n",
      "음표시퀀스 데이터셋 객체구조: (50, 5)\n",
      "\n",
      "음표시퀀스 데이터셋 내용:\n",
      " [[11  9  2 10  8]\n",
      " [ 9  2 10  8  1]\n",
      " [ 2 10  8  1  7]\n",
      " [10  8  1  7  8]\n",
      " [ 8  1  7  8  9]\n",
      " [ 1  7  8  9 10]\n",
      " [ 7  8  9 10 11]\n",
      " [ 8  9 10 11 11]\n",
      " [ 9 10 11 11  4]\n",
      " [10 11 11  4 11]\n",
      " [11 11  4 11  9]\n",
      " [11  4 11  9  9]\n",
      " [ 4 11  9  9  9]\n",
      " [11  9  9  9 10]\n",
      " [ 9  9  9 10  8]\n",
      " [ 9  9 10  8  1]\n",
      " [ 9 10  8  1  7]\n",
      " [10  8  1  7  9]\n",
      " [ 8  1  7  9 11]\n",
      " [ 1  7  9 11 11]\n",
      " [ 7  9 11 11  9]\n",
      " [ 9 11 11  9  9]\n",
      " [11 11  9  9  2]\n",
      " [11  9  9  2  8]\n",
      " [ 9  9  2  8  8]\n",
      " [ 9  2  8  8  8]\n",
      " [ 2  8  8  8  8]\n",
      " [ 8  8  8  8  8]\n",
      " [ 8  8  8  8  9]\n",
      " [ 8  8  8  9  3]\n",
      " [ 8  8  9  3  9]\n",
      " [ 8  9  3  9  9]\n",
      " [ 9  3  9  9  9]\n",
      " [ 3  9  9  9  9]\n",
      " [ 9  9  9  9  9]\n",
      " [ 9  9  9  9 10]\n",
      " [ 9  9  9 10  4]\n",
      " [ 9  9 10  4 11]\n",
      " [ 9 10  4 11  9]\n",
      " [10  4 11  9  2]\n",
      " [ 4 11  9  2 10]\n",
      " [11  9  2 10  8]\n",
      " [ 9  2 10  8  1]\n",
      " [ 2 10  8  1  7]\n",
      " [10  8  1  7  9]\n",
      " [ 8  1  7  9 11]\n",
      " [ 1  7  9 11 11]\n",
      " [ 7  9 11 11  9]\n",
      " [ 9 11 11  9  9]\n",
      " [11 11  9  9  2]]\n"
     ]
    }
   ],
   "source": [
    "# 사용자정의함수를 통해 추출된 음표시퀀스 데이터셋 구조\n",
    "print('음표시퀀스 데이터셋 객체유형:', type(dataset))\n",
    "print('음표시퀀스 데이터셋 객체구조:', dataset.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# 윈도우 길이만큼 분리추출된 음표 데이터셋\n",
    "print('음표시퀀스 데이터셋 내용:\\n', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 bias를 통해 y값을 만들어내야하는데  <br>\n",
    "현재, 음악 코드가 인덱스번호(숫자)이기 때문에 동일한 조건으로 판단 안 할 가능성이 크다. <br>\n",
    "그러므로 독립변수 4자리는 0과 1사이로 스케일링을 해줘야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음표시퀀스 데이터셋 분할: 피처셋과 타겟변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 피처셋 객체유형: <class 'numpy.ndarray'>\n",
      "훈련용 피처셋 객체규모: (50, 4)\n",
      "훈련용 타겟변수 객체유형: <class 'numpy.ndarray'>\n",
      "훈련용 타겟변수 객체규모: (50,)\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 피처셋 추출\n",
    "X_train = dataset[:, 0:4]\n",
    "\n",
    "print('훈련용 피처셋 객체유형:', type(X_train))\n",
    "print('훈련용 피처셋 객체규모:', X_train.shape)\n",
    "\n",
    "# 훈련용 타겟변수 추출\n",
    "y_train = dataset[:, 4]\n",
    "\n",
    "print('훈련용 타겟변수 객체유형:', type(y_train))\n",
    "print('훈련용 타겟변수 객체규모:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>데이터 탐색</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련용 음표 피처셋 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  9,  2, 10],\n",
       "       [ 9,  2, 10,  8],\n",
       "       [ 2, 10,  8,  1],\n",
       "       [10,  8,  1,  7],\n",
       "       [ 8,  1,  7,  8],\n",
       "       [ 1,  7,  8,  9],\n",
       "       [ 7,  8,  9, 10],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [ 9, 10, 11, 11],\n",
       "       [10, 11, 11,  4],\n",
       "       [11, 11,  4, 11],\n",
       "       [11,  4, 11,  9],\n",
       "       [ 4, 11,  9,  9],\n",
       "       [11,  9,  9,  9],\n",
       "       [ 9,  9,  9, 10],\n",
       "       [ 9,  9, 10,  8],\n",
       "       [ 9, 10,  8,  1],\n",
       "       [10,  8,  1,  7],\n",
       "       [ 8,  1,  7,  9],\n",
       "       [ 1,  7,  9, 11],\n",
       "       [ 7,  9, 11, 11],\n",
       "       [ 9, 11, 11,  9],\n",
       "       [11, 11,  9,  9],\n",
       "       [11,  9,  9,  2],\n",
       "       [ 9,  9,  2,  8],\n",
       "       [ 9,  2,  8,  8],\n",
       "       [ 2,  8,  8,  8],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [ 8,  8,  8,  9],\n",
       "       [ 8,  8,  9,  3],\n",
       "       [ 8,  9,  3,  9],\n",
       "       [ 9,  3,  9,  9],\n",
       "       [ 3,  9,  9,  9],\n",
       "       [ 9,  9,  9,  9],\n",
       "       [ 9,  9,  9,  9],\n",
       "       [ 9,  9,  9, 10],\n",
       "       [ 9,  9, 10,  4],\n",
       "       [ 9, 10,  4, 11],\n",
       "       [10,  4, 11,  9],\n",
       "       [ 4, 11,  9,  2],\n",
       "       [11,  9,  2, 10],\n",
       "       [ 9,  2, 10,  8],\n",
       "       [ 2, 10,  8,  1],\n",
       "       [10,  8,  1,  7],\n",
       "       [ 8,  1,  7,  9],\n",
       "       [ 1,  7,  9, 11],\n",
       "       [ 7,  9, 11, 11],\n",
       "       [ 9, 11, 11,  9],\n",
       "       [11, 11,  9,  9]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호 내용조회\n",
    "\n",
    "X_train\n",
    "# - 4개의 시퀀스 음표 인덱스번호가 훈련용 피처셋으로 입력될 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  9,  2, 10,  9,  2, 10,  8,  2, 10,  8,  1, 10,  8,  1,  7,  8,\n",
       "        1,  7,  8,  1,  7,  8,  9,  7,  8,  9, 10,  8,  9, 10, 11,  9, 10,\n",
       "       11, 11, 10, 11, 11,  4, 11, 11,  4, 11, 11,  4, 11,  9,  4, 11,  9,\n",
       "        9, 11,  9,  9,  9,  9,  9,  9, 10,  9,  9, 10,  8,  9, 10,  8,  1,\n",
       "       10,  8,  1,  7,  8,  1,  7,  9,  1,  7,  9, 11,  7,  9, 11, 11,  9,\n",
       "       11, 11,  9, 11, 11,  9,  9, 11,  9,  9,  2,  9,  9,  2,  8,  9,  2,\n",
       "        8,  8,  2,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        9,  8,  8,  9,  3,  8,  9,  3,  9,  9,  3,  9,  9,  3,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10,  9,  9, 10,  4,  9,\n",
       "       10,  4, 11, 10,  4, 11,  9,  4, 11,  9,  2, 11,  9,  2, 10,  9,  2,\n",
       "       10,  8,  2, 10,  8,  1, 10,  8,  1,  7,  8,  1,  7,  9,  1,  7,  9,\n",
       "       11,  7,  9, 11, 11,  9, 11, 11,  9, 11, 11,  9,  9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호 내용을 1차원 배열로 재구조화\n",
    "\n",
    "X_train_1d = X_train.reshape(-1)\n",
    "X_train_1d\n",
    "# - 어떠한 음표 인덱스번호가 많이 사용되었는지를 보기 위해서\n",
    "#   자료구조를 1차원 배열로 만들어서 인덱스 번호숫자별 카운팅을 하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처셋 음표 인덱스번호 최소값: 1\n",
      "피처셋 음표 인덱스번호 최대값: 11\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터셋 중 피처셋 음표 인덱스번호 최소값\n",
    "print('피처셋 음표 인덱스번호 최소값:', np.min(X_train_1d))\n",
    "\n",
    "# 훈련용 데이터셋 중 피처셋 음표 인덱스번호 최대값\n",
    "print('피처셋 음표 인덱스번호 최대값:', np.max(X_train_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처셋 음표유형 인덱스번호:\n",
      " [ 1  2  3  4  7  8  9 10 11]\n",
      "\n",
      "피처셋  음표 인덱스번호별 빈도수:\n",
      " [ 0 12 11  4  8  0  0 12 36 65 20 32]\n",
      "\n",
      "피처셋  음표 인덱스번호별 비율:\n",
      " [0.0, 0.06, 0.055, 0.02, 0.04, 0.0, 0.0, 0.06, 0.18, 0.325, 0.1, 0.16]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호 파악\n",
    "X_train_uq = np.unique(X_train_1d)\n",
    "print('피처셋 음표유형 인덱스번호:\\n', X_train_uq)\n",
    "\n",
    "X_train_bin = np.bincount(X_train_1d)\n",
    "print('\\n피처셋  음표 인덱스번호별 빈도수:\\n', X_train_bin)\n",
    "\n",
    "X_train_por = [round(i / sum(X_train_bin), 3) for i in X_train_bin]\n",
    "print('\\n피처셋  음표 인덱스번호별 비율:\\n', X_train_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처셋 음표 인덱스번호와 빈도수 딕셔너리:\n",
      " Counter({9: 65, 8: 36, 11: 32, 10: 20, 1: 12, 7: 12, 2: 11, 4: 8, 3: 4})\n"
     ]
    }
   ],
   "source": [
    "# collections 모듈을 이용한 피처셋 음표 인덱스번호 유형과 빈도수 동시 계산\n",
    "import collections\n",
    "\n",
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호별 빈도수\n",
    "X_train_dic = collections.Counter(X_train_1d)\n",
    "print('피처셋 음표 인덱스번호와 빈도수 딕셔너리:\\n', X_train_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 12),\n",
       "             (2, 11),\n",
       "             (3, 4),\n",
       "             (4, 8),\n",
       "             (7, 12),\n",
       "             (8, 36),\n",
       "             (9, 65),\n",
       "             (10, 20),\n",
       "             (11, 32)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 딕셔너리 소팅\n",
    "X_train_odic = collections.OrderedDict(sorted(X_train_dic.items()))\n",
    "X_train_odic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처셋 음표 인덱스번호 조회:\n",
      " [1, 2, 3, 4, 7, 8, 9, 10, 11]\n",
      "\n",
      "피처셋 음표 인덱스번호별 음표유형 조회:\n",
      " ['d4', 'e4', 'f4', 'g4', 'c8', 'd8', 'e8', 'f8', 'g8']\n",
      "\n",
      "피처셋 음표 인덱스번호별 빈도수 조회:\n",
      " [12, 11, 4, 8, 12, 36, 65, 20, 32]\n",
      "\n",
      "피처셋 음표 인덱스번호별 비율:\n",
      " [0.005, 0.01, 0.015, 0.02, 0.035, 0.04, 0.045, 0.05, 0.055]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호 조회\n",
    "print('피처셋 음표 인덱스번호 조회:\\n', list(X_train_odic.keys()))\n",
    "\n",
    "# 훈련 데이터셋 중 피처셋 음표 인덱스번호별 음표유형 조회\n",
    "print('\\n피처셋 음표 인덱스번호별 음표유형 조회:\\n', [idx2code.get(i) for i in list(X_train_odic.keys())])\n",
    "\n",
    "# 훈련 데이터셋 중 피처셋음표 인덱스번호별 빈도수 조회\n",
    "print('\\n피처셋 음표 인덱스번호별 빈도수 조회:\\n', list(X_train_odic.values()))\n",
    "\n",
    "X_train_por = [round(i / sum(X_train_odic.values()), 3) for i in X_train_odic.keys()]\n",
    "print('\\n피처셋 음표 인덱스번호별 비율:\\n', X_train_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>code</th>\n",
       "      <th>frequency</th>\n",
       "      <th>oportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>d4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>e4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>f4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>g4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>c8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>d8</td>\n",
       "      <td>36</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>e8</td>\n",
       "      <td>65</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>f8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>g8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx code  frequency  oportion\n",
       "0    1   d4         12     0.005\n",
       "1    2   e4         11     0.010\n",
       "2    3   f4          4     0.015\n",
       "3    4   g4          8     0.020\n",
       "4    7   c8         12     0.035\n",
       "5    8   d8         36     0.040\n",
       "6    9   e8         65     0.045\n",
       "7   10   f8         20     0.050\n",
       "8   11   g8         32     0.055"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표별 분포테이블\n",
    "X_train_tb = pd.DataFrame({'idx': list(X_train_odic.keys()), \n",
    "                           'code': [ idx2code.get(i) for i in list(X_train_odic.keys())],\n",
    "                           'frequency': list(X_train_odic.values()), \n",
    "                           'oportion': X_train_por})\n",
    "X_train_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>code</th>\n",
       "      <th>frequency</th>\n",
       "      <th>oportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>e8</td>\n",
       "      <td>65</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>d8</td>\n",
       "      <td>36</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>g8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>f8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>d4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>c8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>e4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>g4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>f4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx code  frequency  oportion\n",
       "0    9   e8         65     0.045\n",
       "1    8   d8         36     0.040\n",
       "2   11   g8         32     0.055\n",
       "3   10   f8         20     0.050\n",
       "4    1   d4         12     0.005\n",
       "5    7   c8         12     0.035\n",
       "6    2   e4         11     0.010\n",
       "7    4   g4          8     0.020\n",
       "8    3   f4          4     0.015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 음표가 많은지 발생빈도(frequency) 순으로 내림차순 정렬\n",
    "X_train_tb_sorted = X_train_tb.sort_values('frequency', ascending = 0).reset_index(drop = True) \n",
    "X_train_tb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 피처셋 음표 분포그래프\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.barplot(x = 'idx', y = 'frequency', data = X_train_tb) \n",
    "ax.set_title('피처셋 음표 인덱스번호 분포현황', size = 14) \n",
    "ax.set_xlabel('음표 인덱스번호', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = sns.barplot(x = 'idx', y = 'frequency', data = X_train_tb_sorted, \n",
    "                order = X_train_tb_sorted.idx.values) \n",
    "ax.set_title('피처셋 음표 인덱스번호 분포현황(내림차순)', size = 14) \n",
    "ax.set_xlabel('음표 인덱스번호', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "ax = sns.barplot(x = 'code', y = 'frequency', data = X_train_tb) \n",
    "ax.set_title('피처셋 음표유형 분포현황', size = 14) \n",
    "ax.set_xlabel('음표유형', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "ax = sns.barplot(x = 'code', y = 'frequency', data = X_train_tb_sorted, \n",
    "                order = X_train_tb_sorted.code.values) \n",
    "ax.set_title('피처셋 음표유형 분포현황(내림차순)', size = 14) \n",
    "ax.set_xlabel('음표유형', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련용 음표 타겟변수 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  1,  7,  8,  9, 10, 11, 11,  4, 11,  9,  9,  9, 10,  8,  1,  7,\n",
       "        9, 11, 11,  9,  9,  2,  8,  8,  8,  8,  8,  9,  3,  9,  9,  9,  9,\n",
       "        9, 10,  4, 11,  9,  2, 10,  8,  1,  7,  9, 11, 11,  9,  9,  2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호 내용조회\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타겟변수 음표 인덱스번호 최소값: 1\n",
      "타겟변수 음표 인덱스번호 최대값: 11\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터셋 중 타겟변수 음표 인덱스번호 최소값\n",
    "print('타겟변수 음표 인덱스번호 최소값:', np.min(y_train))\n",
    "\n",
    "# 훈련용 데이터셋 중 타겟변수 음표 인덱스번호 최대값\n",
    "print('타겟변수 음표 인덱스번호 최대값:', np.max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타겟변수 음표유형 인덱스번호:\n",
      " [ 1  2  3  4  7  8  9 10 11]\n",
      "\n",
      "타겟변수 음표 인덱스번호별 빈도수:\n",
      " [ 0  3  3  1  2  0  0  3  9 17  4  8]\n",
      "\n",
      "타겟변수 음표 인덱스번호별 비율:\n",
      " [0.0, 0.06, 0.06, 0.02, 0.04, 0.0, 0.0, 0.06, 0.18, 0.34, 0.08, 0.16]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호 파악\n",
    "y_train_uq = np.unique(y_train)\n",
    "print('타겟변수 음표유형 인덱스번호:\\n', y_train_uq)\n",
    "\n",
    "y_train_bin = np.bincount(y_train)\n",
    "print('\\n타겟변수 음표 인덱스번호별 빈도수:\\n', y_train_bin)\n",
    "\n",
    "y_train_por = [round(i / sum(y_train_bin), 3) for i in y_train_bin]\n",
    "print('\\n타겟변수 음표 인덱스번호별 비율:\\n', y_train_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타겟변수 음표 인덱스번호와 빈도수 딕셔너리:\n",
      " Counter({9: 17, 8: 9, 11: 8, 10: 4, 1: 3, 7: 3, 2: 3, 4: 2, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "# collections 모듈을 이용한 타겟변수 음표 인덱스번호 유형과 빈도수 동시 계산\n",
    "import collections\n",
    "\n",
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호별 빈도수\n",
    "y_train_dic = collections.Counter(y_train)\n",
    "print('타겟변수 음표 인덱스번호와 빈도수 딕셔너리:\\n', y_train_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 3),\n",
       "             (2, 3),\n",
       "             (3, 1),\n",
       "             (4, 2),\n",
       "             (7, 3),\n",
       "             (8, 9),\n",
       "             (9, 17),\n",
       "             (10, 4),\n",
       "             (11, 8)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 딕셔너리 소팅\n",
    "y_train_odic = collections.OrderedDict(sorted(y_train_dic.items()))\n",
    "y_train_odic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타겟변수 음표 인덱스번호 조회:\n",
      " [1, 2, 3, 4, 7, 8, 9, 10, 11]\n",
      "\n",
      "타겟변수 음표 인덱스번호별 음표유형 조회:\n",
      " ['d4', 'e4', 'f4', 'g4', 'c8', 'd8', 'e8', 'f8', 'g8']\n",
      "\n",
      "타겟변수 음표 인덱스번호별 빈도수 조회:\n",
      " [3, 3, 1, 2, 3, 9, 17, 4, 8]\n",
      "\n",
      "타겟변수 음표 인덱스번호별 비율:\n",
      " [0.02, 0.04, 0.06, 0.08, 0.14, 0.16, 0.18, 0.2, 0.22]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호 조회\n",
    "print('타겟변수 음표 인덱스번호 조회:\\n', list(y_train_odic.keys()))\n",
    "\n",
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호별 음표유형 조회\n",
    "print('\\n타겟변수 음표 인덱스번호별 음표유형 조회:\\n', [idx2code.get(i) for i in list(y_train_odic.keys())])\n",
    "\n",
    "# 훈련 데이터셋 중 타겟변수 음표 인덱스번호별 빈도수 조회\n",
    "print('\\n타겟변수 음표 인덱스번호별 빈도수 조회:\\n', list(y_train_odic.values()))\n",
    "\n",
    "y_train_por = [round(i / sum(y_train_odic.values()), 3) for i in y_train_odic.keys()]\n",
    "print('\\n타겟변수 음표 인덱스번호별 비율:\\n', y_train_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>code</th>\n",
       "      <th>frequency</th>\n",
       "      <th>portion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>d4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>e4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>f4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>g4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>c8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>d8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>e8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>f8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>g8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx code  frequency  portion\n",
       "0    1   d4          3     0.02\n",
       "1    2   e4          3     0.04\n",
       "2    3   f4          1     0.06\n",
       "3    4   g4          2     0.08\n",
       "4    7   c8          3     0.14\n",
       "5    8   d8          9     0.16\n",
       "6    9   e8         17     0.18\n",
       "7   10   f8          4     0.20\n",
       "8   11   g8          8     0.22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 음표별 분포테이블\n",
    "y_train_tb = pd.DataFrame({'idx': list(y_train_odic.keys()), \n",
    "                           'code': [idx2code.get(i) for i in list(y_train_odic.keys())],\n",
    "                           'frequency': list(y_train_odic.values()), \n",
    "                           'portion': y_train_por})\n",
    "y_train_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>code</th>\n",
       "      <th>frequency</th>\n",
       "      <th>portion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>e8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>d8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>g8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>f8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>d4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>e4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>c8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>g4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>f4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx code  frequency  portion\n",
       "0    9   e8         17     0.18\n",
       "1    8   d8          9     0.16\n",
       "2   11   g8          8     0.22\n",
       "3   10   f8          4     0.20\n",
       "4    1   d4          3     0.02\n",
       "5    2   e4          3     0.04\n",
       "6    7   c8          3     0.14\n",
       "7    4   g4          2     0.08\n",
       "8    3   f4          1     0.06"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 음표가 많은지 발생빈도(frequency) 순으로 내림차순 정렬\n",
    "y_train_tb_sorted = y_train_tb.sort_values('frequency', ascending = 0).reset_index(drop = True) \n",
    "y_train_tb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 데이터셋 중 타겟변수 음표 분포그래프\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.barplot(x = 'idx', y = 'frequency', data = y_train_tb) \n",
    "ax.set_title('타겟변수 음표 인덱스번호 분포현황', size = 14) \n",
    "ax.set_xlabel('음표 인덱스번호', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = sns.barplot(x = 'idx', y = 'frequency', data = y_train_tb_sorted, \n",
    "                order = y_train_tb_sorted.idx.values) \n",
    "ax.set_title('타겟변수 음표 인덱스번호 분포현황(내림차순)', size = 14) \n",
    "ax.set_xlabel('음표 인덱스번호', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "ax = sns.barplot(x = 'code', y = 'frequency', data = y_train_tb) \n",
    "ax.set_title('타겟변수 음표유형 분포현황', size = 14) \n",
    "ax.set_xlabel('음표유형', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "ax = sns.barplot(x = 'code', y = 'frequency', data = y_train_tb_sorted, \n",
    "                order = y_train_tb_sorted.code.values) \n",
    "ax.set_title('타겟변수 음표유형 분포현황(내림차순)', size = 14) \n",
    "ax.set_xlabel('음표유형', size = 12) \n",
    "ax.set_ylabel('갯수', size = 12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>데이터 전처리</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처셋 스케일링: 음표 인덱스번호 수치를 0~1 사이로 변환\n",
    "* sklearn라이브러리 preprocessing모듈의 minmax_scale() 함수를 이용해 최대/최소값이 각각 1, 0이 되도록 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[훈련용   피처셋] 최소:1.0, 최대:11.0, 평균:7.935, 표준편차:2.878\n",
      "[스케일링 피처셋] 최소:0.0, 최대: 1.0, 평균:0.693, 표준편차:0.288\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터셋 중 피처셋 스케일링\n",
    "X_train_scaled = minmax_scale(X_train)\n",
    "\n",
    "print(f'[훈련용   피처셋] 최소:{np.min(X_train):3.1f}, 최대:{np.max(X_train):4.1f}', end = \", \")\n",
    "print(f'평균:{X_train.mean():5.3f}, 표준편차:{X_train.std():5.3f}')\n",
    "      \n",
    "print(f'[스케일링 피처셋] 최소:{np.min(X_train_scaled):3.1f}, 최대:{np.max(X_train_scaled):4.1f}', end=\", \")\n",
    "print(f'평균:{X_train_scaled.mean():5.3f}, 표준편차:{X_train_scaled.std():5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스케일링한 피처셋 객체구조: (50, 4)\n",
      "스케일링한 피처셋 레코드 갯수: 50\n",
      "스케일링한 피처셋 변수컬럼 갯수: 4\n"
     ]
    }
   ],
   "source": [
    "# 스케일링을 통해 정규화한 피처셋 객체구조\n",
    "print('스케일링한 피처셋 객체구조:', X_train_scaled.shape)\n",
    "print('스케일링한 피처셋 레코드 갯수:', X_train_scaled.shape[0])\n",
    "print('스케일링한 피처셋 변수컬럼 갯수:', X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 타겟변수 원핫인코딩: 음표 인덱스번호를 이진벡터행렬로 구성\n",
    "* 타겟변수가 다항 클래스인 경우 원핫인코딩으로 변경해 0과 1로 구성된 상태로 만들어야 딥러닝 연산이 가능해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 원핫인코딩 모듈로딩\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[훈련셋 타겟변수] 원본데이터 구조: (50,) => 원핫인코딩구조: (50, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련셋 중 타겟변수 원핫인코딩\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "\n",
    "print(f'[훈련셋 타겟변수] 원본데이터 구조: {y_train.shape} => 원핫인코딩구조: {y_train_ohe.shape}')\n",
    "\n",
    "y_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원핫인코딩된 타겟변수 객체구조: (50, 12)\n",
      "원핫인코딩된 타겟변수 레코드 갯수: 50\n",
      "원핫인코딩된 타겟변수 변수컬럼 갯수: 12\n"
     ]
    }
   ],
   "source": [
    "# 원핫인코딩된 타겟변수 객체구조\n",
    "print('원핫인코딩된 타겟변수 객체구조:', y_train_ohe.shape)\n",
    "print('원핫인코딩된 타겟변수 레코드 갯수:', y_train_ohe.shape[0])\n",
    "print('원핫인코딩된 타겟변수 변수컬럼 갯수:', y_train_ohe.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>딥러닝 모델링</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 관련 라이브러리\n",
    "import tensorflow as tf\n",
    "# - 딥러닝 학습알고리즘\n",
    "\n",
    "from keras.models import Sequential\n",
    "# - 딥러닝 학습계층을 순차적으로 쌓아올릴 수 있도록 해줌\n",
    "\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "# - 딥러닝 학습계층별 노드/유닛을 밀집시켜 학습방법을 설정할 수 있도록 해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 시드넘버 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 시드넘버 설정으로 샘플 재현성 확보\n",
    "myseed = 0\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(myseed)\n",
    "tf.set_random_seed(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델계층 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 9,676\n",
      "Trainable params: 9,676\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 학습계층 시퀀스객체 로딩\n",
    "md = Sequential()\n",
    "# - 딥러닝 학습계층을 순차적으로 쌓아올릴 수 있는 Sequential()메서드를 md라는 객체로 로딩함\n",
    "\n",
    "# 딥러닝 학습계층 설계\n",
    "# - Dense() 객체로 학습계층별로 여러 개 노드가 밀집된 신경망을 구현함\n",
    "# - 입력층(784개노드)-은닉층(512개노드)-출력층(10개노드)\n",
    "\n",
    "# 입력층: 일반 덴스밀집층(완전연결층)\n",
    "md.add(Dense(128, input_dim = X_train_scaled.shape[1], activation = 'relu'))\n",
    "# -  스케일링한 피처셋 변수컬럼 갯수를 지정하기 위해서\n",
    "#    input_dim = X_train_scaled.shape[1]을 사용함\n",
    "\n",
    "# 은닉층: 일반 덴스밀집층(완전연결층)\n",
    "md.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# 출력층\n",
    "md.add(Dense(y_train_ohe.shape[1], activation = 'softmax'))\n",
    "# - 출력값을 원핫인코딩된 타겟변수 변수컬럼 갯수로 지정하기 위해서\n",
    "#   y_train_ohe.shape[1]라는 코드를 사용함\n",
    "# - activation = 'softmax'로 활성화함수 지정해 \n",
    "#   최종적인 출력값을 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0과 같은 원핫인코딩 방식으로 도출\n",
    "\n",
    "# 딥러닝 계층구조 요약\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 학습방법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 준비된 학습계층의 학습방법 설정\n",
    "md.compile(loss = 'categorical_crossentropy', \n",
    "           optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# - loss: 학습시 오차를 어떻방법으로 계산할지 설정\n",
    "# - optimizser: 발생하는 오차를 어떤방법으로 줄여나갈지 설정\n",
    "#   binary_crossentropy: 이항분류(binary-class classification)에 적합함\n",
    "#   categorical_crossentropy: 다항분류(multi-class classification)에 적합함\n",
    "# - metrics: 모형의 성능을 평가하는 기준으로 어떤 것을 사용할지 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습모델 파일저장 옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 학습모델 저장폴더명 설정\n",
    "import os\n",
    "\n",
    "md_dir = './model/'\n",
    "if not os.path.exists(md_dir):\n",
    "    os.mkdir(md_dir)\n",
    "    \n",
    "# 딥러닝 학습모델별 저장파일명 변경설정\n",
    "md_path = './model/vloss{val_loss:.3f}_vacc{val_acc:.3f}_ep{epoch:d}.hdf5'\n",
    "# - loss: 전체데이터 중 훈련셋(training) 학습오차\n",
    "# - acc: 전체데이터 중 훈련셋(training) 정확도\n",
    "# - val_loss: 전체데이터 중 검증셋(validation) 오차\n",
    "# - val_acc: 전체데이터 중 검증셋(validation) 정확도\n",
    "\n",
    "# ep{epoch:d} 단위배치(batch) 작업인 엑폭번호를 정수형으로 파일명 시작부분에 기록\n",
    "# vloss{val_loss:.3f} 각 학습모델의 검증데이터 오차정도를 소수3자리까지 파일에 기록\n",
    "# vacc{val_acc:.3f} 각 학습모델의 검증데이터 정확정도를 소수3째자리까지 파일에 기록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습모델 체크포인트 옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습모델 체크포인트 모듈 로딩\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트(checkpoint) 변경설정\n",
    "cp = ModelCheckpoint(filepath = md_path, \n",
    "                     monitor = 'val_loss', \n",
    "                     verbose = 1, \n",
    "                     save_best_only = True)\n",
    "# - filepath: 각 학습모델별 성능평가 파일저장 경로\n",
    "# - monitor 옵션: 각 학습모델별 성능평가 기준\n",
    "#   * 전체데이터 중 훈련셋(training) 학습오차: loss\n",
    "#   * 전체데이터 중 훈련셋(training) 정확도: acc\n",
    "#   * 전체데이터 중 검증셋(validation) 오차: val_loss\n",
    "#   * 전체데이터 중 검증셋(validation) 정확도: val_acc\n",
    "#   * 전체데이터 중 시험셋(테스트; test)은 별도로 분할해 놓아야 함\n",
    "# - verbose: 모델별 학습진행사항 출력\n",
    "# - save_best_only: 각 학습모델별 성능평가 파일저장시\n",
    "#                   이전 모델보다 성능이 좋아졌을 때만 저장하도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습모델 과적합 방지 옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 방지용 종료 메서드 로딩\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기종료 옵션설정\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 300)\n",
    "# - monitor: 검증셋 대상 모니터할 값 설정\n",
    "# - patience: 검증셋 오차가 좋아지지 않아도 몇 번까지 기다릴지 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델에 데이터 피팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45 samples, validate on 5 samples\n",
      "Epoch 1/2000\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 2.4193 - acc: 0.2000 - val_loss: 2.3789 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.37889, saving model to ./model/vloss2.379_vacc0.200_ep1.hdf5\n",
      "Epoch 2/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 2.2860 - acc: 0.2444 - val_loss: 2.2464 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.37889 to 2.24638, saving model to ./model/vloss2.246_vacc0.400_ep2.hdf5\n",
      "Epoch 3/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 2.1930 - acc: 0.3333 - val_loss: 2.1506 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.24638 to 2.15057, saving model to ./model/vloss2.151_vacc0.400_ep3.hdf5\n",
      "Epoch 4/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 2.0923 - acc: 0.3333 - val_loss: 2.0598 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.15057 to 2.05975, saving model to ./model/vloss2.060_vacc0.400_ep4.hdf5\n",
      "Epoch 5/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 2.0127 - acc: 0.3333 - val_loss: 1.9906 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.05975 to 1.99062, saving model to ./model/vloss1.991_vacc0.400_ep5.hdf5\n",
      "Epoch 6/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.9533 - acc: 0.3333 - val_loss: 1.9561 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.99062 to 1.95611, saving model to ./model/vloss1.956_vacc0.400_ep6.hdf5\n",
      "Epoch 7/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.9180 - acc: 0.3333 - val_loss: 1.9161 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.95611 to 1.91609, saving model to ./model/vloss1.916_vacc0.400_ep7.hdf5\n",
      "Epoch 8/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.8924 - acc: 0.3333 - val_loss: 1.8944 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.91609 to 1.89438, saving model to ./model/vloss1.894_vacc0.400_ep8.hdf5\n",
      "Epoch 9/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.8588 - acc: 0.3333 - val_loss: 1.8750 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.89438 to 1.87503, saving model to ./model/vloss1.875_vacc0.400_ep9.hdf5\n",
      "Epoch 10/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 1.8396 - acc: 0.3556 - val_loss: 1.8563 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.87503 to 1.85625, saving model to ./model/vloss1.856_vacc0.400_ep10.hdf5\n",
      "Epoch 11/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 1.8160 - acc: 0.3778 - val_loss: 1.8415 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.85625 to 1.84151, saving model to ./model/vloss1.842_vacc0.400_ep11.hdf5\n",
      "Epoch 12/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.8047 - acc: 0.4000 - val_loss: 1.8201 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.84151 to 1.82009, saving model to ./model/vloss1.820_vacc0.400_ep12.hdf5\n",
      "Epoch 13/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.7850 - acc: 0.3778 - val_loss: 1.8031 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.82009 to 1.80307, saving model to ./model/vloss1.803_vacc0.400_ep13.hdf5\n",
      "Epoch 14/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.7690 - acc: 0.4000 - val_loss: 1.7900 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.80307 to 1.78999, saving model to ./model/vloss1.790_vacc0.400_ep14.hdf5\n",
      "Epoch 15/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 1.7464 - acc: 0.3778 - val_loss: 1.7919 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.78999\n",
      "Epoch 16/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.7365 - acc: 0.4000 - val_loss: 1.7735 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.78999 to 1.77351, saving model to ./model/vloss1.774_vacc0.400_ep16.hdf5\n",
      "Epoch 17/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.7156 - acc: 0.4000 - val_loss: 1.7817 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.77351\n",
      "Epoch 18/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.7129 - acc: 0.4000 - val_loss: 1.7694 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.77351 to 1.76938, saving model to ./model/vloss1.769_vacc0.400_ep18.hdf5\n",
      "Epoch 19/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.6932 - acc: 0.4000 - val_loss: 1.7682 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.76938 to 1.76822, saving model to ./model/vloss1.768_vacc0.400_ep19.hdf5\n",
      "Epoch 20/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.6839 - acc: 0.4222 - val_loss: 1.7370 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.76822 to 1.73700, saving model to ./model/vloss1.737_vacc0.400_ep20.hdf5\n",
      "Epoch 21/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.6724 - acc: 0.4000 - val_loss: 1.7537 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.73700\n",
      "Epoch 22/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.6602 - acc: 0.4000 - val_loss: 1.7380 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.73700\n",
      "Epoch 23/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.6493 - acc: 0.4222 - val_loss: 1.7268 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.73700 to 1.72678, saving model to ./model/vloss1.727_vacc0.400_ep23.hdf5\n",
      "Epoch 24/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.6439 - acc: 0.4000 - val_loss: 1.7438 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.72678\n",
      "Epoch 25/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.6263 - acc: 0.3778 - val_loss: 1.7293 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.72678\n",
      "Epoch 26/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.6186 - acc: 0.4000 - val_loss: 1.6970 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.72678 to 1.69698, saving model to ./model/vloss1.697_vacc0.400_ep26.hdf5\n",
      "Epoch 27/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 1.6120 - acc: 0.3778 - val_loss: 1.6991 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.69698\n",
      "Epoch 28/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.6013 - acc: 0.3778 - val_loss: 1.6974 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.69698\n",
      "Epoch 29/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.5912 - acc: 0.3778 - val_loss: 1.6864 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.69698 to 1.68640, saving model to ./model/vloss1.686_vacc0.400_ep29.hdf5\n",
      "Epoch 30/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5908 - acc: 0.3778 - val_loss: 1.6823 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.68640 to 1.68234, saving model to ./model/vloss1.682_vacc0.400_ep30.hdf5\n",
      "Epoch 31/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5757 - acc: 0.4000 - val_loss: 1.6874 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.68234\n",
      "Epoch 32/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5709 - acc: 0.4000 - val_loss: 1.6809 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.68234 to 1.68086, saving model to ./model/vloss1.681_vacc0.400_ep32.hdf5\n",
      "Epoch 33/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5707 - acc: 0.4222 - val_loss: 1.7172 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.68086\n",
      "Epoch 34/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.5588 - acc: 0.4444 - val_loss: 1.6852 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.68086\n",
      "Epoch 35/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5632 - acc: 0.4667 - val_loss: 1.6876 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.68086\n",
      "Epoch 36/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.5460 - acc: 0.4889 - val_loss: 1.6771 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.68086 to 1.67714, saving model to ./model/vloss1.677_vacc0.400_ep36.hdf5\n",
      "Epoch 37/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5414 - acc: 0.4222 - val_loss: 1.6657 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.67714 to 1.66568, saving model to ./model/vloss1.666_vacc0.400_ep37.hdf5\n",
      "Epoch 38/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5387 - acc: 0.4000 - val_loss: 1.6606 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.66568 to 1.66063, saving model to ./model/vloss1.661_vacc0.400_ep38.hdf5\n",
      "Epoch 39/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.5364 - acc: 0.3778 - val_loss: 1.6687 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.66063\n",
      "Epoch 40/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5229 - acc: 0.4000 - val_loss: 1.6707 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.66063\n",
      "Epoch 41/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5215 - acc: 0.4889 - val_loss: 1.6699 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.66063\n",
      "Epoch 42/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.5170 - acc: 0.4889 - val_loss: 1.6746 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.66063\n",
      "Epoch 43/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5115 - acc: 0.4222 - val_loss: 1.6474 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.66063 to 1.64745, saving model to ./model/vloss1.647_vacc0.400_ep43.hdf5\n",
      "Epoch 44/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.5013 - acc: 0.4222 - val_loss: 1.6495 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.64745\n",
      "Epoch 45/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4940 - acc: 0.4889 - val_loss: 1.6694 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.64745\n",
      "Epoch 46/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.5029 - acc: 0.4889 - val_loss: 1.6792 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.64745\n",
      "Epoch 47/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4880 - acc: 0.4889 - val_loss: 1.6487 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.64745\n",
      "Epoch 48/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4888 - acc: 0.4889 - val_loss: 1.6449 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.64745 to 1.64491, saving model to ./model/vloss1.645_vacc0.600_ep48.hdf5\n",
      "Epoch 49/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4874 - acc: 0.4667 - val_loss: 1.6501 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.64491\n",
      "Epoch 50/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4793 - acc: 0.4889 - val_loss: 1.6446 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.64491 to 1.64458, saving model to ./model/vloss1.645_vacc0.600_ep50.hdf5\n",
      "Epoch 51/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.4746 - acc: 0.4667 - val_loss: 1.6513 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.64458\n",
      "Epoch 52/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.4719 - acc: 0.5111 - val_loss: 1.6609 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.64458\n",
      "Epoch 53/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4699 - acc: 0.5111 - val_loss: 1.6459 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.64458\n",
      "Epoch 54/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4648 - acc: 0.5111 - val_loss: 1.6267 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.64458 to 1.62672, saving model to ./model/vloss1.627_vacc0.400_ep54.hdf5\n",
      "Epoch 55/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4586 - acc: 0.4222 - val_loss: 1.6180 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.62672 to 1.61801, saving model to ./model/vloss1.618_vacc0.400_ep55.hdf5\n",
      "Epoch 56/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4525 - acc: 0.4667 - val_loss: 1.6046 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.61801 to 1.60464, saving model to ./model/vloss1.605_vacc0.600_ep56.hdf5\n",
      "Epoch 57/2000\n",
      "45/45 [==============================] - 0s 222us/step - loss: 1.4548 - acc: 0.4889 - val_loss: 1.6322 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.60464\n",
      "Epoch 58/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.4378 - acc: 0.4889 - val_loss: 1.6299 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.60464\n",
      "Epoch 59/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4470 - acc: 0.5111 - val_loss: 1.6490 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.60464\n",
      "Epoch 60/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4404 - acc: 0.5111 - val_loss: 1.6274 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.60464\n",
      "Epoch 61/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.4330 - acc: 0.5111 - val_loss: 1.6125 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.60464\n",
      "Epoch 62/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.4310 - acc: 0.5111 - val_loss: 1.6160 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.60464\n",
      "Epoch 63/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 1.4291 - acc: 0.5111 - val_loss: 1.6118 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.60464\n",
      "Epoch 64/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.4165 - acc: 0.5111 - val_loss: 1.6167 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.60464\n",
      "Epoch 65/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.4197 - acc: 0.5111 - val_loss: 1.6251 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.60464\n",
      "Epoch 66/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4193 - acc: 0.4889 - val_loss: 1.5941 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.60464 to 1.59414, saving model to ./model/vloss1.594_vacc0.600_ep66.hdf5\n",
      "Epoch 67/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4130 - acc: 0.4667 - val_loss: 1.6195 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.59414\n",
      "Epoch 68/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4090 - acc: 0.4444 - val_loss: 1.6193 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.59414\n",
      "Epoch 69/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4039 - acc: 0.4889 - val_loss: 1.5879 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.59414 to 1.58792, saving model to ./model/vloss1.588_vacc0.600_ep69.hdf5\n",
      "Epoch 70/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4037 - acc: 0.4889 - val_loss: 1.5790 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00070: val_loss improved from 1.58792 to 1.57904, saving model to ./model/vloss1.579_vacc0.600_ep70.hdf5\n",
      "Epoch 71/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.3946 - acc: 0.5111 - val_loss: 1.6165 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.57904\n",
      "Epoch 72/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.4023 - acc: 0.5111 - val_loss: 1.6074 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.57904\n",
      "Epoch 73/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.4102 - acc: 0.4444 - val_loss: 1.5802 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.57904\n",
      "Epoch 74/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.3921 - acc: 0.4667 - val_loss: 1.5712 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00074: val_loss improved from 1.57904 to 1.57123, saving model to ./model/vloss1.571_vacc0.600_ep74.hdf5\n",
      "Epoch 75/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3938 - acc: 0.5333 - val_loss: 1.6088 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.57123\n",
      "Epoch 76/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3838 - acc: 0.5556 - val_loss: 1.5773 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.57123\n",
      "Epoch 77/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 266us/step - loss: 1.3771 - acc: 0.4889 - val_loss: 1.5779 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.57123\n",
      "Epoch 78/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3712 - acc: 0.5111 - val_loss: 1.5882 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.57123\n",
      "Epoch 79/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.3750 - acc: 0.5556 - val_loss: 1.5677 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00079: val_loss improved from 1.57123 to 1.56769, saving model to ./model/vloss1.568_vacc0.600_ep79.hdf5\n",
      "Epoch 80/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 1.3678 - acc: 0.5778 - val_loss: 1.5900 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.56769\n",
      "Epoch 81/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3654 - acc: 0.5778 - val_loss: 1.5680 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.56769\n",
      "Epoch 82/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.3600 - acc: 0.5556 - val_loss: 1.5723 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.56769\n",
      "Epoch 83/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3644 - acc: 0.4889 - val_loss: 1.5485 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00083: val_loss improved from 1.56769 to 1.54851, saving model to ./model/vloss1.549_vacc0.600_ep83.hdf5\n",
      "Epoch 84/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 1.3519 - acc: 0.4889 - val_loss: 1.5426 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00084: val_loss improved from 1.54851 to 1.54257, saving model to ./model/vloss1.543_vacc0.600_ep84.hdf5\n",
      "Epoch 85/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3531 - acc: 0.5778 - val_loss: 1.5423 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.54257 to 1.54229, saving model to ./model/vloss1.542_vacc0.600_ep85.hdf5\n",
      "Epoch 86/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 1.3512 - acc: 0.5778 - val_loss: 1.5764 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.54229\n",
      "Epoch 87/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.3464 - acc: 0.5556 - val_loss: 1.5394 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.54229 to 1.53935, saving model to ./model/vloss1.539_vacc0.600_ep87.hdf5\n",
      "Epoch 88/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3400 - acc: 0.5333 - val_loss: 1.5324 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.53935 to 1.53244, saving model to ./model/vloss1.532_vacc0.600_ep88.hdf5\n",
      "Epoch 89/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.3498 - acc: 0.5556 - val_loss: 1.5384 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.53244\n",
      "Epoch 90/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.3396 - acc: 0.5556 - val_loss: 1.5214 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.53244 to 1.52138, saving model to ./model/vloss1.521_vacc0.600_ep90.hdf5\n",
      "Epoch 91/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.3328 - acc: 0.5556 - val_loss: 1.5321 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.52138\n",
      "Epoch 92/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3301 - acc: 0.5556 - val_loss: 1.5570 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.52138\n",
      "Epoch 93/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3331 - acc: 0.5778 - val_loss: 1.5240 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.52138\n",
      "Epoch 94/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3414 - acc: 0.5333 - val_loss: 1.5084 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.52138 to 1.50844, saving model to ./model/vloss1.508_vacc0.600_ep94.hdf5\n",
      "Epoch 95/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.3200 - acc: 0.5111 - val_loss: 1.5044 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.50844 to 1.50440, saving model to ./model/vloss1.504_vacc0.600_ep95.hdf5\n",
      "Epoch 96/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3269 - acc: 0.5778 - val_loss: 1.5429 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.50440\n",
      "Epoch 97/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.3275 - acc: 0.5556 - val_loss: 1.4884 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00097: val_loss improved from 1.50440 to 1.48843, saving model to ./model/vloss1.488_vacc0.400_ep97.hdf5\n",
      "Epoch 98/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3229 - acc: 0.5333 - val_loss: 1.4868 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00098: val_loss improved from 1.48843 to 1.48676, saving model to ./model/vloss1.487_vacc0.600_ep98.hdf5\n",
      "Epoch 99/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.3108 - acc: 0.4889 - val_loss: 1.4948 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.48676\n",
      "Epoch 100/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.3107 - acc: 0.5556 - val_loss: 1.4816 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00100: val_loss improved from 1.48676 to 1.48164, saving model to ./model/vloss1.482_vacc0.600_ep100.hdf5\n",
      "Epoch 101/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3710 - acc: 0.600 - 0s 377us/step - loss: 1.3052 - acc: 0.5778 - val_loss: 1.5050 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.48164\n",
      "Epoch 102/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2977 - acc: 0.5778 - val_loss: 1.4997 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.48164\n",
      "Epoch 103/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.3028 - acc: 0.5333 - val_loss: 1.4664 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00103: val_loss improved from 1.48164 to 1.46645, saving model to ./model/vloss1.466_vacc0.600_ep103.hdf5\n",
      "Epoch 104/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2949 - acc: 0.5333 - val_loss: 1.4700 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.46645\n",
      "Epoch 105/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.2949 - acc: 0.5778 - val_loss: 1.4784 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.46645\n",
      "Epoch 106/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2954 - acc: 0.5778 - val_loss: 1.4779 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.46645\n",
      "Epoch 107/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2842 - acc: 0.5333 - val_loss: 1.4621 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00107: val_loss improved from 1.46645 to 1.46210, saving model to ./model/vloss1.462_vacc0.400_ep107.hdf5\n",
      "Epoch 108/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2878 - acc: 0.5111 - val_loss: 1.4532 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00108: val_loss improved from 1.46210 to 1.45319, saving model to ./model/vloss1.453_vacc0.600_ep108.hdf5\n",
      "Epoch 109/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.2782 - acc: 0.5556 - val_loss: 1.4376 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00109: val_loss improved from 1.45319 to 1.43758, saving model to ./model/vloss1.438_vacc0.600_ep109.hdf5\n",
      "Epoch 110/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2724 - acc: 0.5556 - val_loss: 1.4255 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00110: val_loss improved from 1.43758 to 1.42551, saving model to ./model/vloss1.426_vacc0.600_ep110.hdf5\n",
      "Epoch 111/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2720 - acc: 0.4889 - val_loss: 1.4299 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.42551\n",
      "Epoch 112/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2717 - acc: 0.5556 - val_loss: 1.4523 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.42551\n",
      "Epoch 113/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2700 - acc: 0.5111 - val_loss: 1.4369 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.42551\n",
      "Epoch 114/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2695 - acc: 0.5556 - val_loss: 1.4610 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.42551\n",
      "Epoch 115/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2820 - acc: 0.5778 - val_loss: 1.4473 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.42551\n",
      "Epoch 116/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2560 - acc: 0.5556 - val_loss: 1.4018 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00116: val_loss improved from 1.42551 to 1.40181, saving model to ./model/vloss1.402_vacc0.400_ep116.hdf5\n",
      "Epoch 117/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2623 - acc: 0.4889 - val_loss: 1.4221 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.40181\n",
      "Epoch 118/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2554 - acc: 0.5333 - val_loss: 1.4367 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.40181\n",
      "Epoch 119/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2494 - acc: 0.5556 - val_loss: 1.4091 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.40181\n",
      "Epoch 120/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2582 - acc: 0.5556 - val_loss: 1.4243 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.40181\n",
      "Epoch 121/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2667 - acc: 0.5333 - val_loss: 1.3969 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00121: val_loss improved from 1.40181 to 1.39686, saving model to ./model/vloss1.397_vacc0.400_ep121.hdf5\n",
      "Epoch 122/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2477 - acc: 0.5556 - val_loss: 1.4042 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.39686\n",
      "Epoch 123/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2333 - acc: 0.5556 - val_loss: 1.4123 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.39686\n",
      "Epoch 124/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2339 - acc: 0.5556 - val_loss: 1.3905 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00124: val_loss improved from 1.39686 to 1.39052, saving model to ./model/vloss1.391_vacc0.400_ep124.hdf5\n",
      "Epoch 125/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2292 - acc: 0.5333 - val_loss: 1.3539 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00125: val_loss improved from 1.39052 to 1.35388, saving model to ./model/vloss1.354_vacc0.600_ep125.hdf5\n",
      "Epoch 126/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 1.2346 - acc: 0.5333 - val_loss: 1.3793 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.35388\n",
      "Epoch 127/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2261 - acc: 0.5556 - val_loss: 1.3774 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.35388\n",
      "Epoch 128/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2339 - acc: 0.5111 - val_loss: 1.3711 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.35388\n",
      "Epoch 129/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.2199 - acc: 0.4444 - val_loss: 1.3575 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.35388\n",
      "Epoch 130/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2132 - acc: 0.5111 - val_loss: 1.3617 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.35388\n",
      "Epoch 131/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2268 - acc: 0.5111 - val_loss: 1.3671 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.35388\n",
      "Epoch 132/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2035 - acc: 0.5778 - val_loss: 1.3647 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.35388\n",
      "Epoch 133/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.2197 - acc: 0.5778 - val_loss: 1.3492 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00133: val_loss improved from 1.35388 to 1.34918, saving model to ./model/vloss1.349_vacc0.400_ep133.hdf5\n",
      "Epoch 134/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.2206 - acc: 0.5333 - val_loss: 1.3397 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00134: val_loss improved from 1.34918 to 1.33971, saving model to ./model/vloss1.340_vacc0.600_ep134.hdf5\n",
      "Epoch 135/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2059 - acc: 0.5333 - val_loss: 1.3862 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.33971\n",
      "Epoch 136/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2083 - acc: 0.5556 - val_loss: 1.3645 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.33971\n",
      "Epoch 137/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1974 - acc: 0.5556 - val_loss: 1.3026 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00137: val_loss improved from 1.33971 to 1.30259, saving model to ./model/vloss1.303_vacc0.400_ep137.hdf5\n",
      "Epoch 138/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.2001 - acc: 0.5111 - val_loss: 1.3189 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.30259\n",
      "Epoch 139/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6480 - acc: 0.400 - 0s 310us/step - loss: 1.1973 - acc: 0.6000 - val_loss: 1.3575 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.30259\n",
      "Epoch 140/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0587 - acc: 0.600 - 0s 310us/step - loss: 1.1987 - acc: 0.5778 - val_loss: 1.3014 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00140: val_loss improved from 1.30259 to 1.30144, saving model to ./model/vloss1.301_vacc0.400_ep140.hdf5\n",
      "Epoch 141/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1957 - acc: 0.5333 - val_loss: 1.3050 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.30144\n",
      "Epoch 142/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.2233 - acc: 0.4889 - val_loss: 1.3193 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.30144\n",
      "Epoch 143/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.1819 - acc: 0.5111 - val_loss: 1.3082 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.30144\n",
      "Epoch 144/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 1.1811 - acc: 0.5333 - val_loss: 1.2944 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.30144 to 1.29439, saving model to ./model/vloss1.294_vacc0.400_ep144.hdf5\n",
      "Epoch 145/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1933 - acc: 0.5778 - val_loss: 1.3067 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.29439\n",
      "Epoch 146/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1794 - acc: 0.5556 - val_loss: 1.2833 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00146: val_loss improved from 1.29439 to 1.28332, saving model to ./model/vloss1.283_vacc0.400_ep146.hdf5\n",
      "Epoch 147/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.1803 - acc: 0.4889 - val_loss: 1.2684 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00147: val_loss improved from 1.28332 to 1.26836, saving model to ./model/vloss1.268_vacc0.400_ep147.hdf5\n",
      "Epoch 148/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1714 - acc: 0.4889 - val_loss: 1.3178 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.26836\n",
      "Epoch 149/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1631 - acc: 0.6000 - val_loss: 1.2889 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.26836\n",
      "Epoch 150/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1698 - acc: 0.5778 - val_loss: 1.2564 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00150: val_loss improved from 1.26836 to 1.25636, saving model to ./model/vloss1.256_vacc0.400_ep150.hdf5\n",
      "Epoch 151/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1668 - acc: 0.5556 - val_loss: 1.2663 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.25636\n",
      "Epoch 152/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.1732 - acc: 0.4667 - val_loss: 1.2469 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00152: val_loss improved from 1.25636 to 1.24693, saving model to ./model/vloss1.247_vacc0.400_ep152.hdf5\n",
      "Epoch 153/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1682 - acc: 0.5778 - val_loss: 1.2849 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.24693\n",
      "Epoch 154/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 266us/step - loss: 1.1526 - acc: 0.5778 - val_loss: 1.2579 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.24693\n",
      "Epoch 155/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1627 - acc: 0.5111 - val_loss: 1.2695 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.24693\n",
      "Epoch 156/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1589 - acc: 0.5333 - val_loss: 1.2462 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00156: val_loss improved from 1.24693 to 1.24622, saving model to ./model/vloss1.246_vacc0.400_ep156.hdf5\n",
      "Epoch 157/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 1.1419 - acc: 0.5333 - val_loss: 1.2423 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00157: val_loss improved from 1.24622 to 1.24229, saving model to ./model/vloss1.242_vacc0.400_ep157.hdf5\n",
      "Epoch 158/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 1.1371 - acc: 0.5556 - val_loss: 1.2478 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.24229\n",
      "Epoch 159/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.1356 - acc: 0.5556 - val_loss: 1.2044 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00159: val_loss improved from 1.24229 to 1.20443, saving model to ./model/vloss1.204_vacc0.400_ep159.hdf5\n",
      "Epoch 160/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1386 - acc: 0.5111 - val_loss: 1.2321 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.20443\n",
      "Epoch 161/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1435 - acc: 0.5556 - val_loss: 1.2422 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.20443\n",
      "Epoch 162/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1315 - acc: 0.5778 - val_loss: 1.2231 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.20443\n",
      "Epoch 163/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.1514 - acc: 0.5333 - val_loss: 1.2259 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.20443\n",
      "Epoch 164/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 1.1248 - acc: 0.5333 - val_loss: 1.2024 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00164: val_loss improved from 1.20443 to 1.20239, saving model to ./model/vloss1.202_vacc0.400_ep164.hdf5\n",
      "Epoch 165/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1297 - acc: 0.5556 - val_loss: 1.2433 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.20239\n",
      "Epoch 166/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1153 - acc: 0.6222 - val_loss: 1.2004 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00166: val_loss improved from 1.20239 to 1.20045, saving model to ./model/vloss1.200_vacc0.400_ep166.hdf5\n",
      "Epoch 167/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1227 - acc: 0.5778 - val_loss: 1.2018 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.20045\n",
      "Epoch 168/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1201 - acc: 0.5778 - val_loss: 1.1876 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00168: val_loss improved from 1.20045 to 1.18762, saving model to ./model/vloss1.188_vacc0.400_ep168.hdf5\n",
      "Epoch 169/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.1164 - acc: 0.5556 - val_loss: 1.1677 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00169: val_loss improved from 1.18762 to 1.16769, saving model to ./model/vloss1.168_vacc0.400_ep169.hdf5\n",
      "Epoch 170/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.1109 - acc: 0.5556 - val_loss: 1.2180 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.16769\n",
      "Epoch 171/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.1150 - acc: 0.5778 - val_loss: 1.2059 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.16769\n",
      "Epoch 172/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.1130 - acc: 0.5556 - val_loss: 1.1195 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00172: val_loss improved from 1.16769 to 1.11950, saving model to ./model/vloss1.120_vacc0.400_ep172.hdf5\n",
      "Epoch 173/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 1.1086 - acc: 0.5778 - val_loss: 1.1715 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.11950\n",
      "Epoch 174/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.0963 - acc: 0.5778 - val_loss: 1.1553 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.11950\n",
      "Epoch 175/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.0977 - acc: 0.5556 - val_loss: 1.1463 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.11950\n",
      "Epoch 176/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0867 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.11950\n",
      "Epoch 177/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0880 - acc: 0.5778 - val_loss: 1.1541 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.11950\n",
      "Epoch 178/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0985 - acc: 0.6000 - val_loss: 1.1526 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.11950\n",
      "Epoch 179/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.0843 - acc: 0.6000 - val_loss: 1.1473 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.11950\n",
      "Epoch 180/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0849 - acc: 0.5778 - val_loss: 1.1383 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.11950\n",
      "Epoch 181/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0715 - acc: 0.5556 - val_loss: 1.1049 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00181: val_loss improved from 1.11950 to 1.10487, saving model to ./model/vloss1.105_vacc0.400_ep181.hdf5\n",
      "Epoch 182/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0852 - acc: 0.5333 - val_loss: 1.1163 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.10487\n",
      "Epoch 183/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0839 - acc: 0.5556 - val_loss: 1.1259 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.10487\n",
      "Epoch 184/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0686 - acc: 0.5778 - val_loss: 1.0833 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00184: val_loss improved from 1.10487 to 1.08329, saving model to ./model/vloss1.083_vacc0.400_ep184.hdf5\n",
      "Epoch 185/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.0632 - acc: 0.5778 - val_loss: 1.1014 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.08329\n",
      "Epoch 186/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0646 - acc: 0.6000 - val_loss: 1.0949 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.08329\n",
      "Epoch 187/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0664 - acc: 0.6000 - val_loss: 1.1077 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.08329\n",
      "Epoch 188/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0585 - acc: 0.6000 - val_loss: 1.1048 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.08329\n",
      "Epoch 189/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0617 - acc: 0.5778 - val_loss: 1.1107 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.08329\n",
      "Epoch 190/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0533 - acc: 0.6000 - val_loss: 1.0884 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.08329\n",
      "Epoch 191/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0588 - acc: 0.6000 - val_loss: 1.0777 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00191: val_loss improved from 1.08329 to 1.07774, saving model to ./model/vloss1.078_vacc0.400_ep191.hdf5\n",
      "Epoch 192/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.0535 - acc: 0.5333 - val_loss: 1.0540 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00192: val_loss improved from 1.07774 to 1.05402, saving model to ./model/vloss1.054_vacc0.400_ep192.hdf5\n",
      "Epoch 193/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0540 - acc: 0.6000 - val_loss: 1.0933 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.05402\n",
      "Epoch 194/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 332us/step - loss: 1.0469 - acc: 0.6000 - val_loss: 1.0821 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.05402\n",
      "Epoch 195/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.0607 - acc: 0.5556 - val_loss: 1.0300 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00195: val_loss improved from 1.05402 to 1.03002, saving model to ./model/vloss1.030_vacc0.600_ep195.hdf5\n",
      "Epoch 196/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0351 - acc: 0.5778 - val_loss: 1.0703 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.03002\n",
      "Epoch 197/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0375 - acc: 0.6000 - val_loss: 1.0708 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.03002\n",
      "Epoch 198/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0369 - acc: 0.6000 - val_loss: 1.0670 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.03002\n",
      "Epoch 199/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0326 - acc: 0.5778 - val_loss: 1.0065 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00199: val_loss improved from 1.03002 to 1.00646, saving model to ./model/vloss1.006_vacc0.400_ep199.hdf5\n",
      "Epoch 200/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0175 - acc: 0.5778 - val_loss: 1.0170 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.00646\n",
      "Epoch 201/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 1.0290 - acc: 0.6000 - val_loss: 1.0474 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.00646\n",
      "Epoch 202/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 1.0244 - acc: 0.6000 - val_loss: 1.0031 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00202: val_loss improved from 1.00646 to 1.00307, saving model to ./model/vloss1.003_vacc0.400_ep202.hdf5\n",
      "Epoch 203/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0112 - acc: 0.6222 - val_loss: 1.0235 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.00307\n",
      "Epoch 204/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 1.0245 - acc: 0.5778 - val_loss: 0.9776 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00204: val_loss improved from 1.00307 to 0.97755, saving model to ./model/vloss0.978_vacc0.400_ep204.hdf5\n",
      "Epoch 205/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0214 - acc: 0.5778 - val_loss: 1.0306 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.97755\n",
      "Epoch 206/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0157 - acc: 0.6000 - val_loss: 1.0260 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.97755\n",
      "Epoch 207/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 1.0082 - acc: 0.5778 - val_loss: 0.9609 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.97755 to 0.96088, saving model to ./model/vloss0.961_vacc0.400_ep207.hdf5\n",
      "Epoch 208/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 1.0128 - acc: 0.5778 - val_loss: 1.0038 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.96088\n",
      "Epoch 209/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 1.0088 - acc: 0.6000 - val_loss: 0.9955 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.96088\n",
      "Epoch 210/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 1.0035 - acc: 0.6000 - val_loss: 0.9841 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.96088\n",
      "Epoch 211/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9990 - acc: 0.6000 - val_loss: 0.9636 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.96088\n",
      "Epoch 212/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0088 - acc: 0.6222 - val_loss: 0.9960 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.96088\n",
      "Epoch 213/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 1.0264 - acc: 0.6000 - val_loss: 0.9603 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.96088 to 0.96032, saving model to ./model/vloss0.960_vacc0.400_ep213.hdf5\n",
      "Epoch 214/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9858 - acc: 0.6222 - val_loss: 0.9630 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.96032\n",
      "Epoch 215/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9948 - acc: 0.6000 - val_loss: 0.9432 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.96032 to 0.94320, saving model to ./model/vloss0.943_vacc0.400_ep215.hdf5\n",
      "Epoch 216/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.9943 - acc: 0.6000 - val_loss: 0.9407 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.94320 to 0.94074, saving model to ./model/vloss0.941_vacc0.600_ep216.hdf5\n",
      "Epoch 217/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9840 - acc: 0.6222 - val_loss: 0.9217 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.94074 to 0.92174, saving model to ./model/vloss0.922_vacc0.400_ep217.hdf5\n",
      "Epoch 218/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.9769 - acc: 0.6000 - val_loss: 0.9544 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.92174\n",
      "Epoch 219/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.9749 - acc: 0.6222 - val_loss: 0.9511 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.92174\n",
      "Epoch 220/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9758 - acc: 0.5778 - val_loss: 0.8955 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.92174 to 0.89553, saving model to ./model/vloss0.896_vacc0.600_ep220.hdf5\n",
      "Epoch 221/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.9726 - acc: 0.6000 - val_loss: 0.9250 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.89553\n",
      "Epoch 222/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9830 - acc: 0.6000 - val_loss: 0.9345 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.89553\n",
      "Epoch 223/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9732 - acc: 0.6222 - val_loss: 0.9548 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.89553\n",
      "Epoch 224/2000\n",
      "45/45 [==============================] - 0s 576us/step - loss: 0.9581 - acc: 0.6444 - val_loss: 0.8973 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.89553\n",
      "Epoch 225/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9709 - acc: 0.6222 - val_loss: 0.8568 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.89553 to 0.85678, saving model to ./model/vloss0.857_vacc0.600_ep225.hdf5\n",
      "Epoch 226/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9620 - acc: 0.6222 - val_loss: 0.9318 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.85678\n",
      "Epoch 227/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9761 - acc: 0.6444 - val_loss: 0.9331 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.85678\n",
      "Epoch 228/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9556 - acc: 0.6444 - val_loss: 0.8762 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.85678\n",
      "Epoch 229/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9698 - acc: 0.5556 - val_loss: 0.8603 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.85678\n",
      "Epoch 230/2000\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.9693 - acc: 0.6444 - val_loss: 0.9231 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.85678\n",
      "Epoch 231/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9510 - acc: 0.6444 - val_loss: 0.8873 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.85678\n",
      "Epoch 232/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.9400 - acc: 0.6444 - val_loss: 0.8852 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.85678\n",
      "Epoch 233/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.9487 - acc: 0.6222 - val_loss: 0.8655 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.85678\n",
      "Epoch 234/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9423 - acc: 0.6222 - val_loss: 0.8983 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.85678\n",
      "Epoch 235/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9419 - acc: 0.6000 - val_loss: 0.8936 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.85678\n",
      "Epoch 236/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9599 - acc: 0.6000 - val_loss: 0.8895 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.85678\n",
      "Epoch 237/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.9356 - acc: 0.6444 - val_loss: 0.9105 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.85678\n",
      "Epoch 238/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9336 - acc: 0.6444 - val_loss: 0.8440 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.85678 to 0.84397, saving model to ./model/vloss0.844_vacc0.400_ep238.hdf5\n",
      "Epoch 239/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9345 - acc: 0.6444 - val_loss: 0.8552 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.84397\n",
      "Epoch 240/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9156 - acc: 0.6444 - val_loss: 0.8616 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.84397\n",
      "Epoch 241/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9224 - acc: 0.6222 - val_loss: 0.8402 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.84397 to 0.84025, saving model to ./model/vloss0.840_vacc0.600_ep241.hdf5\n",
      "Epoch 242/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.9336 - acc: 0.6444 - val_loss: 0.8743 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.84025\n",
      "Epoch 243/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9143 - acc: 0.6444 - val_loss: 0.8400 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.84025 to 0.83995, saving model to ./model/vloss0.840_vacc0.600_ep243.hdf5\n",
      "Epoch 244/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9146 - acc: 0.6444 - val_loss: 0.8368 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.83995 to 0.83678, saving model to ./model/vloss0.837_vacc0.600_ep244.hdf5\n",
      "Epoch 245/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9135 - acc: 0.6444 - val_loss: 0.8655 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.83678\n",
      "Epoch 246/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.9308 - acc: 0.6444 - val_loss: 0.8523 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.83678\n",
      "Epoch 247/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.9093 - acc: 0.6222 - val_loss: 0.8153 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.83678 to 0.81532, saving model to ./model/vloss0.815_vacc0.600_ep247.hdf5\n",
      "Epoch 248/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9028 - acc: 0.6444 - val_loss: 0.8083 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.81532 to 0.80828, saving model to ./model/vloss0.808_vacc0.600_ep248.hdf5\n",
      "Epoch 249/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.9099 - acc: 0.6444 - val_loss: 0.8297 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.80828\n",
      "Epoch 250/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9047 - acc: 0.6444 - val_loss: 0.8275 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.80828\n",
      "Epoch 251/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.9085 - acc: 0.6444 - val_loss: 0.8235 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.80828\n",
      "Epoch 252/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9000 - acc: 0.6222 - val_loss: 0.8116 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.80828\n",
      "Epoch 253/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8959 - acc: 0.6444 - val_loss: 0.8233 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.80828\n",
      "Epoch 254/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9143 - acc: 0.6667 - val_loss: 0.8263 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.80828\n",
      "Epoch 255/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.9025 - acc: 0.6444 - val_loss: 0.7857 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.80828 to 0.78572, saving model to ./model/vloss0.786_vacc0.600_ep255.hdf5\n",
      "Epoch 256/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8899 - acc: 0.6222 - val_loss: 0.7796 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.78572 to 0.77957, saving model to ./model/vloss0.780_vacc0.600_ep256.hdf5\n",
      "Epoch 257/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8859 - acc: 0.6444 - val_loss: 0.7965 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.77957\n",
      "Epoch 258/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.9056 - acc: 0.6444 - val_loss: 0.7999 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.77957\n",
      "Epoch 259/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.8853 - acc: 0.6667 - val_loss: 0.8082 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.77957\n",
      "Epoch 260/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.8830 - acc: 0.6444 - val_loss: 0.7874 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.77957\n",
      "Epoch 261/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.8735 - acc: 0.6444 - val_loss: 0.7941 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.77957\n",
      "Epoch 262/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8924 - acc: 0.6222 - val_loss: 0.7726 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.77957 to 0.77261, saving model to ./model/vloss0.773_vacc0.600_ep262.hdf5\n",
      "Epoch 263/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8648 - acc: 0.6667 - val_loss: 0.8046 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.77261\n",
      "Epoch 264/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8768 - acc: 0.6444 - val_loss: 0.7617 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.77261 to 0.76170, saving model to ./model/vloss0.762_vacc0.600_ep264.hdf5\n",
      "Epoch 265/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8684 - acc: 0.6667 - val_loss: 0.7859 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.76170\n",
      "Epoch 266/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8707 - acc: 0.6667 - val_loss: 0.7630 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.76170\n",
      "Epoch 267/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8843 - acc: 0.6000 - val_loss: 0.7330 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.76170 to 0.73300, saving model to ./model/vloss0.733_vacc0.600_ep267.hdf5\n",
      "Epoch 268/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8560 - acc: 0.6444 - val_loss: 0.7840 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.73300\n",
      "Epoch 269/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8584 - acc: 0.6444 - val_loss: 0.8102 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.73300\n",
      "Epoch 270/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8719 - acc: 0.6667 - val_loss: 0.7699 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.73300\n",
      "Epoch 271/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8654 - acc: 0.6444 - val_loss: 0.7477 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.73300\n",
      "Epoch 272/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.8557 - acc: 0.6444 - val_loss: 0.7440 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.73300\n",
      "Epoch 273/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8485 - acc: 0.6444 - val_loss: 0.7635 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.73300\n",
      "Epoch 274/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 288us/step - loss: 0.8657 - acc: 0.6444 - val_loss: 0.7708 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.73300\n",
      "Epoch 275/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.8528 - acc: 0.6444 - val_loss: 0.7363 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.73300\n",
      "Epoch 276/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8444 - acc: 0.6444 - val_loss: 0.7490 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.73300\n",
      "Epoch 277/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8375 - acc: 0.6667 - val_loss: 0.7492 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.73300\n",
      "Epoch 278/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8390 - acc: 0.6667 - val_loss: 0.7465 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.73300\n",
      "Epoch 279/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8349 - acc: 0.6667 - val_loss: 0.7800 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.73300\n",
      "Epoch 280/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.8326 - acc: 0.6667 - val_loss: 0.7374 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.73300\n",
      "Epoch 281/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8594 - acc: 0.6222 - val_loss: 0.7200 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.73300 to 0.71996, saving model to ./model/vloss0.720_vacc0.600_ep281.hdf5\n",
      "Epoch 282/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.8347 - acc: 0.6667 - val_loss: 0.7063 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.71996 to 0.70632, saving model to ./model/vloss0.706_vacc0.800_ep282.hdf5\n",
      "Epoch 283/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8391 - acc: 0.6889 - val_loss: 0.7344 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.70632\n",
      "Epoch 284/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8346 - acc: 0.6889 - val_loss: 0.7721 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.70632\n",
      "Epoch 285/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.8258 - acc: 0.6667 - val_loss: 0.7047 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.70632 to 0.70471, saving model to ./model/vloss0.705_vacc0.800_ep285.hdf5\n",
      "Epoch 286/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8367 - acc: 0.6444 - val_loss: 0.6880 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.70471 to 0.68796, saving model to ./model/vloss0.688_vacc0.600_ep286.hdf5\n",
      "Epoch 287/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8271 - acc: 0.6667 - val_loss: 0.6732 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.68796 to 0.67319, saving model to ./model/vloss0.673_vacc0.800_ep287.hdf5\n",
      "Epoch 288/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8280 - acc: 0.6667 - val_loss: 0.7299 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.67319\n",
      "Epoch 289/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.8139 - acc: 0.6667 - val_loss: 0.6996 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.67319\n",
      "Epoch 290/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.8190 - acc: 0.6889 - val_loss: 0.6949 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.67319\n",
      "Epoch 291/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8175 - acc: 0.6889 - val_loss: 0.7179 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.67319\n",
      "Epoch 292/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.8178 - acc: 0.6444 - val_loss: 0.6831 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.67319\n",
      "Epoch 293/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.8226 - acc: 0.6889 - val_loss: 0.7386 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.67319\n",
      "Epoch 294/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8087 - acc: 0.6889 - val_loss: 0.6871 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.67319\n",
      "Epoch 295/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.8443 - acc: 0.6444 - val_loss: 0.6628 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.67319 to 0.66279, saving model to ./model/vloss0.663_vacc0.800_ep295.hdf5\n",
      "Epoch 296/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8145 - acc: 0.6889 - val_loss: 0.7333 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.66279\n",
      "Epoch 297/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.8011 - acc: 0.6444 - val_loss: 0.6695 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.66279\n",
      "Epoch 298/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.8029 - acc: 0.6444 - val_loss: 0.6793 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.66279\n",
      "Epoch 299/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.7961 - acc: 0.6889 - val_loss: 0.7121 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.66279\n",
      "Epoch 300/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.8006 - acc: 0.6889 - val_loss: 0.6860 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.66279\n",
      "Epoch 301/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.7916 - acc: 0.6889 - val_loss: 0.6490 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.66279 to 0.64899, saving model to ./model/vloss0.649_vacc0.800_ep301.hdf5\n",
      "Epoch 302/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7974 - acc: 0.6667 - val_loss: 0.6647 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.64899\n",
      "Epoch 303/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7990 - acc: 0.6667 - val_loss: 0.6703 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.64899\n",
      "Epoch 304/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7886 - acc: 0.7111 - val_loss: 0.6617 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.64899\n",
      "Epoch 305/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7916 - acc: 0.6667 - val_loss: 0.6839 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.64899\n",
      "Epoch 306/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7788 - acc: 0.7111 - val_loss: 0.6593 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.64899\n",
      "Epoch 307/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7810 - acc: 0.7111 - val_loss: 0.6964 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.64899\n",
      "Epoch 308/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7902 - acc: 0.7111 - val_loss: 0.6623 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.64899\n",
      "Epoch 309/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.7871 - acc: 0.7111 - val_loss: 0.6474 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.64899 to 0.64745, saving model to ./model/vloss0.647_vacc0.800_ep309.hdf5\n",
      "Epoch 310/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7737 - acc: 0.7111 - val_loss: 0.6485 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.64745\n",
      "Epoch 311/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7910 - acc: 0.7111 - val_loss: 0.6444 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.64745 to 0.64445, saving model to ./model/vloss0.644_vacc0.800_ep311.hdf5\n",
      "Epoch 312/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7942 - acc: 0.6667 - val_loss: 0.6202 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.64445 to 0.62024, saving model to ./model/vloss0.620_vacc0.800_ep312.hdf5\n",
      "Epoch 313/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7683 - acc: 0.7111 - val_loss: 0.6768 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.62024\n",
      "Epoch 314/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7870 - acc: 0.7111 - val_loss: 0.6582 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.62024\n",
      "Epoch 315/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7669 - acc: 0.7111 - val_loss: 0.6404 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.62024\n",
      "Epoch 316/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7717 - acc: 0.7111 - val_loss: 0.6202 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.62024 to 0.62018, saving model to ./model/vloss0.620_vacc0.800_ep316.hdf5\n",
      "Epoch 317/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7643 - acc: 0.7111 - val_loss: 0.6472 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.62018\n",
      "Epoch 318/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.7887 - acc: 0.6667 - val_loss: 0.6381 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.62018\n",
      "Epoch 319/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.7604 - acc: 0.7111 - val_loss: 0.6598 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.62018\n",
      "Epoch 320/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7702 - acc: 0.7111 - val_loss: 0.6016 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.62018 to 0.60160, saving model to ./model/vloss0.602_vacc0.800_ep320.hdf5\n",
      "Epoch 321/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7680 - acc: 0.7111 - val_loss: 0.6144 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.60160\n",
      "Epoch 322/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7574 - acc: 0.7111 - val_loss: 0.6403 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.60160\n",
      "Epoch 323/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7641 - acc: 0.7111 - val_loss: 0.6119 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.60160\n",
      "Epoch 324/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7459 - acc: 0.7333 - val_loss: 0.6316 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.60160\n",
      "Epoch 325/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7674 - acc: 0.6889 - val_loss: 0.6197 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.60160\n",
      "Epoch 326/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7542 - acc: 0.7111 - val_loss: 0.6464 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.60160\n",
      "Epoch 327/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.7451 - acc: 0.7111 - val_loss: 0.6021 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.60160\n",
      "Epoch 328/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7788 - acc: 0.7111 - val_loss: 0.6487 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.60160\n",
      "Epoch 329/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7588 - acc: 0.7111 - val_loss: 0.5488 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.60160 to 0.54881, saving model to ./model/vloss0.549_vacc1.000_ep329.hdf5\n",
      "Epoch 330/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7610 - acc: 0.6889 - val_loss: 0.6270 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.54881\n",
      "Epoch 331/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7441 - acc: 0.7333 - val_loss: 0.6003 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.54881\n",
      "Epoch 332/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7607 - acc: 0.6667 - val_loss: 0.5703 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.54881\n",
      "Epoch 333/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.7608 - acc: 0.7111 - val_loss: 0.6122 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.54881\n",
      "Epoch 334/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7397 - acc: 0.7111 - val_loss: 0.5965 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.54881\n",
      "Epoch 335/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7450 - acc: 0.7111 - val_loss: 0.5880 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.54881\n",
      "Epoch 336/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7297 - acc: 0.7111 - val_loss: 0.6098 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.54881\n",
      "Epoch 337/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7363 - acc: 0.7111 - val_loss: 0.6008 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.54881\n",
      "Epoch 338/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7299 - acc: 0.7333 - val_loss: 0.5788 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.54881\n",
      "Epoch 339/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7274 - acc: 0.7111 - val_loss: 0.5726 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.54881\n",
      "Epoch 340/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7491 - acc: 0.7333 - val_loss: 0.6175 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.54881\n",
      "Epoch 341/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7341 - acc: 0.7111 - val_loss: 0.5362 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.54881 to 0.53622, saving model to ./model/vloss0.536_vacc1.000_ep341.hdf5\n",
      "Epoch 342/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7345 - acc: 0.7333 - val_loss: 0.5662 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.53622\n",
      "Epoch 343/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7370 - acc: 0.6889 - val_loss: 0.5572 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.53622\n",
      "Epoch 344/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.7279 - acc: 0.7111 - val_loss: 0.5426 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.53622\n",
      "Epoch 345/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7134 - acc: 0.7111 - val_loss: 0.6132 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.53622\n",
      "Epoch 346/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.7392 - acc: 0.7333 - val_loss: 0.5982 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.53622\n",
      "Epoch 347/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7186 - acc: 0.7111 - val_loss: 0.5343 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.53622 to 0.53427, saving model to ./model/vloss0.534_vacc0.800_ep347.hdf5\n",
      "Epoch 348/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7203 - acc: 0.7111 - val_loss: 0.5458 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.53427\n",
      "Epoch 349/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7287 - acc: 0.7111 - val_loss: 0.5847 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.53427\n",
      "Epoch 350/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.7156 - acc: 0.7111 - val_loss: 0.5394 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.53427\n",
      "Epoch 351/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7153 - acc: 0.7556 - val_loss: 0.5425 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.53427\n",
      "Epoch 352/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7358 - acc: 0.7111 - val_loss: 0.5604 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.53427\n",
      "Epoch 353/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7220 - acc: 0.7111 - val_loss: 0.5627 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.53427\n",
      "Epoch 354/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7213 - acc: 0.7111 - val_loss: 0.5403 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.53427\n",
      "Epoch 355/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7089 - acc: 0.7111 - val_loss: 0.5064 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.53427 to 0.50636, saving model to ./model/vloss0.506_vacc1.000_ep355.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7068 - acc: 0.7111 - val_loss: 0.5283 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.50636\n",
      "Epoch 357/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7097 - acc: 0.6889 - val_loss: 0.5324 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.50636\n",
      "Epoch 358/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6959 - acc: 0.7111 - val_loss: 0.5569 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.50636\n",
      "Epoch 359/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7127 - acc: 0.7111 - val_loss: 0.5293 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.50636\n",
      "Epoch 360/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7081 - acc: 0.7333 - val_loss: 0.5511 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.50636\n",
      "Epoch 361/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.7048 - acc: 0.7333 - val_loss: 0.5508 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.50636\n",
      "Epoch 362/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.7008 - acc: 0.7333 - val_loss: 0.5210 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.50636\n",
      "Epoch 363/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.7238 - acc: 0.7556 - val_loss: 0.4943 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.50636 to 0.49428, saving model to ./model/vloss0.494_vacc1.000_ep363.hdf5\n",
      "Epoch 364/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.7025 - acc: 0.7778 - val_loss: 0.5790 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.49428\n",
      "Epoch 365/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.7092 - acc: 0.7556 - val_loss: 0.5410 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.49428\n",
      "Epoch 366/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6856 - acc: 0.7556 - val_loss: 0.5403 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.49428\n",
      "Epoch 367/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6909 - acc: 0.7333 - val_loss: 0.5156 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.49428\n",
      "Epoch 368/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.6921 - acc: 0.7333 - val_loss: 0.4786 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.49428 to 0.47861, saving model to ./model/vloss0.479_vacc1.000_ep368.hdf5\n",
      "Epoch 369/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.6907 - acc: 0.7333 - val_loss: 0.4756 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.47861 to 0.47557, saving model to ./model/vloss0.476_vacc1.000_ep369.hdf5\n",
      "Epoch 370/2000\n",
      "45/45 [==============================] - 0s 354us/step - loss: 0.6936 - acc: 0.7333 - val_loss: 0.5346 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.47557\n",
      "Epoch 371/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6924 - acc: 0.7333 - val_loss: 0.5562 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.47557\n",
      "Epoch 372/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6910 - acc: 0.7556 - val_loss: 0.5331 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.47557\n",
      "Epoch 373/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6835 - acc: 0.7556 - val_loss: 0.4842 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.47557\n",
      "Epoch 374/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6909 - acc: 0.7556 - val_loss: 0.5004 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.47557\n",
      "Epoch 375/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6923 - acc: 0.7111 - val_loss: 0.4412 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.47557 to 0.44125, saving model to ./model/vloss0.441_vacc0.800_ep375.hdf5\n",
      "Epoch 376/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6858 - acc: 0.7111 - val_loss: 0.5378 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.44125\n",
      "Epoch 377/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6771 - acc: 0.7556 - val_loss: 0.4817 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.44125\n",
      "Epoch 378/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6985 - acc: 0.7333 - val_loss: 0.5329 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.44125\n",
      "Epoch 379/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6743 - acc: 0.7778 - val_loss: 0.4753 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.44125\n",
      "Epoch 380/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6778 - acc: 0.7333 - val_loss: 0.5068 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.44125\n",
      "Epoch 381/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6980 - acc: 0.7333 - val_loss: 0.5057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.44125\n",
      "Epoch 382/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6585 - acc: 0.7556 - val_loss: 0.4874 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.44125\n",
      "Epoch 383/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.6768 - acc: 0.7333 - val_loss: 0.4764 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.44125\n",
      "Epoch 384/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6614 - acc: 0.7111 - val_loss: 0.4843 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.44125\n",
      "Epoch 385/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6686 - acc: 0.7778 - val_loss: 0.5169 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.44125\n",
      "Epoch 386/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6556 - acc: 0.7556 - val_loss: 0.4995 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.44125\n",
      "Epoch 387/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6651 - acc: 0.7333 - val_loss: 0.4542 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.44125\n",
      "Epoch 388/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6597 - acc: 0.7778 - val_loss: 0.4432 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.44125\n",
      "Epoch 389/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6474 - acc: 0.7111 - val_loss: 0.5032 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.44125\n",
      "Epoch 390/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5153 - acc: 1.000 - 0s 266us/step - loss: 0.6561 - acc: 0.7556 - val_loss: 0.4990 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.44125\n",
      "Epoch 391/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6523 - acc: 0.7556 - val_loss: 0.4763 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.44125\n",
      "Epoch 392/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6463 - acc: 0.7556 - val_loss: 0.4797 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.44125\n",
      "Epoch 393/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6529 - acc: 0.7556 - val_loss: 0.4549 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.44125\n",
      "Epoch 394/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.6542 - acc: 0.7333 - val_loss: 0.4671 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.44125\n",
      "Epoch 395/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6611 - acc: 0.7333 - val_loss: 0.4661 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.44125\n",
      "Epoch 396/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6539 - acc: 0.7556 - val_loss: 0.5005 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.44125\n",
      "Epoch 397/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.6430 - acc: 0.7556 - val_loss: 0.4401 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.44125 to 0.44011, saving model to ./model/vloss0.440_vacc1.000_ep397.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6579 - acc: 0.7333 - val_loss: 0.4982 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.44011\n",
      "Epoch 399/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6442 - acc: 0.7556 - val_loss: 0.4590 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.44011\n",
      "Epoch 400/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6456 - acc: 0.7556 - val_loss: 0.4177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.44011 to 0.41765, saving model to ./model/vloss0.418_vacc1.000_ep400.hdf5\n",
      "Epoch 401/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6496 - acc: 0.7333 - val_loss: 0.4312 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.41765\n",
      "Epoch 402/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6498 - acc: 0.7778 - val_loss: 0.4464 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.41765\n",
      "Epoch 403/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6456 - acc: 0.7333 - val_loss: 0.4618 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.41765\n",
      "Epoch 404/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6393 - acc: 0.7556 - val_loss: 0.4562 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.41765\n",
      "Epoch 405/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6401 - acc: 0.7333 - val_loss: 0.4346 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.41765\n",
      "Epoch 406/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6503 - acc: 0.7556 - val_loss: 0.4589 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.41765\n",
      "Epoch 407/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6344 - acc: 0.7556 - val_loss: 0.4287 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.41765\n",
      "Epoch 408/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6459 - acc: 0.7556 - val_loss: 0.4600 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.41765\n",
      "Epoch 409/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6370 - acc: 0.7778 - val_loss: 0.4027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.41765 to 0.40272, saving model to ./model/vloss0.403_vacc1.000_ep409.hdf5\n",
      "Epoch 410/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6447 - acc: 0.7778 - val_loss: 0.4297 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.40272\n",
      "Epoch 411/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6315 - acc: 0.7778 - val_loss: 0.4309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.40272\n",
      "Epoch 412/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6354 - acc: 0.7556 - val_loss: 0.4439 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.40272\n",
      "Epoch 413/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6256 - acc: 0.7556 - val_loss: 0.4414 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.40272\n",
      "Epoch 414/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6386 - acc: 0.7556 - val_loss: 0.4137 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.40272\n",
      "Epoch 415/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6457 - acc: 0.7778 - val_loss: 0.4224 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.40272\n",
      "Epoch 416/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.6212 - acc: 0.8222 - val_loss: 0.4822 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.40272\n",
      "Epoch 417/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6327 - acc: 0.7556 - val_loss: 0.4452 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.40272\n",
      "Epoch 418/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6388 - acc: 0.7778 - val_loss: 0.3718 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.40272 to 0.37182, saving model to ./model/vloss0.372_vacc1.000_ep418.hdf5\n",
      "Epoch 419/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6433 - acc: 0.7333 - val_loss: 0.4722 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.37182\n",
      "Epoch 420/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6279 - acc: 0.7556 - val_loss: 0.4177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.37182\n",
      "Epoch 421/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6197 - acc: 0.7778 - val_loss: 0.3934 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.37182\n",
      "Epoch 422/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6371 - acc: 0.7778 - val_loss: 0.4144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.37182\n",
      "Epoch 423/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6315 - acc: 0.7778 - val_loss: 0.4627 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.37182\n",
      "Epoch 424/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6156 - acc: 0.8000 - val_loss: 0.4045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.37182\n",
      "Epoch 425/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6162 - acc: 0.8000 - val_loss: 0.4462 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.37182\n",
      "Epoch 426/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6181 - acc: 0.7778 - val_loss: 0.4399 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.37182\n",
      "Epoch 427/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6066 - acc: 0.7778 - val_loss: 0.4486 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.37182\n",
      "Epoch 428/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6079 - acc: 0.7778 - val_loss: 0.4507 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.37182\n",
      "Epoch 429/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6239 - acc: 0.7778 - val_loss: 0.3717 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.37182 to 0.37172, saving model to ./model/vloss0.372_vacc1.000_ep429.hdf5\n",
      "Epoch 430/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.6232 - acc: 0.7778 - val_loss: 0.4145 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.37172\n",
      "Epoch 431/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6058 - acc: 0.8000 - val_loss: 0.4374 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.37172\n",
      "Epoch 432/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6228 - acc: 0.7556 - val_loss: 0.4225 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.37172\n",
      "Epoch 433/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.6062 - acc: 0.7778 - val_loss: 0.4155 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.37172\n",
      "Epoch 434/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5998 - acc: 0.7778 - val_loss: 0.3663 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.37172 to 0.36632, saving model to ./model/vloss0.366_vacc1.000_ep434.hdf5\n",
      "Epoch 435/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5982 - acc: 0.7778 - val_loss: 0.3999 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.36632\n",
      "Epoch 436/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.6204 - acc: 0.7333 - val_loss: 0.4689 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.36632\n",
      "Epoch 437/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6157 - acc: 0.7778 - val_loss: 0.3925 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.36632\n",
      "Epoch 438/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6037 - acc: 0.8000 - val_loss: 0.3511 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.36632 to 0.35107, saving model to ./model/vloss0.351_vacc1.000_ep438.hdf5\n",
      "Epoch 439/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.5886 - acc: 0.7778 - val_loss: 0.3900 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00439: val_loss did not improve from 0.35107\n",
      "Epoch 440/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6153 - acc: 0.7778 - val_loss: 0.4545 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.35107\n",
      "Epoch 441/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.5874 - acc: 0.7556 - val_loss: 0.3802 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.35107\n",
      "Epoch 442/2000\n",
      "45/45 [==============================] - 0s 643us/step - loss: 0.6047 - acc: 0.7778 - val_loss: 0.4016 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.35107\n",
      "Epoch 443/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.6014 - acc: 0.7778 - val_loss: 0.4186 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.35107\n",
      "Epoch 444/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5929 - acc: 0.7778 - val_loss: 0.3646 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.35107\n",
      "Epoch 445/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.6078 - acc: 0.7556 - val_loss: 0.4171 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.35107\n",
      "Epoch 446/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5835 - acc: 0.7556 - val_loss: 0.3414 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.35107 to 0.34143, saving model to ./model/vloss0.341_vacc1.000_ep446.hdf5\n",
      "Epoch 447/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5912 - acc: 0.7556 - val_loss: 0.3878 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.34143\n",
      "Epoch 448/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5845 - acc: 0.7556 - val_loss: 0.4177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.34143\n",
      "Epoch 449/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5885 - acc: 0.8000 - val_loss: 0.3677 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.34143\n",
      "Epoch 450/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5839 - acc: 0.7556 - val_loss: 0.4318 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.34143\n",
      "Epoch 451/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5806 - acc: 0.7778 - val_loss: 0.3710 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.34143\n",
      "Epoch 452/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5887 - acc: 0.7778 - val_loss: 0.3880 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.34143\n",
      "Epoch 453/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5764 - acc: 0.8000 - val_loss: 0.3659 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.34143\n",
      "Epoch 454/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5809 - acc: 0.7778 - val_loss: 0.3639 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.34143\n",
      "Epoch 455/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5936 - acc: 0.7778 - val_loss: 0.3269 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.34143 to 0.32691, saving model to ./model/vloss0.327_vacc1.000_ep455.hdf5\n",
      "Epoch 456/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.5998 - acc: 0.8000 - val_loss: 0.3976 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.32691\n",
      "Epoch 457/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5717 - acc: 0.8000 - val_loss: 0.3873 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.32691\n",
      "Epoch 458/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5927 - acc: 0.7778 - val_loss: 0.3464 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.32691\n",
      "Epoch 459/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5749 - acc: 0.7778 - val_loss: 0.4036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.32691\n",
      "Epoch 460/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5782 - acc: 0.7556 - val_loss: 0.3956 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.32691\n",
      "Epoch 461/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5683 - acc: 0.7778 - val_loss: 0.3456 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.32691\n",
      "Epoch 462/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5813 - acc: 0.7778 - val_loss: 0.3454 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.32691\n",
      "Epoch 463/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.6258 - acc: 0.7556 - val_loss: 0.3993 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.32691\n",
      "Epoch 464/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5898 - acc: 0.7778 - val_loss: 0.3444 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.32691\n",
      "Epoch 465/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.5775 - acc: 0.8000 - val_loss: 0.3806 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.32691\n",
      "Epoch 466/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.5863 - acc: 0.7778 - val_loss: 0.3466 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.32691\n",
      "Epoch 467/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.5678 - acc: 0.8000 - val_loss: 0.3967 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.32691\n",
      "Epoch 468/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5706 - acc: 0.8000 - val_loss: 0.3645 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.32691\n",
      "Epoch 469/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5721 - acc: 0.7778 - val_loss: 0.3912 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.32691\n",
      "Epoch 470/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.5614 - acc: 0.7778 - val_loss: 0.3356 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.32691\n",
      "Epoch 471/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5583 - acc: 0.7778 - val_loss: 0.3142 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.32691 to 0.31415, saving model to ./model/vloss0.314_vacc1.000_ep471.hdf5\n",
      "Epoch 472/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5659 - acc: 0.7778 - val_loss: 0.3469 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.31415\n",
      "Epoch 473/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5594 - acc: 0.7778 - val_loss: 0.3491 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.31415\n",
      "Epoch 474/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5558 - acc: 0.7778 - val_loss: 0.3560 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.31415\n",
      "Epoch 475/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5597 - acc: 0.8000 - val_loss: 0.3587 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.31415\n",
      "Epoch 476/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5776 - acc: 0.8000 - val_loss: 0.2885 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.31415 to 0.28849, saving model to ./model/vloss0.288_vacc1.000_ep476.hdf5\n",
      "Epoch 477/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5617 - acc: 0.7556 - val_loss: 0.4113 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.28849\n",
      "Epoch 478/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5576 - acc: 0.8000 - val_loss: 0.3509 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.28849\n",
      "Epoch 479/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5584 - acc: 0.8000 - val_loss: 0.3407 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.28849\n",
      "Epoch 480/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5601 - acc: 0.7556 - val_loss: 0.3691 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.28849\n",
      "Epoch 481/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5504 - acc: 0.8000 - val_loss: 0.3504 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.28849\n",
      "Epoch 482/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 288us/step - loss: 0.5526 - acc: 0.7778 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.28849\n",
      "Epoch 483/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5529 - acc: 0.8000 - val_loss: 0.3258 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.28849\n",
      "Epoch 484/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5469 - acc: 0.8000 - val_loss: 0.3484 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.28849\n",
      "Epoch 485/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5626 - acc: 0.7556 - val_loss: 0.3196 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.28849\n",
      "Epoch 486/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5655 - acc: 0.8222 - val_loss: 0.3820 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.28849\n",
      "Epoch 487/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5585 - acc: 0.7556 - val_loss: 0.3104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.28849\n",
      "Epoch 488/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5471 - acc: 0.7556 - val_loss: 0.3184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.28849\n",
      "Epoch 489/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5429 - acc: 0.7778 - val_loss: 0.3717 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.28849\n",
      "Epoch 490/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.5585 - acc: 0.7778 - val_loss: 0.3479 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.28849\n",
      "Epoch 491/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5412 - acc: 0.8222 - val_loss: 0.3057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.28849\n",
      "Epoch 492/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5401 - acc: 0.8222 - val_loss: 0.3351 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.28849\n",
      "Epoch 493/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5423 - acc: 0.7556 - val_loss: 0.3433 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.28849\n",
      "Epoch 494/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5399 - acc: 0.8000 - val_loss: 0.3087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.28849\n",
      "Epoch 495/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.5495 - acc: 0.8222 - val_loss: 0.3180 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.28849\n",
      "Epoch 496/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5340 - acc: 0.8000 - val_loss: 0.3030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.28849\n",
      "Epoch 497/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5560 - acc: 0.8000 - val_loss: 0.3516 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.28849\n",
      "Epoch 498/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5589 - acc: 0.8000 - val_loss: 0.2978 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.28849\n",
      "Epoch 499/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5492 - acc: 0.7778 - val_loss: 0.3538 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.28849\n",
      "Epoch 500/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5317 - acc: 0.8222 - val_loss: 0.3035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.28849\n",
      "Epoch 501/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5380 - acc: 0.8000 - val_loss: 0.2884 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00501: val_loss improved from 0.28849 to 0.28840, saving model to ./model/vloss0.288_vacc1.000_ep501.hdf5\n",
      "Epoch 502/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5369 - acc: 0.7778 - val_loss: 0.3135 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.28840\n",
      "Epoch 503/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5318 - acc: 0.8000 - val_loss: 0.3166 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.28840\n",
      "Epoch 504/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5362 - acc: 0.8000 - val_loss: 0.2863 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.28840 to 0.28631, saving model to ./model/vloss0.286_vacc1.000_ep504.hdf5\n",
      "Epoch 505/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.5262 - acc: 0.8222 - val_loss: 0.3276 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.28631\n",
      "Epoch 506/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5443 - acc: 0.8000 - val_loss: 0.3303 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.28631\n",
      "Epoch 507/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.5489 - acc: 0.8000 - val_loss: 0.3241 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.28631\n",
      "Epoch 508/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5234 - acc: 0.8000 - val_loss: 0.2809 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.28631 to 0.28093, saving model to ./model/vloss0.281_vacc1.000_ep508.hdf5\n",
      "Epoch 509/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5413 - acc: 0.8444 - val_loss: 0.3293 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.28093\n",
      "Epoch 510/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5328 - acc: 0.8000 - val_loss: 0.2939 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.28093\n",
      "Epoch 511/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5218 - acc: 0.8222 - val_loss: 0.3000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.28093\n",
      "Epoch 512/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5247 - acc: 0.8000 - val_loss: 0.2949 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.28093\n",
      "Epoch 513/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5272 - acc: 0.8000 - val_loss: 0.3016 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.28093\n",
      "Epoch 514/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5156 - acc: 0.8000 - val_loss: 0.3174 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.28093\n",
      "Epoch 515/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.5183 - acc: 0.8000 - val_loss: 0.2984 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.28093\n",
      "Epoch 516/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5224 - acc: 0.8000 - val_loss: 0.2608 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.28093 to 0.26081, saving model to ./model/vloss0.261_vacc1.000_ep516.hdf5\n",
      "Epoch 517/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5174 - acc: 0.8222 - val_loss: 0.3173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.26081\n",
      "Epoch 518/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5148 - acc: 0.8444 - val_loss: 0.3148 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.26081\n",
      "Epoch 519/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5211 - acc: 0.7778 - val_loss: 0.2763 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.26081\n",
      "Epoch 520/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5251 - acc: 0.8000 - val_loss: 0.2896 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.26081\n",
      "Epoch 521/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5228 - acc: 0.7778 - val_loss: 0.2648 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.26081\n",
      "Epoch 522/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5230 - acc: 0.8222 - val_loss: 0.2777 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.26081\n",
      "Epoch 523/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5078 - acc: 0.8444 - val_loss: 0.3421 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.26081\n",
      "Epoch 524/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5146 - acc: 0.8222 - val_loss: 0.3336 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.26081\n",
      "Epoch 525/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5223 - acc: 0.8222 - val_loss: 0.2840 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.26081\n",
      "Epoch 526/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5325 - acc: 0.8000 - val_loss: 0.2605 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.26081 to 0.26051, saving model to ./model/vloss0.261_vacc1.000_ep526.hdf5\n",
      "Epoch 527/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5342 - acc: 0.8000 - val_loss: 0.3407 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.26051\n",
      "Epoch 528/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5154 - acc: 0.8222 - val_loss: 0.2372 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.26051 to 0.23724, saving model to ./model/vloss0.237_vacc1.000_ep528.hdf5\n",
      "Epoch 529/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5143 - acc: 0.8444 - val_loss: 0.2607 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.23724\n",
      "Epoch 530/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4986 - acc: 0.8444 - val_loss: 0.3153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.23724\n",
      "Epoch 531/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5180 - acc: 0.7556 - val_loss: 0.2685 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.23724\n",
      "Epoch 532/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5155 - acc: 0.8000 - val_loss: 0.2879 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.23724\n",
      "Epoch 533/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5038 - acc: 0.8222 - val_loss: 0.2437 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.23724\n",
      "Epoch 534/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5042 - acc: 0.8000 - val_loss: 0.2909 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.23724\n",
      "Epoch 535/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5216 - acc: 0.8222 - val_loss: 0.2916 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.23724\n",
      "Epoch 536/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4898 - acc: 0.8000 - val_loss: 0.2418 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.23724\n",
      "Epoch 537/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5031 - acc: 0.8222 - val_loss: 0.2705 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.23724\n",
      "Epoch 538/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5039 - acc: 0.8000 - val_loss: 0.2466 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.23724\n",
      "Epoch 539/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5128 - acc: 0.7778 - val_loss: 0.2785 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.23724\n",
      "Epoch 540/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5166 - acc: 0.8000 - val_loss: 0.2790 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.23724\n",
      "Epoch 541/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.5015 - acc: 0.7778 - val_loss: 0.2433 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.23724\n",
      "Epoch 542/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.4931 - acc: 0.8222 - val_loss: 0.2912 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.23724\n",
      "Epoch 543/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4937 - acc: 0.8222 - val_loss: 0.2707 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.23724\n",
      "Epoch 544/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4898 - acc: 0.7778 - val_loss: 0.2413 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.23724\n",
      "Epoch 545/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4956 - acc: 0.8222 - val_loss: 0.2607 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.23724\n",
      "Epoch 546/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5241 - acc: 0.8222 - val_loss: 0.3358 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.23724\n",
      "Epoch 547/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4946 - acc: 0.8222 - val_loss: 0.2072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.23724 to 0.20717, saving model to ./model/vloss0.207_vacc1.000_ep547.hdf5\n",
      "Epoch 548/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.5071 - acc: 0.8222 - val_loss: 0.2279 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.20717\n",
      "Epoch 549/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4973 - acc: 0.8444 - val_loss: 0.2801 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.20717\n",
      "Epoch 550/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.5167 - acc: 0.8000 - val_loss: 0.2415 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.20717\n",
      "Epoch 551/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4959 - acc: 0.8000 - val_loss: 0.2986 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.20717\n",
      "Epoch 552/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4863 - acc: 0.8222 - val_loss: 0.2362 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.20717\n",
      "Epoch 553/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4848 - acc: 0.8444 - val_loss: 0.2583 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.20717\n",
      "Epoch 554/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4883 - acc: 0.8222 - val_loss: 0.2848 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.20717\n",
      "Epoch 555/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4923 - acc: 0.8000 - val_loss: 0.2209 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.20717\n",
      "Epoch 556/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4870 - acc: 0.8000 - val_loss: 0.2575 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.20717\n",
      "Epoch 557/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4942 - acc: 0.8222 - val_loss: 0.2439 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.20717\n",
      "Epoch 558/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4958 - acc: 0.7778 - val_loss: 0.2305 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.20717\n",
      "Epoch 559/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4829 - acc: 0.8222 - val_loss: 0.2568 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.20717\n",
      "Epoch 560/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4836 - acc: 0.8222 - val_loss: 0.2061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.20717 to 0.20608, saving model to ./model/vloss0.206_vacc1.000_ep560.hdf5\n",
      "Epoch 561/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4933 - acc: 0.8222 - val_loss: 0.2869 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.20608\n",
      "Epoch 562/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4963 - acc: 0.7778 - val_loss: 0.2487 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.20608\n",
      "Epoch 563/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4833 - acc: 0.8444 - val_loss: 0.2362 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.20608\n",
      "Epoch 564/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4790 - acc: 0.8222 - val_loss: 0.2660 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.20608\n",
      "Epoch 565/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4968 - acc: 0.8000 - val_loss: 0.2417 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.20608\n",
      "Epoch 566/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 266us/step - loss: 0.5058 - acc: 0.8222 - val_loss: 0.2563 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.20608\n",
      "Epoch 567/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4733 - acc: 0.8444 - val_loss: 0.2158 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.20608\n",
      "Epoch 568/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.4880 - acc: 0.7778 - val_loss: 0.2222 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.20608\n",
      "Epoch 569/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4793 - acc: 0.8222 - val_loss: 0.2845 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.20608\n",
      "Epoch 570/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4779 - acc: 0.8222 - val_loss: 0.2526 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.20608\n",
      "Epoch 571/2000\n",
      "45/45 [==============================] - 0s 576us/step - loss: 0.4763 - acc: 0.8667 - val_loss: 0.2302 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.20608\n",
      "Epoch 572/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4737 - acc: 0.8222 - val_loss: 0.2368 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.20608\n",
      "Epoch 573/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4663 - acc: 0.7778 - val_loss: 0.2440 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.20608\n",
      "Epoch 574/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4769 - acc: 0.8444 - val_loss: 0.2550 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.20608\n",
      "Epoch 575/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4683 - acc: 0.8444 - val_loss: 0.2437 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.20608\n",
      "Epoch 576/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4732 - acc: 0.8000 - val_loss: 0.2331 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.20608\n",
      "Epoch 577/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4790 - acc: 0.8444 - val_loss: 0.2136 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.20608\n",
      "Epoch 578/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3923 - acc: 0.800 - 0s 288us/step - loss: 0.4684 - acc: 0.8000 - val_loss: 0.2287 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.20608\n",
      "Epoch 579/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4594 - acc: 0.8222 - val_loss: 0.2072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.20608\n",
      "Epoch 580/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4598 - acc: 0.8222 - val_loss: 0.2313 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.20608\n",
      "Epoch 581/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4891 - acc: 0.8000 - val_loss: 0.2231 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.20608\n",
      "Epoch 582/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4598 - acc: 0.8222 - val_loss: 0.2518 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.20608\n",
      "Epoch 583/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4691 - acc: 0.8444 - val_loss: 0.2193 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.20608\n",
      "Epoch 584/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4814 - acc: 0.8667 - val_loss: 0.2211 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.20608\n",
      "Epoch 585/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4668 - acc: 0.8000 - val_loss: 0.2422 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.20608\n",
      "Epoch 586/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.4629 - acc: 0.8222 - val_loss: 0.2325 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.20608\n",
      "Epoch 587/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4649 - acc: 0.8222 - val_loss: 0.2128 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.20608\n",
      "Epoch 588/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4596 - acc: 0.8000 - val_loss: 0.2190 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.20608\n",
      "Epoch 589/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4550 - acc: 0.8000 - val_loss: 0.2511 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.20608\n",
      "Epoch 590/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4554 - acc: 0.8444 - val_loss: 0.2105 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.20608\n",
      "Epoch 591/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4626 - acc: 0.8222 - val_loss: 0.1996 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.20608 to 0.19959, saving model to ./model/vloss0.200_vacc1.000_ep591.hdf5\n",
      "Epoch 592/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4567 - acc: 0.8222 - val_loss: 0.2273 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.19959\n",
      "Epoch 593/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4650 - acc: 0.7778 - val_loss: 0.2054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.19959\n",
      "Epoch 594/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4793 - acc: 0.8000 - val_loss: 0.2660 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.19959\n",
      "Epoch 595/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4734 - acc: 0.8444 - val_loss: 0.1874 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00595: val_loss improved from 0.19959 to 0.18740, saving model to ./model/vloss0.187_vacc1.000_ep595.hdf5\n",
      "Epoch 596/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4499 - acc: 0.8667 - val_loss: 0.2242 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.18740\n",
      "Epoch 597/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4546 - acc: 0.8667 - val_loss: 0.2153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.18740\n",
      "Epoch 598/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4529 - acc: 0.8222 - val_loss: 0.2052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.18740\n",
      "Epoch 599/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.4700 - acc: 0.8000 - val_loss: 0.1848 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.18740 to 0.18477, saving model to ./model/vloss0.185_vacc1.000_ep599.hdf5\n",
      "Epoch 600/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4432 - acc: 0.8444 - val_loss: 0.2210 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.18477\n",
      "Epoch 601/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4634 - acc: 0.8222 - val_loss: 0.2775 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.18477\n",
      "Epoch 602/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4506 - acc: 0.8444 - val_loss: 0.1910 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.18477\n",
      "Epoch 603/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4616 - acc: 0.8667 - val_loss: 0.1921 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.18477\n",
      "Epoch 604/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4612 - acc: 0.8444 - val_loss: 0.2034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.18477\n",
      "Epoch 605/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4489 - acc: 0.8222 - val_loss: 0.2370 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.18477\n",
      "Epoch 606/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4442 - acc: 0.8444 - val_loss: 0.2004 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.18477\n",
      "Epoch 607/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4467 - acc: 0.8667 - val_loss: 0.1844 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00607: val_loss improved from 0.18477 to 0.18439, saving model to ./model/vloss0.184_vacc1.000_ep607.hdf5\n",
      "Epoch 608/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4486 - acc: 0.8222 - val_loss: 0.1804 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00608: val_loss improved from 0.18439 to 0.18042, saving model to ./model/vloss0.180_vacc1.000_ep608.hdf5\n",
      "Epoch 609/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4332 - acc: 0.8444 - val_loss: 0.2352 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.18042\n",
      "Epoch 610/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4541 - acc: 0.8222 - val_loss: 0.2232 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.18042\n",
      "Epoch 611/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4405 - acc: 0.8222 - val_loss: 0.1949 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.18042\n",
      "Epoch 612/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4406 - acc: 0.8222 - val_loss: 0.1728 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00612: val_loss improved from 0.18042 to 0.17285, saving model to ./model/vloss0.173_vacc1.000_ep612.hdf5\n",
      "Epoch 613/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4570 - acc: 0.7778 - val_loss: 0.1916 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.17285\n",
      "Epoch 614/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4357 - acc: 0.8222 - val_loss: 0.1755 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.17285\n",
      "Epoch 615/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4466 - acc: 0.8222 - val_loss: 0.2529 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.17285\n",
      "Epoch 616/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4547 - acc: 0.8222 - val_loss: 0.1852 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.17285\n",
      "Epoch 617/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4528 - acc: 0.8444 - val_loss: 0.1939 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.17285\n",
      "Epoch 618/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4355 - acc: 0.8667 - val_loss: 0.2241 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.17285\n",
      "Epoch 619/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4421 - acc: 0.8222 - val_loss: 0.2218 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.17285\n",
      "Epoch 620/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4378 - acc: 0.8000 - val_loss: 0.1967 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.17285\n",
      "Epoch 621/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4380 - acc: 0.8222 - val_loss: 0.1883 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.17285\n",
      "Epoch 622/2000\n",
      "45/45 [==============================] - 0s 621us/step - loss: 0.4561 - acc: 0.8000 - val_loss: 0.1709 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00622: val_loss improved from 0.17285 to 0.17085, saving model to ./model/vloss0.171_vacc1.000_ep622.hdf5\n",
      "Epoch 623/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4501 - acc: 0.8444 - val_loss: 0.2416 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.17085\n",
      "Epoch 624/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4362 - acc: 0.8444 - val_loss: 0.1868 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.17085\n",
      "Epoch 625/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4598 - acc: 0.8444 - val_loss: 0.1672 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.17085 to 0.16722, saving model to ./model/vloss0.167_vacc1.000_ep625.hdf5\n",
      "Epoch 626/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4370 - acc: 0.8444 - val_loss: 0.1901 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.16722\n",
      "Epoch 627/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4295 - acc: 0.8444 - val_loss: 0.2150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.16722\n",
      "Epoch 628/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4369 - acc: 0.8222 - val_loss: 0.1885 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.16722\n",
      "Epoch 629/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4326 - acc: 0.8222 - val_loss: 0.1939 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.16722\n",
      "Epoch 630/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4310 - acc: 0.8444 - val_loss: 0.1691 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.16722\n",
      "Epoch 631/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4342 - acc: 0.8444 - val_loss: 0.1674 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.16722\n",
      "Epoch 632/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.4283 - acc: 0.8222 - val_loss: 0.1977 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.16722\n",
      "Epoch 633/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4479 - acc: 0.7778 - val_loss: 0.1894 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.16722\n",
      "Epoch 634/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4251 - acc: 0.8667 - val_loss: 0.1750 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.16722\n",
      "Epoch 635/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4243 - acc: 0.8444 - val_loss: 0.1769 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.16722\n",
      "Epoch 636/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4385 - acc: 0.8222 - val_loss: 0.1955 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.16722\n",
      "Epoch 637/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4393 - acc: 0.8000 - val_loss: 0.1984 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.16722\n",
      "Epoch 638/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4166 - acc: 0.8444 - val_loss: 0.1725 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.16722\n",
      "Epoch 639/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4350 - acc: 0.8667 - val_loss: 0.2051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.16722\n",
      "Epoch 640/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4274 - acc: 0.8444 - val_loss: 0.1789 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.16722\n",
      "Epoch 641/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4217 - acc: 0.8667 - val_loss: 0.1507 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00641: val_loss improved from 0.16722 to 0.15075, saving model to ./model/vloss0.151_vacc1.000_ep641.hdf5\n",
      "Epoch 642/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4253 - acc: 0.8444 - val_loss: 0.2009 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.15075\n",
      "Epoch 643/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.4247 - acc: 0.8222 - val_loss: 0.1984 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.15075\n",
      "Epoch 644/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4320 - acc: 0.8222 - val_loss: 0.1559 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.15075\n",
      "Epoch 645/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.4250 - acc: 0.7778 - val_loss: 0.1807 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.15075\n",
      "Epoch 646/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4237 - acc: 0.8444 - val_loss: 0.1746 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.15075\n",
      "Epoch 647/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4245 - acc: 0.8444 - val_loss: 0.1694 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.15075\n",
      "Epoch 648/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4266 - acc: 0.8222 - val_loss: 0.1968 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.15075\n",
      "Epoch 649/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4255 - acc: 0.8222 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.15075\n",
      "Epoch 650/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.4118 - acc: 0.8667 - val_loss: 0.1791 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.15075\n",
      "Epoch 651/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4306 - acc: 0.8222 - val_loss: 0.1896 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.15075\n",
      "Epoch 652/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4431 - acc: 0.8444 - val_loss: 0.1613 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.15075\n",
      "Epoch 653/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4055 - acc: 0.8444 - val_loss: 0.2116 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.15075\n",
      "Epoch 654/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4485 - acc: 0.8222 - val_loss: 0.1713 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.15075\n",
      "Epoch 655/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4592 - acc: 0.8444 - val_loss: 0.1322 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00655: val_loss improved from 0.15075 to 0.13220, saving model to ./model/vloss0.132_vacc1.000_ep655.hdf5\n",
      "Epoch 656/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4026 - acc: 0.8667 - val_loss: 0.2033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.13220\n",
      "Epoch 657/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4296 - acc: 0.8222 - val_loss: 0.1739 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.13220\n",
      "Epoch 658/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4303 - acc: 0.8222 - val_loss: 0.1981 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.13220\n",
      "Epoch 659/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4334 - acc: 0.8222 - val_loss: 0.1484 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.13220\n",
      "Epoch 660/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4071 - acc: 0.8667 - val_loss: 0.1683 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.13220\n",
      "Epoch 661/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4134 - acc: 0.8000 - val_loss: 0.1546 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.13220\n",
      "Epoch 662/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.4047 - acc: 0.7778 - val_loss: 0.1703 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.13220\n",
      "Epoch 663/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4000 - acc: 0.8222 - val_loss: 0.1751 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.13220\n",
      "Epoch 664/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4082 - acc: 0.8222 - val_loss: 0.1544 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.13220\n",
      "Epoch 665/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4124 - acc: 0.8222 - val_loss: 0.1880 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.13220\n",
      "Epoch 666/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4006 - acc: 0.8222 - val_loss: 0.1496 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.13220\n",
      "Epoch 667/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4081 - acc: 0.8222 - val_loss: 0.1560 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.13220\n",
      "Epoch 668/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4146 - acc: 0.8444 - val_loss: 0.1458 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.13220\n",
      "Epoch 669/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4122 - acc: 0.8667 - val_loss: 0.1810 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.13220\n",
      "Epoch 670/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4067 - acc: 0.8444 - val_loss: 0.1772 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.13220\n",
      "Epoch 671/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.3984 - acc: 0.8444 - val_loss: 0.1416 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.13220\n",
      "Epoch 672/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4191 - acc: 0.8667 - val_loss: 0.1334 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.13220\n",
      "Epoch 673/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3930 - acc: 0.8667 - val_loss: 0.1658 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.13220\n",
      "Epoch 674/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4075 - acc: 0.8444 - val_loss: 0.1799 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.13220\n",
      "Epoch 675/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3997 - acc: 0.8444 - val_loss: 0.1453 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.13220\n",
      "Epoch 676/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.4311 - acc: 0.8667 - val_loss: 0.1362 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.13220\n",
      "Epoch 677/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3884 - acc: 0.8667 - val_loss: 0.1959 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.13220\n",
      "Epoch 678/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4380 - acc: 0.8222 - val_loss: 0.1827 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.13220\n",
      "Epoch 679/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4099 - acc: 0.8000 - val_loss: 0.1221 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.13220 to 0.12212, saving model to ./model/vloss0.122_vacc1.000_ep679.hdf5\n",
      "Epoch 680/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4081 - acc: 0.8667 - val_loss: 0.1560 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.12212\n",
      "Epoch 681/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4015 - acc: 0.8222 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.12212\n",
      "Epoch 682/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3967 - acc: 0.8222 - val_loss: 0.1430 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.12212\n",
      "Epoch 683/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4032 - acc: 0.7778 - val_loss: 0.1442 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.12212\n",
      "Epoch 684/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4254 - acc: 0.8667 - val_loss: 0.1303 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.12212\n",
      "Epoch 685/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4434 - acc: 0.800 - 0s 310us/step - loss: 0.4023 - acc: 0.8444 - val_loss: 0.2281 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.12212\n",
      "Epoch 686/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.2049 - acc: 1.000 - 0s 310us/step - loss: 0.4313 - acc: 0.8444 - val_loss: 0.1487 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.12212\n",
      "Epoch 687/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3884 - acc: 0.8444 - val_loss: 0.1463 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.12212\n",
      "Epoch 688/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.4020 - acc: 0.8444 - val_loss: 0.1679 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.12212\n",
      "Epoch 689/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4001 - acc: 0.8222 - val_loss: 0.1364 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.12212\n",
      "Epoch 690/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3852 - acc: 0.8444 - val_loss: 0.1397 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.12212\n",
      "Epoch 691/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3894 - acc: 0.8444 - val_loss: 0.1548 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.12212\n",
      "Epoch 692/2000\n",
      "45/45 [==============================] - 0s 598us/step - loss: 0.3901 - acc: 0.8444 - val_loss: 0.1528 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00692: val_loss did not improve from 0.12212\n",
      "Epoch 693/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3881 - acc: 0.8667 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.12212\n",
      "Epoch 694/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3876 - acc: 0.8444 - val_loss: 0.1459 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.12212\n",
      "Epoch 695/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3919 - acc: 0.8444 - val_loss: 0.1476 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.12212\n",
      "Epoch 696/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3867 - acc: 0.8444 - val_loss: 0.1411 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.12212\n",
      "Epoch 697/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3783 - acc: 0.8667 - val_loss: 0.1296 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.12212\n",
      "Epoch 698/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3979 - acc: 0.8222 - val_loss: 0.1365 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.12212\n",
      "Epoch 699/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3874 - acc: 0.8222 - val_loss: 0.1622 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.12212\n",
      "Epoch 700/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3987 - acc: 0.8444 - val_loss: 0.1365 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.12212\n",
      "Epoch 701/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4173 - acc: 0.8667 - val_loss: 0.1494 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.12212\n",
      "Epoch 702/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.4283 - acc: 0.8222 - val_loss: 0.1552 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.12212\n",
      "Epoch 703/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.3917 - acc: 0.8222 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00703: val_loss improved from 0.12212 to 0.12127, saving model to ./model/vloss0.121_vacc1.000_ep703.hdf5\n",
      "Epoch 704/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3986 - acc: 0.8000 - val_loss: 0.1565 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.12127\n",
      "Epoch 705/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3798 - acc: 0.8222 - val_loss: 0.1504 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.12127\n",
      "Epoch 706/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3824 - acc: 0.8444 - val_loss: 0.1308 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.12127\n",
      "Epoch 707/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3807 - acc: 0.8667 - val_loss: 0.1244 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.12127\n",
      "Epoch 708/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3790 - acc: 0.8444 - val_loss: 0.1464 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.12127\n",
      "Epoch 709/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3917 - acc: 0.8444 - val_loss: 0.1694 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.12127\n",
      "Epoch 710/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3832 - acc: 0.8222 - val_loss: 0.1166 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00710: val_loss improved from 0.12127 to 0.11663, saving model to ./model/vloss0.117_vacc1.000_ep710.hdf5\n",
      "Epoch 711/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3832 - acc: 0.8667 - val_loss: 0.1153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00711: val_loss improved from 0.11663 to 0.11531, saving model to ./model/vloss0.115_vacc1.000_ep711.hdf5\n",
      "Epoch 712/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3751 - acc: 0.8667 - val_loss: 0.1677 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.11531\n",
      "Epoch 713/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3847 - acc: 0.8222 - val_loss: 0.1579 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.11531\n",
      "Epoch 714/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.4113 - acc: 0.8444 - val_loss: 0.1062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00714: val_loss improved from 0.11531 to 0.10616, saving model to ./model/vloss0.106_vacc1.000_ep714.hdf5\n",
      "Epoch 715/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3905 - acc: 0.8222 - val_loss: 0.1710 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.10616\n",
      "Epoch 716/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3839 - acc: 0.8222 - val_loss: 0.1376 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.10616\n",
      "Epoch 717/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3908 - acc: 0.8444 - val_loss: 0.1321 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.10616\n",
      "Epoch 718/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3713 - acc: 0.8222 - val_loss: 0.1470 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.10616\n",
      "Epoch 719/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3884 - acc: 0.8444 - val_loss: 0.1545 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.10616\n",
      "Epoch 720/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3933 - acc: 0.8667 - val_loss: 0.1021 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00720: val_loss improved from 0.10616 to 0.10206, saving model to ./model/vloss0.102_vacc1.000_ep720.hdf5\n",
      "Epoch 721/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3771 - acc: 0.8667 - val_loss: 0.1208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.10206\n",
      "Epoch 722/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3734 - acc: 0.8444 - val_loss: 0.1369 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.10206\n",
      "Epoch 723/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3721 - acc: 0.8222 - val_loss: 0.1375 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.10206\n",
      "Epoch 724/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3674 - acc: 0.8222 - val_loss: 0.1211 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.10206\n",
      "Epoch 725/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3712 - acc: 0.8444 - val_loss: 0.1227 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.10206\n",
      "Epoch 726/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3786 - acc: 0.8222 - val_loss: 0.1423 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.10206\n",
      "Epoch 727/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3842 - acc: 0.8222 - val_loss: 0.1348 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.10206\n",
      "Epoch 728/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.3779 - acc: 0.8667 - val_loss: 0.1086 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.10206\n",
      "Epoch 729/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3658 - acc: 0.8222 - val_loss: 0.1254 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.10206\n",
      "Epoch 730/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3800 - acc: 0.8444 - val_loss: 0.1432 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.10206\n",
      "Epoch 731/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3686 - acc: 0.8667 - val_loss: 0.1177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.10206\n",
      "Epoch 732/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3627 - acc: 0.8444 - val_loss: 0.1182 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.10206\n",
      "Epoch 733/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3706 - acc: 0.8667 - val_loss: 0.1280 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.10206\n",
      "Epoch 734/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.3709 - acc: 0.8444 - val_loss: 0.1239 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00734: val_loss did not improve from 0.10206\n",
      "Epoch 735/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3667 - acc: 0.8667 - val_loss: 0.1150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.10206\n",
      "Epoch 736/2000\n",
      "45/45 [==============================] - 0s 466us/step - loss: 0.3835 - acc: 0.8444 - val_loss: 0.1523 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.10206\n",
      "Epoch 737/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3672 - acc: 0.8222 - val_loss: 0.1178 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.10206\n",
      "Epoch 738/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3738 - acc: 0.8444 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.10206\n",
      "Epoch 739/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3700 - acc: 0.8444 - val_loss: 0.1415 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.10206\n",
      "Epoch 740/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.4134 - acc: 0.8000 - val_loss: 0.0947 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00740: val_loss improved from 0.10206 to 0.09474, saving model to ./model/vloss0.095_vacc1.000_ep740.hdf5\n",
      "Epoch 741/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3786 - acc: 0.8444 - val_loss: 0.1308 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.09474\n",
      "Epoch 742/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3616 - acc: 0.8222 - val_loss: 0.1152 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.09474\n",
      "Epoch 743/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3618 - acc: 0.8222 - val_loss: 0.1326 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.09474\n",
      "Epoch 744/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3691 - acc: 0.8222 - val_loss: 0.1386 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.09474\n",
      "Epoch 745/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3588 - acc: 0.8667 - val_loss: 0.1120 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.09474\n",
      "Epoch 746/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3706 - acc: 0.8444 - val_loss: 0.1208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.09474\n",
      "Epoch 747/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3609 - acc: 0.8222 - val_loss: 0.1111 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.09474\n",
      "Epoch 748/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3644 - acc: 0.8222 - val_loss: 0.1309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.09474\n",
      "Epoch 749/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3695 - acc: 0.8667 - val_loss: 0.1043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.09474\n",
      "Epoch 750/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3706 - acc: 0.8444 - val_loss: 0.1270 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.09474\n",
      "Epoch 751/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3571 - acc: 0.8444 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.09474\n",
      "Epoch 752/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3598 - acc: 0.8667 - val_loss: 0.1041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.09474\n",
      "Epoch 753/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3634 - acc: 0.8444 - val_loss: 0.1096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.09474\n",
      "Epoch 754/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3656 - acc: 0.8667 - val_loss: 0.1289 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.09474\n",
      "Epoch 755/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3632 - acc: 0.8444 - val_loss: 0.1182 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.09474\n",
      "Epoch 756/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3657 - acc: 0.8222 - val_loss: 0.1249 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.09474\n",
      "Epoch 757/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3485 - acc: 0.8222 - val_loss: 0.1032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.09474\n",
      "Epoch 758/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3815 - acc: 0.8444 - val_loss: 0.1124 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.09474\n",
      "Epoch 759/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3668 - acc: 0.8667 - val_loss: 0.0978 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.09474\n",
      "Epoch 760/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3746 - acc: 0.8444 - val_loss: 0.1497 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.09474\n",
      "Epoch 761/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3671 - acc: 0.8444 - val_loss: 0.0996 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.09474\n",
      "Epoch 762/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.3541 - acc: 0.8000 - val_loss: 0.1117 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.09474\n",
      "Epoch 763/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3545 - acc: 0.8222 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.09474\n",
      "Epoch 764/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3505 - acc: 0.8222 - val_loss: 0.1100 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.09474\n",
      "Epoch 765/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3669 - acc: 0.8222 - val_loss: 0.0985 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.09474\n",
      "Epoch 766/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3495 - acc: 0.8667 - val_loss: 0.1231 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.09474\n",
      "Epoch 767/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3576 - acc: 0.8222 - val_loss: 0.1147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.09474\n",
      "Epoch 768/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3601 - acc: 0.8444 - val_loss: 0.1060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.09474\n",
      "Epoch 769/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3470 - acc: 0.8444 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.09474\n",
      "Epoch 770/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3543 - acc: 0.8444 - val_loss: 0.1047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.09474\n",
      "Epoch 771/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3642 - acc: 0.8222 - val_loss: 0.1182 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.09474\n",
      "Epoch 772/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3566 - acc: 0.8667 - val_loss: 0.0991 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.09474\n",
      "Epoch 773/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3542 - acc: 0.8444 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.09474\n",
      "Epoch 774/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3475 - acc: 0.8667 - val_loss: 0.1079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.09474\n",
      "Epoch 775/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3489 - acc: 0.8667 - val_loss: 0.0917 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00775: val_loss improved from 0.09474 to 0.09172, saving model to ./model/vloss0.092_vacc1.000_ep775.hdf5\n",
      "Epoch 776/2000\n",
      "45/45 [==============================] - 0s 510us/step - loss: 0.3737 - acc: 0.8000 - val_loss: 0.1244 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.09172\n",
      "Epoch 777/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.3564 - acc: 0.8222 - val_loss: 0.0915 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00777: val_loss improved from 0.09172 to 0.09146, saving model to ./model/vloss0.091_vacc1.000_ep777.hdf5\n",
      "Epoch 778/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3616 - acc: 0.8222 - val_loss: 0.1351 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.09146\n",
      "Epoch 779/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3597 - acc: 0.8444 - val_loss: 0.0920 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.09146\n",
      "Epoch 780/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3542 - acc: 0.8667 - val_loss: 0.1207 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.09146\n",
      "Epoch 781/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3730 - acc: 0.8222 - val_loss: 0.1587 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.09146\n",
      "Epoch 782/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 0.3638 - acc: 0.8667 - val_loss: 0.0725 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00782: val_loss improved from 0.09146 to 0.07251, saving model to ./model/vloss0.073_vacc1.000_ep782.hdf5\n",
      "Epoch 783/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3581 - acc: 0.8444 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.07251\n",
      "Epoch 784/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3538 - acc: 0.8667 - val_loss: 0.1107 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.07251\n",
      "Epoch 785/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3435 - acc: 0.8222 - val_loss: 0.1202 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.07251\n",
      "Epoch 786/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3425 - acc: 0.8000 - val_loss: 0.1237 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.07251\n",
      "Epoch 787/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3425 - acc: 0.8444 - val_loss: 0.0982 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.07251\n",
      "Epoch 788/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3341 - acc: 0.8667 - val_loss: 0.0952 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.07251\n",
      "Epoch 789/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3428 - acc: 0.8667 - val_loss: 0.1017 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.07251\n",
      "Epoch 790/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3337 - acc: 0.8444 - val_loss: 0.1080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.07251\n",
      "Epoch 791/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3392 - acc: 0.8222 - val_loss: 0.1013 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.07251\n",
      "Epoch 792/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3468 - acc: 0.8667 - val_loss: 0.0928 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.07251\n",
      "Epoch 793/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3343 - acc: 0.8667 - val_loss: 0.1027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.07251\n",
      "Epoch 794/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3666 - acc: 0.8222 - val_loss: 0.1242 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.07251\n",
      "Epoch 795/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3530 - acc: 0.8222 - val_loss: 0.1304 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.07251\n",
      "Epoch 796/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3425 - acc: 0.8444 - val_loss: 0.0912 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.07251\n",
      "Epoch 797/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3368 - acc: 0.8667 - val_loss: 0.0881 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.07251\n",
      "Epoch 798/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3495 - acc: 0.8222 - val_loss: 0.1032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.07251\n",
      "Epoch 799/2000\n",
      "45/45 [==============================] - 0s 909us/step - loss: 0.3403 - acc: 0.8667 - val_loss: 0.0870 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.07251\n",
      "Epoch 800/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3370 - acc: 0.8222 - val_loss: 0.0954 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.07251\n",
      "Epoch 801/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3335 - acc: 0.8222 - val_loss: 0.1032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.07251\n",
      "Epoch 802/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3450 - acc: 0.8444 - val_loss: 0.1107 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.07251\n",
      "Epoch 803/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3455 - acc: 0.8667 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.07251\n",
      "Epoch 804/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3360 - acc: 0.8444 - val_loss: 0.0878 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.07251\n",
      "Epoch 805/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3286 - acc: 0.8222 - val_loss: 0.0961 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.07251\n",
      "Epoch 806/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.3371 - acc: 0.8444 - val_loss: 0.0981 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.07251\n",
      "Epoch 807/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3381 - acc: 0.8222 - val_loss: 0.0969 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.07251\n",
      "Epoch 808/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3296 - acc: 0.8667 - val_loss: 0.0805 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.07251\n",
      "Epoch 809/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3412 - acc: 0.8444 - val_loss: 0.1166 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.07251\n",
      "Epoch 810/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3553 - acc: 0.8222 - val_loss: 0.0932 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.07251\n",
      "Epoch 811/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.3343 - acc: 0.8444 - val_loss: 0.1089 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.07251\n",
      "Epoch 812/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3626 - acc: 0.8000 - val_loss: 0.0906 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.07251\n",
      "Epoch 813/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3326 - acc: 0.8667 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.07251\n",
      "Epoch 814/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3279 - acc: 0.8444 - val_loss: 0.0946 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.07251\n",
      "Epoch 815/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3360 - acc: 0.8222 - val_loss: 0.0839 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.07251\n",
      "Epoch 816/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3345 - acc: 0.8444 - val_loss: 0.0886 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.07251\n",
      "Epoch 817/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3414 - acc: 0.8222 - val_loss: 0.1046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.07251\n",
      "Epoch 818/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3454 - acc: 0.8667 - val_loss: 0.0830 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.07251\n",
      "Epoch 819/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3278 - acc: 0.8667 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.07251\n",
      "Epoch 820/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3349 - acc: 0.8444 - val_loss: 0.1003 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00820: val_loss did not improve from 0.07251\n",
      "Epoch 821/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3572 - acc: 0.8222 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.07251\n",
      "Epoch 822/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3295 - acc: 0.8222 - val_loss: 0.1022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.07251\n",
      "Epoch 823/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3358 - acc: 0.8222 - val_loss: 0.0894 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.07251\n",
      "Epoch 824/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3485 - acc: 0.8222 - val_loss: 0.0951 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.07251\n",
      "Epoch 825/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3272 - acc: 0.8222 - val_loss: 0.1040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.07251\n",
      "Epoch 826/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3192 - acc: 0.8667 - val_loss: 0.0950 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.07251\n",
      "Epoch 827/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3208 - acc: 0.8667 - val_loss: 0.0873 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.07251\n",
      "Epoch 828/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3247 - acc: 0.8667 - val_loss: 0.0881 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.07251\n",
      "Epoch 829/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3366 - acc: 0.8444 - val_loss: 0.1060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.07251\n",
      "Epoch 830/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3340 - acc: 0.8444 - val_loss: 0.0754 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.07251\n",
      "Epoch 831/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3466 - acc: 0.8222 - val_loss: 0.0800 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.07251\n",
      "Epoch 832/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3226 - acc: 0.8444 - val_loss: 0.0765 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.07251\n",
      "Epoch 833/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3267 - acc: 0.8222 - val_loss: 0.0963 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.07251\n",
      "Epoch 834/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3238 - acc: 0.8444 - val_loss: 0.0825 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.07251\n",
      "Epoch 835/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3251 - acc: 0.8444 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.07251\n",
      "Epoch 836/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3251 - acc: 0.8667 - val_loss: 0.0921 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.07251\n",
      "Epoch 837/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3474 - acc: 0.8667 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00837: val_loss improved from 0.07251 to 0.06422, saving model to ./model/vloss0.064_vacc1.000_ep837.hdf5\n",
      "Epoch 838/2000\n",
      "45/45 [==============================] - 0s 621us/step - loss: 0.3133 - acc: 0.8667 - val_loss: 0.1022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.06422\n",
      "Epoch 839/2000\n",
      "45/45 [==============================] - 0s 354us/step - loss: 0.3456 - acc: 0.8444 - val_loss: 0.1140 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.06422\n",
      "Epoch 840/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3314 - acc: 0.8889 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00840: val_loss improved from 0.06422 to 0.05849, saving model to ./model/vloss0.058_vacc1.000_ep840.hdf5\n",
      "Epoch 841/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3250 - acc: 0.8444 - val_loss: 0.0901 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.05849\n",
      "Epoch 842/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3278 - acc: 0.8667 - val_loss: 0.0892 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.05849\n",
      "Epoch 843/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3159 - acc: 0.8667 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.05849\n",
      "Epoch 844/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3364 - acc: 0.8444 - val_loss: 0.0838 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.05849\n",
      "Epoch 845/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3251 - acc: 0.8444 - val_loss: 0.0978 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.05849\n",
      "Epoch 846/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3194 - acc: 0.8444 - val_loss: 0.0778 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.05849\n",
      "Epoch 847/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3276 - acc: 0.8444 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.05849\n",
      "Epoch 848/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3111 - acc: 0.8222 - val_loss: 0.0915 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.05849\n",
      "Epoch 849/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.3188 - acc: 0.8222 - val_loss: 0.0850 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.05849\n",
      "Epoch 850/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3128 - acc: 0.8444 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.05849\n",
      "Epoch 851/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3347 - acc: 0.8667 - val_loss: 0.1056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.05849\n",
      "Epoch 852/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3085 - acc: 0.8667 - val_loss: 0.0702 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.05849\n",
      "Epoch 853/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3262 - acc: 0.8444 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.05849\n",
      "Epoch 854/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3262 - acc: 0.8444 - val_loss: 0.0850 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.05849\n",
      "Epoch 855/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3136 - acc: 0.8667 - val_loss: 0.0848 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.05849\n",
      "Epoch 856/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3221 - acc: 0.8444 - val_loss: 0.0865 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.05849\n",
      "Epoch 857/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3193 - acc: 0.8667 - val_loss: 0.0730 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.05849\n",
      "Epoch 858/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3149 - acc: 0.8667 - val_loss: 0.0735 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.05849\n",
      "Epoch 859/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3215 - acc: 0.8000 - val_loss: 0.0790 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.05849\n",
      "Epoch 860/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3176 - acc: 0.8667 - val_loss: 0.0829 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.05849\n",
      "Epoch 861/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3319 - acc: 0.8444 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.05849\n",
      "Epoch 862/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3149 - acc: 0.8667 - val_loss: 0.0825 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.05849\n",
      "Epoch 863/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3228 - acc: 0.8889 - val_loss: 0.1055 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00863: val_loss did not improve from 0.05849\n",
      "Epoch 864/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3083 - acc: 0.8667 - val_loss: 0.0659 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.05849\n",
      "Epoch 865/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3353 - acc: 0.8444 - val_loss: 0.0750 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.05849\n",
      "Epoch 866/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3057 - acc: 0.8667 - val_loss: 0.0988 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.05849\n",
      "Epoch 867/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3236 - acc: 0.8444 - val_loss: 0.0636 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.05849\n",
      "Epoch 868/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.3186 - acc: 0.8667 - val_loss: 0.0677 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.05849\n",
      "Epoch 869/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3336 - acc: 0.8444 - val_loss: 0.1042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.05849\n",
      "Epoch 870/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3292 - acc: 0.8444 - val_loss: 0.0609 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.05849\n",
      "Epoch 871/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3173 - acc: 0.8667 - val_loss: 0.0628 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.05849\n",
      "Epoch 872/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3393 - acc: 0.8222 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.05849\n",
      "Epoch 873/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3050 - acc: 0.8667 - val_loss: 0.0629 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.05849\n",
      "Epoch 874/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3044 - acc: 0.8667 - val_loss: 0.0595 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.05849\n",
      "Epoch 875/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3063 - acc: 0.8222 - val_loss: 0.0709 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.05849\n",
      "Epoch 876/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3144 - acc: 0.8667 - val_loss: 0.0778 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.05849\n",
      "Epoch 877/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3181 - acc: 0.8667 - val_loss: 0.0618 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.05849\n",
      "Epoch 878/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3166 - acc: 0.8667 - val_loss: 0.0863 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.05849\n",
      "Epoch 879/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3061 - acc: 0.8444 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.05849\n",
      "Epoch 880/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3279 - acc: 0.8444 - val_loss: 0.0713 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.05849\n",
      "Epoch 881/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3138 - acc: 0.8444 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.05849\n",
      "Epoch 882/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3136 - acc: 0.8667 - val_loss: 0.0622 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.05849\n",
      "Epoch 883/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3197 - acc: 0.8667 - val_loss: 0.0967 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.05849\n",
      "Epoch 884/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3079 - acc: 0.8667 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.05849\n",
      "Epoch 885/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3034 - acc: 0.8667 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00885: val_loss improved from 0.05849 to 0.05670, saving model to ./model/vloss0.057_vacc1.000_ep885.hdf5\n",
      "Epoch 886/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3156 - acc: 0.8222 - val_loss: 0.0751 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.05670\n",
      "Epoch 887/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3238 - acc: 0.8222 - val_loss: 0.0866 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.05670\n",
      "Epoch 888/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3206 - acc: 0.8222 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.05670\n",
      "Epoch 889/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3252 - acc: 0.8444 - val_loss: 0.0965 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.05670\n",
      "Epoch 890/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2968 - acc: 0.8667 - val_loss: 0.0698 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.05670\n",
      "Epoch 891/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3195 - acc: 0.8444 - val_loss: 0.0603 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.05670\n",
      "Epoch 892/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2966 - acc: 0.8667 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.05670\n",
      "Epoch 893/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3145 - acc: 0.8000 - val_loss: 0.0764 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.05670\n",
      "Epoch 894/2000\n",
      "45/45 [==============================] - 0s 510us/step - loss: 0.3096 - acc: 0.8000 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.05670\n",
      "Epoch 895/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3015 - acc: 0.8667 - val_loss: 0.0645 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.05670\n",
      "Epoch 896/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2988 - acc: 0.8667 - val_loss: 0.0677 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.05670\n",
      "Epoch 897/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3009 - acc: 0.8889 - val_loss: 0.0907 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.05670\n",
      "Epoch 898/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2994 - acc: 0.8222 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.05670\n",
      "Epoch 899/2000\n",
      "45/45 [==============================] - 0s 621us/step - loss: 0.3056 - acc: 0.8444 - val_loss: 0.0541 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00899: val_loss improved from 0.05670 to 0.05405, saving model to ./model/vloss0.054_vacc1.000_ep899.hdf5\n",
      "Epoch 900/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3307 - acc: 0.8000 - val_loss: 0.0764 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.05405\n",
      "Epoch 901/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3077 - acc: 0.8667 - val_loss: 0.0536 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00901: val_loss improved from 0.05405 to 0.05356, saving model to ./model/vloss0.054_vacc1.000_ep901.hdf5\n",
      "Epoch 902/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2941 - acc: 0.8667 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.05356\n",
      "Epoch 903/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3238 - acc: 0.8444 - val_loss: 0.0937 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.05356\n",
      "Epoch 904/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3129 - acc: 0.8667 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.05356\n",
      "Epoch 905/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3262 - acc: 0.8667 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00905: val_loss improved from 0.05356 to 0.04407, saving model to ./model/vloss0.044_vacc1.000_ep905.hdf5\n",
      "Epoch 906/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2874 - acc: 0.8667 - val_loss: 0.0808 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.04407\n",
      "Epoch 907/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3005 - acc: 0.8667 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.04407\n",
      "Epoch 908/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.3280 - acc: 0.8222 - val_loss: 0.0555 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.04407\n",
      "Epoch 909/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.2898 - acc: 0.8667 - val_loss: 0.0860 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.04407\n",
      "Epoch 910/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3106 - acc: 0.8667 - val_loss: 0.0616 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.04407\n",
      "Epoch 911/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3093 - acc: 0.8667 - val_loss: 0.0781 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.04407\n",
      "Epoch 912/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2906 - acc: 0.8667 - val_loss: 0.0627 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.04407\n",
      "Epoch 913/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2982 - acc: 0.8444 - val_loss: 0.0578 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.04407\n",
      "Epoch 914/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3174 - acc: 0.8222 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.04407\n",
      "Epoch 915/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.3001 - acc: 0.8444 - val_loss: 0.0741 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.04407\n",
      "Epoch 916/2000\n",
      "45/45 [==============================] - 0s 510us/step - loss: 0.2951 - acc: 0.8444 - val_loss: 0.0668 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.04407\n",
      "Epoch 917/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2964 - acc: 0.8444 - val_loss: 0.0555 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.04407\n",
      "Epoch 918/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2959 - acc: 0.8667 - val_loss: 0.0523 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.04407\n",
      "Epoch 919/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3016 - acc: 0.8667 - val_loss: 0.0709 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.04407\n",
      "Epoch 920/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2960 - acc: 0.8889 - val_loss: 0.0621 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.04407\n",
      "Epoch 921/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2980 - acc: 0.8222 - val_loss: 0.0658 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.04407\n",
      "Epoch 922/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3065 - acc: 0.8444 - val_loss: 0.0636 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.04407\n",
      "Epoch 923/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3097 - acc: 0.8444 - val_loss: 0.0496 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.04407\n",
      "Epoch 924/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.3057 - acc: 0.8222 - val_loss: 0.0554 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.04407\n",
      "Epoch 925/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2997 - acc: 0.8667 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.04407\n",
      "Epoch 926/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2940 - acc: 0.8444 - val_loss: 0.0675 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.04407\n",
      "Epoch 927/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3083 - acc: 0.8444 - val_loss: 0.0830 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.04407\n",
      "Epoch 928/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3127 - acc: 0.8444 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.04407\n",
      "Epoch 929/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2991 - acc: 0.8667 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.04407\n",
      "Epoch 930/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3109 - acc: 0.8000 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.04407\n",
      "Epoch 931/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3013 - acc: 0.8444 - val_loss: 0.0576 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.04407\n",
      "Epoch 932/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2961 - acc: 0.8444 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.04407\n",
      "Epoch 933/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2887 - acc: 0.8667 - val_loss: 0.0593 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.04407\n",
      "Epoch 934/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2941 - acc: 0.8444 - val_loss: 0.0572 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.04407\n",
      "Epoch 935/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2859 - acc: 0.8667 - val_loss: 0.0568 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.04407\n",
      "Epoch 936/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2853 - acc: 0.8667 - val_loss: 0.0562 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.04407\n",
      "Epoch 937/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2879 - acc: 0.8667 - val_loss: 0.0613 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.04407\n",
      "Epoch 938/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2823 - acc: 0.8444 - val_loss: 0.0521 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.04407\n",
      "Epoch 939/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2828 - acc: 0.8889 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.04407\n",
      "Epoch 940/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2920 - acc: 0.8444 - val_loss: 0.0550 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.04407\n",
      "Epoch 941/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2974 - acc: 0.8667 - val_loss: 0.0979 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.04407\n",
      "Epoch 942/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3009 - acc: 0.8000 - val_loss: 0.0524 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.04407\n",
      "Epoch 943/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2917 - acc: 0.8889 - val_loss: 0.0575 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.04407\n",
      "Epoch 944/2000\n",
      "45/45 [==============================] - 0s 510us/step - loss: 0.2927 - acc: 0.8444 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.04407\n",
      "Epoch 945/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2884 - acc: 0.8667 - val_loss: 0.0662 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.04407\n",
      "Epoch 946/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2869 - acc: 0.8667 - val_loss: 0.0615 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.04407\n",
      "Epoch 947/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2812 - acc: 0.8889 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00947: val_loss improved from 0.04407 to 0.04406, saving model to ./model/vloss0.044_vacc1.000_ep947.hdf5\n",
      "Epoch 948/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2953 - acc: 0.8222 - val_loss: 0.0544 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.04406\n",
      "Epoch 949/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.2861 - acc: 0.8889 - val_loss: 0.0589 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.04406\n",
      "Epoch 950/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2957 - acc: 0.8667 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00950: val_loss improved from 0.04406 to 0.04229, saving model to ./model/vloss0.042_vacc1.000_ep950.hdf5\n",
      "Epoch 951/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2817 - acc: 0.8889 - val_loss: 0.0680 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.04229\n",
      "Epoch 952/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3013 - acc: 0.8444 - val_loss: 0.0592 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.04229\n",
      "Epoch 953/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2890 - acc: 0.8667 - val_loss: 0.0564 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.04229\n",
      "Epoch 954/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2903 - acc: 0.8444 - val_loss: 0.0518 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.04229\n",
      "Epoch 955/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2963 - acc: 0.8222 - val_loss: 0.0477 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.04229\n",
      "Epoch 956/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2860 - acc: 0.8444 - val_loss: 0.0500 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.04229\n",
      "Epoch 957/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2877 - acc: 0.8889 - val_loss: 0.0693 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.04229\n",
      "Epoch 958/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2839 - acc: 0.8667 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.04229\n",
      "Epoch 959/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3129 - acc: 0.8667 - val_loss: 0.0370 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00959: val_loss improved from 0.04229 to 0.03699, saving model to ./model/vloss0.037_vacc1.000_ep959.hdf5\n",
      "Epoch 960/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3130 - acc: 0.8222 - val_loss: 0.0640 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.03699\n",
      "Epoch 961/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2816 - acc: 0.8667 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.03699\n",
      "Epoch 962/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2858 - acc: 0.8667 - val_loss: 0.0514 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.03699\n",
      "Epoch 963/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2945 - acc: 0.8889 - val_loss: 0.0396 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.03699\n",
      "Epoch 964/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2732 - acc: 0.8667 - val_loss: 0.0737 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.03699\n",
      "Epoch 965/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.3033 - acc: 0.8667 - val_loss: 0.0724 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.03699\n",
      "Epoch 966/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2904 - acc: 0.8889 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.03699\n",
      "Epoch 967/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2892 - acc: 0.8667 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.03699\n",
      "Epoch 968/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2845 - acc: 0.8667 - val_loss: 0.0482 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.03699\n",
      "Epoch 969/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.3074 - acc: 0.8667 - val_loss: 0.0805 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.03699\n",
      "Epoch 970/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2975 - acc: 0.8444 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.03699\n",
      "Epoch 971/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2828 - acc: 0.8667 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.03699\n",
      "Epoch 972/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2793 - acc: 0.8444 - val_loss: 0.0701 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.03699\n",
      "Epoch 973/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2862 - acc: 0.8222 - val_loss: 0.0481 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.03699\n",
      "Epoch 974/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2861 - acc: 0.8667 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.03699\n",
      "Epoch 975/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2771 - acc: 0.8667 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.03699\n",
      "Epoch 976/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2841 - acc: 0.8889 - val_loss: 0.0507 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.03699\n",
      "Epoch 977/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2898 - acc: 0.8444 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.03699\n",
      "Epoch 978/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2890 - acc: 0.8889 - val_loss: 0.0629 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.03699\n",
      "Epoch 979/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2753 - acc: 0.8444 - val_loss: 0.0509 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.03699\n",
      "Epoch 980/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2767 - acc: 0.8667 - val_loss: 0.0465 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.03699\n",
      "Epoch 981/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2850 - acc: 0.8889 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.03699\n",
      "Epoch 982/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0193 - acc: 1.000 - 0s 288us/step - loss: 0.2833 - acc: 0.8667 - val_loss: 0.0602 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.03699\n",
      "Epoch 983/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2798 - acc: 0.8667 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.03699\n",
      "Epoch 984/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2781 - acc: 0.8667 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.03699\n",
      "Epoch 985/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2770 - acc: 0.8667 - val_loss: 0.0596 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.03699\n",
      "Epoch 986/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2808 - acc: 0.8889 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.03699\n",
      "Epoch 987/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2865 - acc: 0.8444 - val_loss: 0.0536 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.03699\n",
      "Epoch 988/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2780 - acc: 0.8667 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.03699\n",
      "Epoch 989/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2879 - acc: 0.8667 - val_loss: 0.0367 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00989: val_loss improved from 0.03699 to 0.03669, saving model to ./model/vloss0.037_vacc1.000_ep989.hdf5\n",
      "Epoch 990/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.3002 - acc: 0.8444 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.03669\n",
      "Epoch 991/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2698 - acc: 0.8889 - val_loss: 0.0404 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.03669\n",
      "Epoch 992/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2900 - acc: 0.8444 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.03669\n",
      "Epoch 993/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2792 - acc: 0.8889 - val_loss: 0.0411 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.03669\n",
      "Epoch 994/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2715 - acc: 0.8889 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.03669\n",
      "Epoch 995/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2949 - acc: 0.8444 - val_loss: 0.0790 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.03669\n",
      "Epoch 996/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2800 - acc: 0.8667 - val_loss: 0.0414 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.03669\n",
      "Epoch 997/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2865 - acc: 0.8667 - val_loss: 0.0463 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.03669\n",
      "Epoch 998/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2755 - acc: 0.8667 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.03669\n",
      "Epoch 999/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2786 - acc: 0.8667 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.03669\n",
      "Epoch 1000/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2713 - acc: 0.8667 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.03669\n",
      "Epoch 1001/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2693 - acc: 0.8889 - val_loss: 0.0520 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.03669\n",
      "Epoch 1002/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2765 - acc: 0.8889 - val_loss: 0.0475 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.03669\n",
      "Epoch 1003/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.2577 - acc: 0.8889 - val_loss: 0.0396 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.03669\n",
      "Epoch 1004/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2881 - acc: 0.8444 - val_loss: 0.0474 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.03669\n",
      "Epoch 1005/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2713 - acc: 0.8889 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.03669\n",
      "Epoch 1006/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2900 - acc: 0.8667 - val_loss: 0.0517 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.03669\n",
      "Epoch 1007/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2675 - acc: 0.8667 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.03669\n",
      "Epoch 1008/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2801 - acc: 0.8889 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.03669\n",
      "Epoch 1009/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2650 - acc: 0.8667 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.03669\n",
      "Epoch 1010/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2734 - acc: 0.8889 - val_loss: 0.0581 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.03669\n",
      "Epoch 1011/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2816 - acc: 0.8667 - val_loss: 0.0339 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01011: val_loss improved from 0.03669 to 0.03385, saving model to ./model/vloss0.034_vacc1.000_ep1011.hdf5\n",
      "Epoch 1012/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2705 - acc: 0.8444 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.03385\n",
      "Epoch 1013/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2754 - acc: 0.8667 - val_loss: 0.0544 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.03385\n",
      "Epoch 1014/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2626 - acc: 0.8667 - val_loss: 0.0371 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.03385\n",
      "Epoch 1015/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2705 - acc: 0.8667 - val_loss: 0.0336 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01015: val_loss improved from 0.03385 to 0.03355, saving model to ./model/vloss0.034_vacc1.000_ep1015.hdf5\n",
      "Epoch 1016/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2654 - acc: 0.8889 - val_loss: 0.0430 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.03355\n",
      "Epoch 1017/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2815 - acc: 0.8667 - val_loss: 0.0474 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.03355\n",
      "Epoch 1018/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2788 - acc: 0.8667 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.03355\n",
      "Epoch 1019/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2738 - acc: 0.8889 - val_loss: 0.0393 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.03355\n",
      "Epoch 1020/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2997 - acc: 0.8444 - val_loss: 0.0669 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.03355\n",
      "Epoch 1021/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2660 - acc: 0.8667 - val_loss: 0.0469 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.03355\n",
      "Epoch 1022/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2724 - acc: 0.8444 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01022: val_loss improved from 0.03355 to 0.03243, saving model to ./model/vloss0.032_vacc1.000_ep1022.hdf5\n",
      "Epoch 1023/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2781 - acc: 0.8889 - val_loss: 0.0355 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.03243\n",
      "Epoch 1024/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2742 - acc: 0.8444 - val_loss: 0.0516 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.03243\n",
      "Epoch 1025/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2777 - acc: 0.8667 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.03243\n",
      "Epoch 1026/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2591 - acc: 0.8667 - val_loss: 0.0476 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.03243\n",
      "Epoch 1027/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2654 - acc: 0.8889 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.03243\n",
      "Epoch 1028/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2629 - acc: 0.8444 - val_loss: 0.0409 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.03243\n",
      "Epoch 1029/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2639 - acc: 0.8889 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.03243\n",
      "Epoch 1030/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2701 - acc: 0.8667 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.03243\n",
      "Epoch 1031/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2676 - acc: 0.8444 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.03243\n",
      "Epoch 1032/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2858 - acc: 0.8444 - val_loss: 0.0512 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.03243\n",
      "Epoch 1033/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.2616 - acc: 0.8889 - val_loss: 0.0314 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01033: val_loss improved from 0.03243 to 0.03143, saving model to ./model/vloss0.031_vacc1.000_ep1033.hdf5\n",
      "Epoch 1034/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2749 - acc: 0.8889 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.03143\n",
      "Epoch 1035/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2644 - acc: 0.8889 - val_loss: 0.0579 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.03143\n",
      "Epoch 1036/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2640 - acc: 0.8889 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.03143\n",
      "Epoch 1037/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2698 - acc: 0.8667 - val_loss: 0.0321 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.03143\n",
      "Epoch 1038/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2579 - acc: 0.8889 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.03143\n",
      "Epoch 1039/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2785 - acc: 0.8889 - val_loss: 0.0601 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.03143\n",
      "Epoch 1040/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2605 - acc: 0.8889 - val_loss: 0.0385 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.03143\n",
      "Epoch 1041/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2639 - acc: 0.8667 - val_loss: 0.0381 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.03143\n",
      "Epoch 1042/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2686 - acc: 0.8667 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01042: val_loss improved from 0.03143 to 0.03083, saving model to ./model/vloss0.031_vacc1.000_ep1042.hdf5\n",
      "Epoch 1043/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2839 - acc: 0.8444 - val_loss: 0.0602 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.03083\n",
      "Epoch 1044/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2581 - acc: 0.9111 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 0.03083\n",
      "Epoch 1045/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2668 - acc: 0.8667 - val_loss: 0.0313 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.03083\n",
      "Epoch 1046/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2735 - acc: 0.8444 - val_loss: 0.0517 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.03083\n",
      "Epoch 1047/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2851 - acc: 0.8222 - val_loss: 0.0410 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.03083\n",
      "Epoch 1048/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2760 - acc: 0.8444 - val_loss: 0.0539 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.03083\n",
      "Epoch 1049/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 0.2638 - acc: 0.8889 - val_loss: 0.0297 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01049: val_loss improved from 0.03083 to 0.02968, saving model to ./model/vloss0.030_vacc1.000_ep1049.hdf5\n",
      "Epoch 1050/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2748 - acc: 0.8667 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.02968\n",
      "Epoch 1051/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2871 - acc: 0.8444 - val_loss: 0.0303 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.02968\n",
      "Epoch 1052/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2567 - acc: 0.8444 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.02968\n",
      "Epoch 1053/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2588 - acc: 0.8889 - val_loss: 0.0459 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.02968\n",
      "Epoch 1054/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2587 - acc: 0.8889 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.02968\n",
      "Epoch 1055/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2748 - acc: 0.8667 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.02968\n",
      "Epoch 1056/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2856 - acc: 0.8444 - val_loss: 0.0807 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.02968\n",
      "Epoch 1057/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2658 - acc: 0.8889 - val_loss: 0.0348 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.02968\n",
      "Epoch 1058/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2569 - acc: 0.8667 - val_loss: 0.0342 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.02968\n",
      "Epoch 1059/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2682 - acc: 0.8000 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.02968\n",
      "Epoch 1060/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2601 - acc: 0.8667 - val_loss: 0.0319 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.02968\n",
      "Epoch 1061/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2653 - acc: 0.8667 - val_loss: 0.0391 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.02968\n",
      "Epoch 1062/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2658 - acc: 0.8667 - val_loss: 0.0365 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.02968\n",
      "Epoch 1063/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2590 - acc: 0.8667 - val_loss: 0.0361 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.02968\n",
      "Epoch 1064/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2573 - acc: 0.8889 - val_loss: 0.0462 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.02968\n",
      "Epoch 1065/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2557 - acc: 0.8889 - val_loss: 0.0380 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.02968\n",
      "Epoch 1066/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2472 - acc: 0.8889 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.02968\n",
      "Epoch 1067/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2524 - acc: 0.8889 - val_loss: 0.0301 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.02968\n",
      "Epoch 1068/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2659 - acc: 0.8667 - val_loss: 0.0380 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.02968\n",
      "Epoch 1069/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2681 - acc: 0.8667 - val_loss: 0.0379 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.02968\n",
      "Epoch 1070/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2521 - acc: 0.8889 - val_loss: 0.0406 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.02968\n",
      "Epoch 1071/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2688 - acc: 0.8444 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.02968\n",
      "Epoch 1072/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2509 - acc: 0.8889 - val_loss: 0.0392 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.02968\n",
      "Epoch 1073/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2638 - acc: 0.8667 - val_loss: 0.0286 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01073: val_loss improved from 0.02968 to 0.02861, saving model to ./model/vloss0.029_vacc1.000_ep1073.hdf5\n",
      "Epoch 1074/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2873 - acc: 0.8444 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.02861\n",
      "Epoch 1075/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2664 - acc: 0.9111 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.02861\n",
      "Epoch 1076/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2633 - acc: 0.8667 - val_loss: 0.0350 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.02861\n",
      "Epoch 1077/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2799 - acc: 0.8222 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.02861\n",
      "Epoch 1078/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2539 - acc: 0.8889 - val_loss: 0.0346 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.02861\n",
      "Epoch 1079/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2554 - acc: 0.8667 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.02861\n",
      "Epoch 1080/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2700 - acc: 0.8667 - val_loss: 0.0619 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.02861\n",
      "Epoch 1081/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2515 - acc: 0.8889 - val_loss: 0.0365 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.02861\n",
      "Epoch 1082/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3822 - acc: 0.800 - 0s 288us/step - loss: 0.2798 - acc: 0.8667 - val_loss: 0.0261 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01082: val_loss improved from 0.02861 to 0.02612, saving model to ./model/vloss0.026_vacc1.000_ep1082.hdf5\n",
      "Epoch 1083/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2758 - acc: 0.8667 - val_loss: 0.0584 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.02612\n",
      "Epoch 1084/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2562 - acc: 0.9333 - val_loss: 0.0313 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.02612\n",
      "Epoch 1085/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2546 - acc: 0.8889 - val_loss: 0.0357 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.02612\n",
      "Epoch 1086/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2546 - acc: 0.8667 - val_loss: 0.0320 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.02612\n",
      "Epoch 1087/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2477 - acc: 0.8889 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.02612\n",
      "Epoch 1088/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2700 - acc: 0.8889 - val_loss: 0.0234 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01088: val_loss improved from 0.02612 to 0.02338, saving model to ./model/vloss0.023_vacc1.000_ep1088.hdf5\n",
      "Epoch 1089/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2503 - acc: 0.8444 - val_loss: 0.0417 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.02338\n",
      "Epoch 1090/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2656 - acc: 0.8667 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.02338\n",
      "Epoch 1091/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2585 - acc: 0.8667 - val_loss: 0.0316 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.02338\n",
      "Epoch 1092/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2444 - acc: 0.8667 - val_loss: 0.0363 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.02338\n",
      "Epoch 1093/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2520 - acc: 0.8889 - val_loss: 0.0332 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.02338\n",
      "Epoch 1094/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2526 - acc: 0.8889 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.02338\n",
      "Epoch 1095/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2614 - acc: 0.8444 - val_loss: 0.0476 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.02338\n",
      "Epoch 1096/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2390 - acc: 0.9111 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.02338\n",
      "Epoch 1097/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2549 - acc: 0.8667 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.02338\n",
      "Epoch 1098/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2701 - acc: 0.8222 - val_loss: 0.0403 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.02338\n",
      "Epoch 1099/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2480 - acc: 0.8889 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.02338\n",
      "Epoch 1100/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2548 - acc: 0.8667 - val_loss: 0.0326 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.02338\n",
      "Epoch 1101/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2527 - acc: 0.8667 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.02338\n",
      "Epoch 1102/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2505 - acc: 0.8889 - val_loss: 0.0345 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.02338\n",
      "Epoch 1103/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2521 - acc: 0.8667 - val_loss: 0.0334 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.02338\n",
      "Epoch 1104/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2536 - acc: 0.8889 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.02338\n",
      "Epoch 1105/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2465 - acc: 0.8667 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.02338\n",
      "Epoch 1106/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2588 - acc: 0.8667 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.02338\n",
      "Epoch 1107/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2548 - acc: 0.8667 - val_loss: 0.0368 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.02338\n",
      "Epoch 1108/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2583 - acc: 0.8444 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.02338\n",
      "Epoch 1109/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2473 - acc: 0.8667 - val_loss: 0.0278 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.02338\n",
      "Epoch 1110/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2444 - acc: 0.8889 - val_loss: 0.0327 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.02338\n",
      "Epoch 1111/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2594 - acc: 0.8667 - val_loss: 0.0340 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.02338\n",
      "Epoch 1112/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.2577 - acc: 0.8667 - val_loss: 0.0268 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.02338\n",
      "Epoch 1113/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2417 - acc: 0.8889 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.02338\n",
      "Epoch 1114/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2442 - acc: 0.8889 - val_loss: 0.0386 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.02338\n",
      "Epoch 1115/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2553 - acc: 0.8444 - val_loss: 0.0374 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.02338\n",
      "Epoch 1116/2000\n",
      "45/45 [==============================] - 0s 354us/step - loss: 0.2491 - acc: 0.8889 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.02338\n",
      "Epoch 1117/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2559 - acc: 0.8667 - val_loss: 0.0269 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01117: val_loss did not improve from 0.02338\n",
      "Epoch 1118/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2422 - acc: 0.8889 - val_loss: 0.0237 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.02338\n",
      "Epoch 1119/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2471 - acc: 0.8667 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.02338\n",
      "Epoch 1120/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2494 - acc: 0.8889 - val_loss: 0.0398 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.02338\n",
      "Epoch 1121/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2389 - acc: 0.8889 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.02338\n",
      "Epoch 1122/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2400 - acc: 0.8889 - val_loss: 0.0280 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.02338\n",
      "Epoch 1123/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2510 - acc: 0.8889 - val_loss: 0.0242 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.02338\n",
      "Epoch 1124/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2389 - acc: 0.8889 - val_loss: 0.0355 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.02338\n",
      "Epoch 1125/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2540 - acc: 0.8444 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.02338\n",
      "Epoch 1126/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2479 - acc: 0.8889 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.02338\n",
      "Epoch 1127/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2512 - acc: 0.8667 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.02338\n",
      "Epoch 1128/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.2610 - acc: 0.8222 - val_loss: 0.0368 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.02338\n",
      "Epoch 1129/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2512 - acc: 0.8444 - val_loss: 0.0287 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.02338\n",
      "Epoch 1130/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2453 - acc: 0.8667 - val_loss: 0.0260 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.02338\n",
      "Epoch 1131/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2506 - acc: 0.8889 - val_loss: 0.0278 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.02338\n",
      "Epoch 1132/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2479 - acc: 0.8667 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.02338\n",
      "Epoch 1133/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2454 - acc: 0.8667 - val_loss: 0.0328 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.02338\n",
      "Epoch 1134/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2471 - acc: 0.8444 - val_loss: 0.0267 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.02338\n",
      "Epoch 1135/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2454 - acc: 0.8667 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.02338\n",
      "Epoch 1136/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2595 - acc: 0.8889 - val_loss: 0.0285 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.02338\n",
      "Epoch 1137/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2389 - acc: 0.8889 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.02338\n",
      "Epoch 1138/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2422 - acc: 0.8889 - val_loss: 0.0312 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.02338\n",
      "Epoch 1139/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2497 - acc: 0.8667 - val_loss: 0.0269 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.02338\n",
      "Epoch 1140/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2467 - acc: 0.8667 - val_loss: 0.0270 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.02338\n",
      "Epoch 1141/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2485 - acc: 0.8889 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.02338\n",
      "Epoch 1142/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2392 - acc: 0.8889 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.02338\n",
      "Epoch 1143/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2408 - acc: 0.8667 - val_loss: 0.0337 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.02338\n",
      "Epoch 1144/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2384 - acc: 0.8889 - val_loss: 0.0282 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.02338\n",
      "Epoch 1145/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2425 - acc: 0.8889 - val_loss: 0.0251 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.02338\n",
      "Epoch 1146/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2487 - acc: 0.8889 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.02338\n",
      "Epoch 1147/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2814 - acc: 0.8444 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.02338\n",
      "Epoch 1148/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2431 - acc: 0.8889 - val_loss: 0.0251 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.02338\n",
      "Epoch 1149/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2458 - acc: 0.8889 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.02338\n",
      "Epoch 1150/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2607 - acc: 0.9111 - val_loss: 0.0502 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.02338\n",
      "Epoch 1151/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2734 - acc: 0.8444 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01151: val_loss improved from 0.02338 to 0.01867, saving model to ./model/vloss0.019_vacc1.000_ep1151.hdf5\n",
      "Epoch 1152/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2589 - acc: 0.8889 - val_loss: 0.0249 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 0.01867\n",
      "Epoch 1153/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2379 - acc: 0.8667 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.01867\n",
      "Epoch 1154/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2562 - acc: 0.8667 - val_loss: 0.0354 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.01867\n",
      "Epoch 1155/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2589 - acc: 0.8889 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.01867\n",
      "Epoch 1156/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2473 - acc: 0.8444 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.01867\n",
      "Epoch 1157/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2468 - acc: 0.8444 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.01867\n",
      "Epoch 1158/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2514 - acc: 0.8889 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.01867\n",
      "Epoch 1159/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2356 - acc: 0.8667 - val_loss: 0.0293 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.01867\n",
      "Epoch 1160/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2380 - acc: 0.9111 - val_loss: 0.0330 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.01867\n",
      "Epoch 1161/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 288us/step - loss: 0.2370 - acc: 0.8889 - val_loss: 0.0238 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.01867\n",
      "Epoch 1162/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2468 - acc: 0.8889 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.01867\n",
      "Epoch 1163/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2514 - acc: 0.8889 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.01867\n",
      "Epoch 1164/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2418 - acc: 0.9111 - val_loss: 0.0264 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.01867\n",
      "Epoch 1165/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2354 - acc: 0.8889 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01165: val_loss improved from 0.01867 to 0.01841, saving model to ./model/vloss0.018_vacc1.000_ep1165.hdf5\n",
      "Epoch 1166/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2814 - acc: 0.8222 - val_loss: 0.0371 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.01841\n",
      "Epoch 1167/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2353 - acc: 0.8889 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.01841\n",
      "Epoch 1168/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2421 - acc: 0.8889 - val_loss: 0.0193 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.01841\n",
      "Epoch 1169/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2397 - acc: 0.8889 - val_loss: 0.0297 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.01841\n",
      "Epoch 1170/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2355 - acc: 0.8889 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.01841\n",
      "Epoch 1171/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2452 - acc: 0.8667 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.01841\n",
      "Epoch 1172/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2415 - acc: 0.8667 - val_loss: 0.0298 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.01841\n",
      "Epoch 1173/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2617 - acc: 0.8667 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01173: val_loss improved from 0.01841 to 0.01765, saving model to ./model/vloss0.018_vacc1.000_ep1173.hdf5\n",
      "Epoch 1174/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2400 - acc: 0.8667 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.01765\n",
      "Epoch 1175/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2436 - acc: 0.8889 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.01765\n",
      "Epoch 1176/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2353 - acc: 0.9111 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.01765\n",
      "Epoch 1177/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2500 - acc: 0.8667 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.01765\n",
      "Epoch 1178/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2347 - acc: 0.9111 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.01765\n",
      "Epoch 1179/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2346 - acc: 0.9111 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01179: val_loss improved from 0.01765 to 0.01679, saving model to ./model/vloss0.017_vacc1.000_ep1179.hdf5\n",
      "Epoch 1180/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2455 - acc: 0.8889 - val_loss: 0.0331 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.01679\n",
      "Epoch 1181/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2504 - acc: 0.8444 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 0.01679\n",
      "Epoch 1182/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2302 - acc: 0.9111 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.01679\n",
      "Epoch 1183/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2532 - acc: 0.8889 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.01679\n",
      "Epoch 1184/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2382 - acc: 0.8889 - val_loss: 0.0327 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.01679\n",
      "Epoch 1185/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2377 - acc: 0.8889 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.01679\n",
      "Epoch 1186/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2296 - acc: 0.9111 - val_loss: 0.0242 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.01679\n",
      "Epoch 1187/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2352 - acc: 0.8889 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.01679\n",
      "Epoch 1188/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2269 - acc: 0.8889 - val_loss: 0.0264 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.01679\n",
      "Epoch 1189/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2477 - acc: 0.8889 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.01679\n",
      "Epoch 1190/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2325 - acc: 0.9111 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.01679\n",
      "Epoch 1191/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2370 - acc: 0.8889 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.01679\n",
      "Epoch 1192/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2530 - acc: 0.8667 - val_loss: 0.0301 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.01679\n",
      "Epoch 1193/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2369 - acc: 0.8889 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.01679\n",
      "Epoch 1194/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2478 - acc: 0.8667 - val_loss: 0.0292 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.01679\n",
      "Epoch 1195/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2365 - acc: 0.8667 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.01679\n",
      "Epoch 1196/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2529 - acc: 0.8444 - val_loss: 0.0289 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.01679\n",
      "Epoch 1197/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2371 - acc: 0.8889 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.01679\n",
      "Epoch 1198/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2562 - acc: 0.8667 - val_loss: 0.0202 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.01679\n",
      "Epoch 1199/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2297 - acc: 0.8889 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.01679\n",
      "Epoch 1200/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2518 - acc: 0.8667 - val_loss: 0.0334 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 0.01679\n",
      "Epoch 1201/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2412 - acc: 0.8667 - val_loss: 0.0300 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.01679\n",
      "Epoch 1202/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2362 - acc: 0.8889 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.01679\n",
      "Epoch 1203/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2457 - acc: 0.8444 - val_loss: 0.0258 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.01679\n",
      "Epoch 1204/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2345 - acc: 0.8667 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.01679\n",
      "Epoch 1205/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2374 - acc: 0.8889 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.01679\n",
      "Epoch 1206/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2569 - acc: 0.8444 - val_loss: 0.0240 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.01679\n",
      "Epoch 1207/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2678 - acc: 0.8667 - val_loss: 0.0330 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.01679\n",
      "Epoch 1208/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2170 - acc: 0.8889 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.01679\n",
      "Epoch 1209/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2443 - acc: 0.8889 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01209: val_loss improved from 0.01679 to 0.01608, saving model to ./model/vloss0.016_vacc1.000_ep1209.hdf5\n",
      "Epoch 1210/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2361 - acc: 0.8889 - val_loss: 0.0199 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.01608\n",
      "Epoch 1211/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2633 - acc: 0.8667 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.01608\n",
      "Epoch 1212/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2285 - acc: 0.8667 - val_loss: 0.0235 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.01608\n",
      "Epoch 1213/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2303 - acc: 0.8889 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.01608\n",
      "Epoch 1214/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2299 - acc: 0.8889 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.01608\n",
      "Epoch 1215/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2367 - acc: 0.9111 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.01608\n",
      "Epoch 1216/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2743 - acc: 0.8667 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01216: val_loss improved from 0.01608 to 0.01607, saving model to ./model/vloss0.016_vacc1.000_ep1216.hdf5\n",
      "Epoch 1217/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2376 - acc: 0.8444 - val_loss: 0.0359 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.01607\n",
      "Epoch 1218/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2334 - acc: 0.8889 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.01607\n",
      "Epoch 1219/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2200 - acc: 0.9111 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01219: val_loss improved from 0.01607 to 0.01509, saving model to ./model/vloss0.015_vacc1.000_ep1219.hdf5\n",
      "Epoch 1220/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2432 - acc: 0.8889 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01220: val_loss improved from 0.01509 to 0.01342, saving model to ./model/vloss0.013_vacc1.000_ep1220.hdf5\n",
      "Epoch 1221/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2169 - acc: 0.9111 - val_loss: 0.0269 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.01342\n",
      "Epoch 1222/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2352 - acc: 0.8889 - val_loss: 0.0251 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.01342\n",
      "Epoch 1223/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2240 - acc: 0.8889 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.01342\n",
      "Epoch 1224/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2254 - acc: 0.8889 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.01342\n",
      "Epoch 1225/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2435 - acc: 0.9111 - val_loss: 0.0232 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.01342\n",
      "Epoch 1226/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2184 - acc: 0.8889 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.01342\n",
      "Epoch 1227/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2327 - acc: 0.8889 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.01342\n",
      "Epoch 1228/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2295 - acc: 0.9111 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.01342\n",
      "Epoch 1229/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2329 - acc: 0.8889 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.01342\n",
      "Epoch 1230/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2388 - acc: 0.8889 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.01342\n",
      "Epoch 1231/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2283 - acc: 0.9111 - val_loss: 0.0298 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.01342\n",
      "Epoch 1232/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2379 - acc: 0.8889 - val_loss: 0.0286 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.01342\n",
      "Epoch 1233/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2228 - acc: 0.8889 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.01342\n",
      "Epoch 1234/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2406 - acc: 0.8889 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.01342\n",
      "Epoch 1235/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2421 - acc: 0.8667 - val_loss: 0.0306 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.01342\n",
      "Epoch 1236/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2302 - acc: 0.8889 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.01342\n",
      "Epoch 1237/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2309 - acc: 0.8889 - val_loss: 0.0199 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.01342\n",
      "Epoch 1238/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2326 - acc: 0.8889 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.01342\n",
      "Epoch 1239/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2153 - acc: 0.8889 - val_loss: 0.0201 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.01342\n",
      "Epoch 1240/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2281 - acc: 0.8667 - val_loss: 0.0206 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.01342\n",
      "Epoch 1241/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.2398 - acc: 0.9111 - val_loss: 0.0202 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.01342\n",
      "Epoch 1242/2000\n",
      "45/45 [==============================] - 0s 621us/step - loss: 0.2383 - acc: 0.8889 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.01342\n",
      "Epoch 1243/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2381 - acc: 0.8889 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.01342\n",
      "Epoch 1244/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0249 - acc: 1.000 - 0s 310us/step - loss: 0.2210 - acc: 0.9111 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.01342\n",
      "Epoch 1245/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 288us/step - loss: 0.2277 - acc: 0.8889 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.01342\n",
      "Epoch 1246/2000\n",
      "45/45 [==============================] - 0s 621us/step - loss: 0.2428 - acc: 0.8444 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.01342\n",
      "Epoch 1247/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2630 - acc: 0.8444 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.01342\n",
      "Epoch 1248/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2200 - acc: 0.9111 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.01342\n",
      "Epoch 1249/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2307 - acc: 0.8889 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.01342\n",
      "Epoch 1250/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2388 - acc: 0.8889 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.01342\n",
      "Epoch 1251/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2253 - acc: 0.8889 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.01342\n",
      "Epoch 1252/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2276 - acc: 0.8889 - val_loss: 0.0261 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.01342\n",
      "Epoch 1253/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2355 - acc: 0.8889 - val_loss: 0.0162 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.01342\n",
      "Epoch 1254/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2270 - acc: 0.8889 - val_loss: 0.0221 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.01342\n",
      "Epoch 1255/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2267 - acc: 0.8889 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.01342\n",
      "Epoch 1256/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2214 - acc: 0.8667 - val_loss: 0.0209 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.01342\n",
      "Epoch 1257/2000\n",
      "45/45 [==============================] - 0s 354us/step - loss: 0.2202 - acc: 0.8889 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.01342\n",
      "Epoch 1258/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2317 - acc: 0.8667 - val_loss: 0.0180 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.01342\n",
      "Epoch 1259/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2307 - acc: 0.8889 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.01342\n",
      "Epoch 1260/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2439 - acc: 0.8667 - val_loss: 0.0271 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.01342\n",
      "Epoch 1261/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2293 - acc: 0.8667 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.01342\n",
      "Epoch 1262/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2291 - acc: 0.8889 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.01342\n",
      "Epoch 1263/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2539 - acc: 0.8667 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.01342\n",
      "Epoch 1264/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2537 - acc: 0.8444 - val_loss: 0.0203 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.01342\n",
      "Epoch 1265/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2316 - acc: 0.9111 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.01342\n",
      "Epoch 1266/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2411 - acc: 0.8667 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.01342\n",
      "Epoch 1267/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2309 - acc: 0.8667 - val_loss: 0.0217 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.01342\n",
      "Epoch 1268/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2536 - acc: 0.9111 - val_loss: 0.0279 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.01342\n",
      "Epoch 1269/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2166 - acc: 0.9111 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.01342\n",
      "Epoch 1270/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2349 - acc: 0.8889 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01270: val_loss improved from 0.01342 to 0.01295, saving model to ./model/vloss0.013_vacc1.000_ep1270.hdf5\n",
      "Epoch 1271/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2397 - acc: 0.8444 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.01295\n",
      "Epoch 1272/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2221 - acc: 0.9111 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.01295\n",
      "Epoch 1273/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2161 - acc: 0.9111 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.01295\n",
      "Epoch 1274/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2311 - acc: 0.8889 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.01295\n",
      "Epoch 1275/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2270 - acc: 0.8889 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.01295\n",
      "Epoch 1276/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2310 - acc: 0.8889 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.01295\n",
      "Epoch 1277/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2434 - acc: 0.8667 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01277: val_loss improved from 0.01295 to 0.01265, saving model to ./model/vloss0.013_vacc1.000_ep1277.hdf5\n",
      "Epoch 1278/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2320 - acc: 0.8889 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.01265\n",
      "Epoch 1279/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2202 - acc: 0.8889 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.01265\n",
      "Epoch 1280/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2247 - acc: 0.8889 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.01265\n",
      "Epoch 1281/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2249 - acc: 0.8889 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.01265\n",
      "Epoch 1282/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2232 - acc: 0.8889 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.01265\n",
      "Epoch 1283/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2264 - acc: 0.8889 - val_loss: 0.0274 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.01265\n",
      "Epoch 1284/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2214 - acc: 0.8889 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.01265\n",
      "Epoch 1285/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2378 - acc: 0.8667 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.01265\n",
      "Epoch 1286/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2323 - acc: 0.8889 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.01265\n",
      "Epoch 1287/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2431 - acc: 0.8889 - val_loss: 0.0299 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.01265\n",
      "Epoch 1288/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 355us/step - loss: 0.2476 - acc: 0.8889 - val_loss: 0.0124 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01288: val_loss improved from 0.01265 to 0.01235, saving model to ./model/vloss0.012_vacc1.000_ep1288.hdf5\n",
      "Epoch 1289/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2191 - acc: 0.8889 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.01235\n",
      "Epoch 1290/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2209 - acc: 0.8667 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.01235\n",
      "Epoch 1291/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2478 - acc: 0.8667 - val_loss: 0.0202 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.01235\n",
      "Epoch 1292/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2165 - acc: 0.8889 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.01235\n",
      "Epoch 1293/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2247 - acc: 0.8889 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.01235\n",
      "Epoch 1294/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2217 - acc: 0.8889 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.01235\n",
      "Epoch 1295/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2425 - acc: 0.8889 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.01235\n",
      "Epoch 1296/2000\n",
      "45/45 [==============================] - 0s 222us/step - loss: 0.2154 - acc: 0.9111 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.01235\n",
      "Epoch 1297/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2235 - acc: 0.9111 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.01235\n",
      "Epoch 1298/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2175 - acc: 0.8889 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.01235\n",
      "Epoch 1299/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2241 - acc: 0.9111 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.01235\n",
      "Epoch 1300/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2205 - acc: 0.8889 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.01235\n",
      "Epoch 1301/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2230 - acc: 0.8889 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.01235\n",
      "Epoch 1302/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2265 - acc: 0.9111 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.01235\n",
      "Epoch 1303/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2244 - acc: 0.9111 - val_loss: 0.0255 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.01235\n",
      "Epoch 1304/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2376 - acc: 0.9111 - val_loss: 0.0160 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.01235\n",
      "Epoch 1305/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2198 - acc: 0.8889 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.01235\n",
      "Epoch 1306/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2157 - acc: 0.8889 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.01235\n",
      "Epoch 1307/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2257 - acc: 0.8667 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.01235\n",
      "Epoch 1308/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2118 - acc: 0.9111 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.01235\n",
      "Epoch 1309/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2231 - acc: 0.8889 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 0.01235\n",
      "Epoch 1310/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2222 - acc: 0.9111 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.01235\n",
      "Epoch 1311/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2214 - acc: 0.8889 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.01235\n",
      "Epoch 1312/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2296 - acc: 0.8889 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.01235\n",
      "Epoch 1313/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2145 - acc: 0.8667 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.01235\n",
      "Epoch 1314/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2183 - acc: 0.8667 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.01235\n",
      "Epoch 1315/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2193 - acc: 0.8889 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 0.01235\n",
      "Epoch 1316/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2373 - acc: 0.8889 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 0.01235\n",
      "Epoch 1317/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2138 - acc: 0.8889 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 0.01235\n",
      "Epoch 1318/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2214 - acc: 0.8889 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.01235\n",
      "Epoch 1319/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2272 - acc: 0.8667 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01319: val_loss improved from 0.01235 to 0.01180, saving model to ./model/vloss0.012_vacc1.000_ep1319.hdf5\n",
      "Epoch 1320/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2341 - acc: 0.8889 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01320: val_loss improved from 0.01180 to 0.01104, saving model to ./model/vloss0.011_vacc1.000_ep1320.hdf5\n",
      "Epoch 1321/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2294 - acc: 0.8667 - val_loss: 0.0259 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.01104\n",
      "Epoch 1322/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2268 - acc: 0.8889 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.01104\n",
      "Epoch 1323/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2203 - acc: 0.9111 - val_loss: 0.0203 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.01104\n",
      "Epoch 1324/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2182 - acc: 0.8889 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.01104\n",
      "Epoch 1325/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2234 - acc: 0.8889 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 0.01104\n",
      "Epoch 1326/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2120 - acc: 0.9111 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.01104\n",
      "Epoch 1327/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2159 - acc: 0.8889 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.01104\n",
      "Epoch 1328/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2153 - acc: 0.8889 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.01104\n",
      "Epoch 1329/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2088 - acc: 0.8889 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.01104\n",
      "Epoch 1330/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2176 - acc: 0.9111 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.01104\n",
      "Epoch 1331/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2116 - acc: 0.9111 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.01104\n",
      "Epoch 1332/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2105 - acc: 0.9111 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.01104\n",
      "Epoch 1333/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2304 - acc: 0.8667 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.01104\n",
      "Epoch 1334/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2397 - acc: 0.8889 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.01104\n",
      "Epoch 1335/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2338 - acc: 0.8889 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.01104\n",
      "Epoch 1336/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2193 - acc: 0.9111 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.01104\n",
      "Epoch 1337/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0137 - acc: 1.000 - 0s 510us/step - loss: 0.2204 - acc: 0.8889 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.01104\n",
      "Epoch 1338/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.2228 - acc: 0.9111 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.01104\n",
      "Epoch 1339/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2124 - acc: 0.8889 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.01104\n",
      "Epoch 1340/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2362 - acc: 0.8667 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.01104\n",
      "Epoch 1341/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2331 - acc: 0.8889 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.01104\n",
      "Epoch 1342/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2192 - acc: 0.9111 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.01104\n",
      "Epoch 1343/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2306 - acc: 0.9111 - val_loss: 0.0237 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.01104\n",
      "Epoch 1344/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2246 - acc: 0.9111 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01344: val_loss improved from 0.01104 to 0.01062, saving model to ./model/vloss0.011_vacc1.000_ep1344.hdf5\n",
      "Epoch 1345/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2371 - acc: 0.8667 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.01062\n",
      "Epoch 1346/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2352 - acc: 0.9111 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.01062\n",
      "Epoch 1347/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2174 - acc: 0.8889 - val_loss: 0.0124 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.01062\n",
      "Epoch 1348/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2224 - acc: 0.9111 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.01062\n",
      "Epoch 1349/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2264 - acc: 0.8667 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 0.01062\n",
      "Epoch 1350/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2208 - acc: 0.8889 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.01062\n",
      "Epoch 1351/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2316 - acc: 0.8667 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01351: val_loss improved from 0.01062 to 0.00949, saving model to ./model/vloss0.009_vacc1.000_ep1351.hdf5\n",
      "Epoch 1352/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2117 - acc: 0.8889 - val_loss: 0.0141 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.00949\n",
      "Epoch 1353/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2096 - acc: 0.9111 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 0.00949\n",
      "Epoch 1354/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2255 - acc: 0.8889 - val_loss: 0.0202 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.00949\n",
      "Epoch 1355/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2319 - acc: 0.8667 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 0.00949\n",
      "Epoch 1356/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2208 - acc: 0.8444 - val_loss: 0.0160 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.00949\n",
      "Epoch 1357/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2069 - acc: 0.9111 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.00949\n",
      "Epoch 1358/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2161 - acc: 0.8889 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.00949\n",
      "Epoch 1359/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2220 - acc: 0.9111 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.00949\n",
      "Epoch 1360/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2124 - acc: 0.9111 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.00949\n",
      "Epoch 1361/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2245 - acc: 0.9111 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.00949\n",
      "Epoch 1362/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2254 - acc: 0.8667 - val_loss: 0.0220 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.00949\n",
      "Epoch 1363/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2226 - acc: 0.8889 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.00949\n",
      "Epoch 1364/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2192 - acc: 0.8667 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.00949\n",
      "Epoch 1365/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2158 - acc: 0.8889 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.00949\n",
      "Epoch 1366/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2168 - acc: 0.9111 - val_loss: 0.0142 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.00949\n",
      "Epoch 1367/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2292 - acc: 0.8667 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.00949\n",
      "Epoch 1368/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2117 - acc: 0.9111 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.00949\n",
      "Epoch 1369/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2451 - acc: 0.8889 - val_loss: 0.0199 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.00949\n",
      "Epoch 1370/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2097 - acc: 0.9111 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.00949\n",
      "Epoch 1371/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2211 - acc: 0.8889 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.00949\n",
      "Epoch 1372/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2231 - acc: 0.8889 - val_loss: 0.0150 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01372: val_loss did not improve from 0.00949\n",
      "Epoch 1373/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2275 - acc: 0.9111 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01373: val_loss improved from 0.00949 to 0.00779, saving model to ./model/vloss0.008_vacc1.000_ep1373.hdf5\n",
      "Epoch 1374/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.2208 - acc: 0.8667 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.00779\n",
      "Epoch 1375/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2182 - acc: 0.8889 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.00779\n",
      "Epoch 1376/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2201 - acc: 0.8667 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 0.00779\n",
      "Epoch 1377/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2155 - acc: 0.9111 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 0.00779\n",
      "Epoch 1378/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2258 - acc: 0.9111 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.00779\n",
      "Epoch 1379/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2255 - acc: 0.8889 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 0.00779\n",
      "Epoch 1380/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2105 - acc: 0.8667 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.00779\n",
      "Epoch 1381/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2093 - acc: 0.9111 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 0.00779\n",
      "Epoch 1382/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2170 - acc: 0.8889 - val_loss: 0.0100 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.00779\n",
      "Epoch 1383/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2112 - acc: 0.8889 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.00779\n",
      "Epoch 1384/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2272 - acc: 0.8667 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 0.00779\n",
      "Epoch 1385/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2180 - acc: 0.8889 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 0.00779\n",
      "Epoch 1386/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2267 - acc: 0.8889 - val_loss: 0.0142 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 0.00779\n",
      "Epoch 1387/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2072 - acc: 0.9111 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.00779\n",
      "Epoch 1388/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2239 - acc: 0.9111 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.00779\n",
      "Epoch 1389/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2062 - acc: 0.8889 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.00779\n",
      "Epoch 1390/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2236 - acc: 0.8667 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.00779\n",
      "Epoch 1391/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2013 - acc: 0.9111 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.00779\n",
      "Epoch 1392/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2241 - acc: 0.8889 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 0.00779\n",
      "Epoch 1393/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2288 - acc: 0.8444 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01393: val_loss improved from 0.00779 to 0.00749, saving model to ./model/vloss0.007_vacc1.000_ep1393.hdf5\n",
      "Epoch 1394/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2183 - acc: 0.8889 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.00749\n",
      "Epoch 1395/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2113 - acc: 0.9111 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 0.00749\n",
      "Epoch 1396/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2040 - acc: 0.9111 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 0.00749\n",
      "Epoch 1397/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2031 - acc: 0.8889 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.00749\n",
      "Epoch 1398/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2310 - acc: 0.8444 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.00749\n",
      "Epoch 1399/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2270 - acc: 0.8444 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.00749\n",
      "Epoch 1400/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2093 - acc: 0.8889 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.00749\n",
      "Epoch 1401/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2101 - acc: 0.8889 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 0.00749\n",
      "Epoch 1402/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2105 - acc: 0.8889 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.00749\n",
      "Epoch 1403/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2288 - acc: 0.8667 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.00749\n",
      "Epoch 1404/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2050 - acc: 0.9111 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.00749\n",
      "Epoch 1405/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2178 - acc: 0.8667 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.00749\n",
      "Epoch 1406/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2096 - acc: 0.9333 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 0.00749\n",
      "Epoch 1407/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2249 - acc: 0.8889 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 0.00749\n",
      "Epoch 1408/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2183 - acc: 0.8889 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 0.00749\n",
      "Epoch 1409/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2154 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 0.00749\n",
      "Epoch 1410/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2306 - acc: 0.8889 - val_loss: 0.0200 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 0.00749\n",
      "Epoch 1411/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2273 - acc: 0.8444 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.00749\n",
      "Epoch 1412/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2146 - acc: 0.8667 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.00749\n",
      "Epoch 1413/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2246 - acc: 0.8667 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.00749\n",
      "Epoch 1414/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2171 - acc: 0.8667 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.00749\n",
      "Epoch 1415/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.2298 - acc: 0.8889 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.00749\n",
      "Epoch 1416/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1962 - acc: 0.9111 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.00749\n",
      "Epoch 1417/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2118 - acc: 0.8889 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 0.00749\n",
      "Epoch 1418/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2135 - acc: 0.8889 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.00749\n",
      "Epoch 1419/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2419 - acc: 0.8889 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 0.00749\n",
      "Epoch 1420/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2035 - acc: 0.8889 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 0.00749\n",
      "Epoch 1421/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2163 - acc: 0.8889 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.00749\n",
      "Epoch 1422/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2042 - acc: 0.9111 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 0.00749\n",
      "Epoch 1423/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2175 - acc: 0.9111 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 0.00749\n",
      "Epoch 1424/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2095 - acc: 0.8667 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.00749\n",
      "Epoch 1425/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2108 - acc: 0.8889 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 0.00749\n",
      "Epoch 1426/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2052 - acc: 0.8889 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.00749\n",
      "Epoch 1427/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2208 - acc: 0.8889 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 0.00749\n",
      "Epoch 1428/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1957 - acc: 0.9111 - val_loss: 0.0141 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 0.00749\n",
      "Epoch 1429/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2096 - acc: 0.8889 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 0.00749\n",
      "Epoch 1430/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2078 - acc: 0.8889 - val_loss: 0.0120 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.00749\n",
      "Epoch 1431/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2285 - acc: 0.8000 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 0.00749\n",
      "Epoch 1432/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2170 - acc: 0.8889 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 0.00749\n",
      "Epoch 1433/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2130 - acc: 0.9111 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.00749\n",
      "Epoch 1434/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2507 - acc: 0.8667 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01434: val_loss improved from 0.00749 to 0.00638, saving model to ./model/vloss0.006_vacc1.000_ep1434.hdf5\n",
      "Epoch 1435/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2034 - acc: 0.9111 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.00638\n",
      "Epoch 1436/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2048 - acc: 0.9111 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 0.00638\n",
      "Epoch 1437/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2038 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 0.00638\n",
      "Epoch 1438/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2056 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.00638\n",
      "Epoch 1439/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2186 - acc: 0.8889 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.00638\n",
      "Epoch 1440/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2270 - acc: 0.9111 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.00638\n",
      "Epoch 1441/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3339 - acc: 0.800 - 0s 310us/step - loss: 0.2098 - acc: 0.8889 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.00638\n",
      "Epoch 1442/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2163 - acc: 0.8889 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.00638\n",
      "Epoch 1443/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1937 - acc: 0.9111 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.00638\n",
      "Epoch 1444/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2009 - acc: 0.8889 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.00638\n",
      "Epoch 1445/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2410 - acc: 0.8667 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.00638\n",
      "Epoch 1446/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2035 - acc: 0.8667 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 0.00638\n",
      "Epoch 1447/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2198 - acc: 0.9111 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.00638\n",
      "Epoch 1448/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2078 - acc: 0.8889 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 0.00638\n",
      "Epoch 1449/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 0.2314 - acc: 0.8667 - val_loss: 0.0179 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.00638\n",
      "Epoch 1450/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2135 - acc: 0.9111 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 0.00638\n",
      "Epoch 1451/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2131 - acc: 0.8889 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01451: val_loss improved from 0.00638 to 0.00581, saving model to ./model/vloss0.006_vacc1.000_ep1451.hdf5\n",
      "Epoch 1452/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2041 - acc: 0.9111 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.00581\n",
      "Epoch 1453/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2142 - acc: 0.9111 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.00581\n",
      "Epoch 1454/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1989 - acc: 0.9111 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 0.00581\n",
      "Epoch 1455/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2134 - acc: 0.8889 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 0.00581\n",
      "Epoch 1456/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2205 - acc: 0.8667 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 0.00581\n",
      "Epoch 1457/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2141 - acc: 0.8667 - val_loss: 0.0128 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01457: val_loss did not improve from 0.00581\n",
      "Epoch 1458/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.2786 - acc: 0.800 - 0s 288us/step - loss: 0.2089 - acc: 0.9111 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.00581\n",
      "Epoch 1459/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2123 - acc: 0.9111 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.00581\n",
      "Epoch 1460/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2166 - acc: 0.8667 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.00581\n",
      "Epoch 1461/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2295 - acc: 0.8889 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 0.00581\n",
      "Epoch 1462/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2006 - acc: 0.9111 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.00581\n",
      "Epoch 1463/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2112 - acc: 0.9111 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.00581\n",
      "Epoch 1464/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2095 - acc: 0.8889 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.00581\n",
      "Epoch 1465/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2044 - acc: 0.8889 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.00581\n",
      "Epoch 1466/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2080 - acc: 0.8889 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.00581\n",
      "Epoch 1467/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2040 - acc: 0.8889 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.00581\n",
      "Epoch 1468/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2276 - acc: 0.8667 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.00581\n",
      "Epoch 1469/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2010 - acc: 0.8889 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.00581\n",
      "Epoch 1470/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2328 - acc: 0.8444 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 0.00581\n",
      "Epoch 1471/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2251 - acc: 0.8667 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 0.00581\n",
      "Epoch 1472/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2281 - acc: 0.9111 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 0.00581\n",
      "Epoch 1473/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2005 - acc: 0.9111 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 0.00581\n",
      "Epoch 1474/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2009 - acc: 0.9111 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.00581\n",
      "Epoch 1475/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2036 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 0.00581\n",
      "Epoch 1476/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1998 - acc: 0.8889 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.00581\n",
      "Epoch 1477/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.1976 - acc: 0.9111 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.00581\n",
      "Epoch 1478/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2139 - acc: 0.8889 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.00581\n",
      "Epoch 1479/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2011 - acc: 0.8889 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.00581\n",
      "Epoch 1480/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1974 - acc: 0.9111 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.00581\n",
      "Epoch 1481/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1972 - acc: 0.9111 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.00581\n",
      "Epoch 1482/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2109 - acc: 0.8889 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.00581\n",
      "Epoch 1483/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1918 - acc: 0.9111 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.00581\n",
      "Epoch 1484/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2252 - acc: 0.8889 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 0.00581\n",
      "Epoch 1485/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1952 - acc: 0.9111 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.00581\n",
      "Epoch 1486/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2107 - acc: 0.8889 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.00581\n",
      "Epoch 1487/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2089 - acc: 0.9111 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.00581\n",
      "Epoch 1488/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2013 - acc: 0.8667 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 0.00581\n",
      "Epoch 1489/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2107 - acc: 0.8667 - val_loss: 0.0140 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.00581\n",
      "Epoch 1490/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.2162 - acc: 0.8667 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 0.00581\n",
      "Epoch 1491/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2313 - acc: 0.8222 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.00581\n",
      "Epoch 1492/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2023 - acc: 0.8889 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.00581\n",
      "Epoch 1493/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1961 - acc: 0.9111 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.00581\n",
      "Epoch 1494/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2002 - acc: 0.9111 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.00581\n",
      "Epoch 1495/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2083 - acc: 0.8889 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.00581\n",
      "Epoch 1496/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2077 - acc: 0.8889 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 0.00581\n",
      "Epoch 1497/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.2023 - acc: 0.8889 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 0.00581\n",
      "Epoch 1498/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1989 - acc: 0.8889 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.00581\n",
      "Epoch 1499/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2249 - acc: 0.8889 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 0.00581\n",
      "Epoch 1500/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1957 - acc: 0.9333 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.00581\n",
      "Epoch 1501/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 355us/step - loss: 0.2020 - acc: 0.8889 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 0.00581\n",
      "Epoch 1502/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2029 - acc: 0.9111 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 0.00581\n",
      "Epoch 1503/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2269 - acc: 0.8889 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 0.00581\n",
      "Epoch 1504/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2254 - acc: 0.8889 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 0.00581\n",
      "Epoch 1505/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2042 - acc: 0.8667 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 0.00581\n",
      "Epoch 1506/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2213 - acc: 0.8889 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01506: val_loss improved from 0.00581 to 0.00533, saving model to ./model/vloss0.005_vacc1.000_ep1506.hdf5\n",
      "Epoch 1507/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1981 - acc: 0.9111 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 0.00533\n",
      "Epoch 1508/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2062 - acc: 0.8889 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 0.00533\n",
      "Epoch 1509/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2120 - acc: 0.8889 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 0.00533\n",
      "Epoch 1510/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2085 - acc: 0.8889 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 0.00533\n",
      "Epoch 1511/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2068 - acc: 0.8667 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 0.00533\n",
      "Epoch 1512/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2037 - acc: 0.8889 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 0.00533\n",
      "Epoch 1513/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2036 - acc: 0.9111 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 0.00533\n",
      "Epoch 1514/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2116 - acc: 0.9111 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 0.00533\n",
      "Epoch 1515/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2061 - acc: 0.9111 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 0.00533\n",
      "Epoch 1516/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2312 - acc: 0.8667 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 0.00533\n",
      "Epoch 1517/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1989 - acc: 0.8889 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 0.00533\n",
      "Epoch 1518/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1935 - acc: 0.9111 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 0.00533\n",
      "Epoch 1519/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2100 - acc: 0.9111 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 0.00533\n",
      "Epoch 1520/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2115 - acc: 0.9111 - val_loss: 0.0124 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 0.00533\n",
      "Epoch 1521/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2038 - acc: 0.9111 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 0.00533\n",
      "Epoch 1522/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2243 - acc: 0.8889 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 0.00533\n",
      "Epoch 1523/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2021 - acc: 0.8889 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 0.00533\n",
      "Epoch 1524/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1947 - acc: 0.9111 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 0.00533\n",
      "Epoch 1525/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1969 - acc: 0.8889 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 0.00533\n",
      "Epoch 1526/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1996 - acc: 0.8889 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 0.00533\n",
      "Epoch 1527/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1936 - acc: 0.8889 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 0.00533\n",
      "Epoch 1528/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2080 - acc: 0.8889 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 0.00533\n",
      "Epoch 1529/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1974 - acc: 0.8889 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 0.00533\n",
      "Epoch 1530/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2129 - acc: 0.8889 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 0.00533\n",
      "Epoch 1531/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2012 - acc: 0.9111 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 0.00533\n",
      "Epoch 1532/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.1977 - acc: 0.9111 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 0.00533\n",
      "Epoch 1533/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1990 - acc: 0.8889 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 0.00533\n",
      "Epoch 1534/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1979 - acc: 0.8667 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 0.00533\n",
      "Epoch 1535/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1925 - acc: 0.8889 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 0.00533\n",
      "Epoch 1536/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2033 - acc: 0.8889 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 0.00533\n",
      "Epoch 1537/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.2087 - acc: 0.8889 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 0.00533\n",
      "Epoch 1538/2000\n",
      "45/45 [==============================] - 0s 222us/step - loss: 0.2029 - acc: 0.9111 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 0.00533\n",
      "Epoch 1539/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1935 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 0.00533\n",
      "Epoch 1540/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2003 - acc: 0.8889 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 0.00533\n",
      "Epoch 1541/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1948 - acc: 0.9111 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 0.00533\n",
      "Epoch 1542/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1980 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01542: val_loss improved from 0.00533 to 0.00532, saving model to ./model/vloss0.005_vacc1.000_ep1542.hdf5\n",
      "Epoch 1543/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2030 - acc: 0.9111 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01543: val_loss improved from 0.00532 to 0.00525, saving model to ./model/vloss0.005_vacc1.000_ep1543.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1544/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0066 - acc: 1.000 - 0s 310us/step - loss: 0.2174 - acc: 0.8667 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 0.00525\n",
      "Epoch 1545/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.1099 - acc: 1.000 - 0s 309us/step - loss: 0.2115 - acc: 0.8667 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 0.00525\n",
      "Epoch 1546/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1968 - acc: 0.9111 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 0.00525\n",
      "Epoch 1547/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1972 - acc: 0.8889 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 0.00525\n",
      "Epoch 1548/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2071 - acc: 0.8889 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 0.00525\n",
      "Epoch 1549/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2168 - acc: 0.8444 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 0.00525\n",
      "Epoch 1550/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1954 - acc: 0.9111 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 0.00525\n",
      "Epoch 1551/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2089 - acc: 0.9111 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 0.00525\n",
      "Epoch 1552/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1975 - acc: 0.9111 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 0.00525\n",
      "Epoch 1553/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1956 - acc: 0.8889 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 0.00525\n",
      "Epoch 1554/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2036 - acc: 0.8667 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 0.00525\n",
      "Epoch 1555/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1937 - acc: 0.8667 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 0.00525\n",
      "Epoch 1556/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1935 - acc: 0.9111 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 0.00525\n",
      "Epoch 1557/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2132 - acc: 0.9111 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 0.00525\n",
      "Epoch 1558/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2286 - acc: 0.8889 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01558: val_loss did not improve from 0.00525\n",
      "Epoch 1559/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1992 - acc: 0.8889 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 0.00525\n",
      "Epoch 1560/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1985 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01560: val_loss improved from 0.00525 to 0.00445, saving model to ./model/vloss0.004_vacc1.000_ep1560.hdf5\n",
      "Epoch 1561/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1932 - acc: 0.9111 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 0.00445\n",
      "Epoch 1562/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2075 - acc: 0.8889 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 0.00445\n",
      "Epoch 1563/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1898 - acc: 0.8889 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 0.00445\n",
      "Epoch 1564/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1993 - acc: 0.9111 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 0.00445\n",
      "Epoch 1565/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2030 - acc: 0.8889 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 0.00445\n",
      "Epoch 1566/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1962 - acc: 0.9111 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 0.00445\n",
      "Epoch 1567/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2015 - acc: 0.8667 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 0.00445\n",
      "Epoch 1568/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2048 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01568: val_loss improved from 0.00445 to 0.00390, saving model to ./model/vloss0.004_vacc1.000_ep1568.hdf5\n",
      "Epoch 1569/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2005 - acc: 0.8889 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 0.00390\n",
      "Epoch 1570/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2047 - acc: 0.9111 - val_loss: 0.0113 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 0.00390\n",
      "Epoch 1571/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1922 - acc: 0.9111 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 0.00390\n",
      "Epoch 1572/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.2107 - acc: 0.9111 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 0.00390\n",
      "Epoch 1573/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2056 - acc: 0.8667 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 0.00390\n",
      "Epoch 1574/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1967 - acc: 0.8889 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 0.00390\n",
      "Epoch 1575/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1989 - acc: 0.9111 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 0.00390\n",
      "Epoch 1576/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1968 - acc: 0.9111 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 0.00390\n",
      "Epoch 1577/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1991 - acc: 0.9111 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 0.00390\n",
      "Epoch 1578/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1928 - acc: 0.8889 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 0.00390\n",
      "Epoch 1579/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2186 - acc: 0.9111 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 0.00390\n",
      "Epoch 1580/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1937 - acc: 0.9333 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 0.00390\n",
      "Epoch 1581/2000\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.2106 - acc: 0.8889 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 0.00390\n",
      "Epoch 1582/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2052 - acc: 0.8889 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 0.00390\n",
      "Epoch 1583/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2068 - acc: 0.8889 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 0.00390\n",
      "Epoch 1584/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1871 - acc: 0.9111 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 0.00390\n",
      "Epoch 1585/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2061 - acc: 0.8889 - val_loss: 0.0086 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 0.00390\n",
      "Epoch 1586/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.1925 - acc: 0.8889 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 0.00390\n",
      "Epoch 1587/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2023 - acc: 0.8889 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 0.00390\n",
      "Epoch 1588/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2055 - acc: 0.8889 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 0.00390\n",
      "Epoch 1589/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2050 - acc: 0.9111 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 0.00390\n",
      "Epoch 1590/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2484 - acc: 0.8444 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 0.00390\n",
      "Epoch 1591/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1867 - acc: 0.9111 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 0.00390\n",
      "Epoch 1592/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1984 - acc: 0.9111 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 0.00390\n",
      "Epoch 1593/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2025 - acc: 0.8889 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01593: val_loss did not improve from 0.00390\n",
      "Epoch 1594/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1928 - acc: 0.9111 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 0.00390\n",
      "Epoch 1595/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1904 - acc: 0.9111 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 0.00390\n",
      "Epoch 1596/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1951 - acc: 0.8667 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 0.00390\n",
      "Epoch 1597/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2069 - acc: 0.8889 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 0.00390\n",
      "Epoch 1598/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2201 - acc: 0.8667 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 0.00390\n",
      "Epoch 1599/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2148 - acc: 0.9111 - val_loss: 0.0142 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 0.00390\n",
      "Epoch 1600/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2097 - acc: 0.9111 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 0.00390\n",
      "Epoch 1601/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1986 - acc: 0.9111 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 0.00390\n",
      "Epoch 1602/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1951 - acc: 0.9111 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 0.00390\n",
      "Epoch 1603/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1999 - acc: 0.8889 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 0.00390\n",
      "Epoch 1604/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2084 - acc: 0.8667 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 0.00390\n",
      "Epoch 1605/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - 0s 288us/step - loss: 0.1934 - acc: 0.9111 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 0.00390\n",
      "Epoch 1606/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2099 - acc: 0.8889 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 0.00390\n",
      "Epoch 1607/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2174 - acc: 0.8667 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 0.00390\n",
      "Epoch 1608/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1928 - acc: 0.8889 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 0.00390\n",
      "Epoch 1609/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2067 - acc: 0.8889 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 0.00390\n",
      "Epoch 1610/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1994 - acc: 0.8889 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 0.00390\n",
      "Epoch 1611/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1892 - acc: 0.8889 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 0.00390\n",
      "Epoch 1612/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2075 - acc: 0.8667 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 0.00390\n",
      "Epoch 1613/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1948 - acc: 0.8889 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 0.00390\n",
      "Epoch 1614/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2053 - acc: 0.9111 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 0.00390\n",
      "Epoch 1615/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1948 - acc: 0.8667 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 0.00390\n",
      "Epoch 1616/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2017 - acc: 0.8889 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 0.00390\n",
      "Epoch 1617/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2021 - acc: 0.8889 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 0.00390\n",
      "Epoch 1618/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2095 - acc: 0.8667 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 0.00390\n",
      "Epoch 1619/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2171 - acc: 0.9111 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 0.00390\n",
      "Epoch 1620/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2331 - acc: 0.8889 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 0.00390\n",
      "Epoch 1621/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2084 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01621: val_loss improved from 0.00390 to 0.00358, saving model to ./model/vloss0.004_vacc1.000_ep1621.hdf5\n",
      "Epoch 1622/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2118 - acc: 0.9111 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 0.00358\n",
      "Epoch 1623/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2262 - acc: 0.8889 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 0.00358\n",
      "Epoch 1624/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2037 - acc: 0.9111 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01624: val_loss improved from 0.00358 to 0.00337, saving model to ./model/vloss0.003_vacc1.000_ep1624.hdf5\n",
      "Epoch 1625/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2060 - acc: 0.9111 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 0.00337\n",
      "Epoch 1626/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2047 - acc: 0.8667 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 0.00337\n",
      "Epoch 1627/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1945 - acc: 0.9111 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 0.00337\n",
      "Epoch 1628/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2027 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01628: val_loss did not improve from 0.00337\n",
      "Epoch 1629/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1988 - acc: 0.8889 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 0.00337\n",
      "Epoch 1630/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1987 - acc: 0.8889 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01630: val_loss did not improve from 0.00337\n",
      "Epoch 1631/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2140 - acc: 0.8889 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 0.00337\n",
      "Epoch 1632/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2053 - acc: 0.9111 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 0.00337\n",
      "Epoch 1633/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2009 - acc: 0.8667 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 0.00337\n",
      "Epoch 1634/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2035 - acc: 0.8889 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 0.00337\n",
      "Epoch 1635/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1801 - acc: 0.9111 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 0.00337\n",
      "Epoch 1636/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2078 - acc: 0.9111 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 0.00337\n",
      "Epoch 1637/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2148 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 0.00337\n",
      "Epoch 1638/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1931 - acc: 0.8889 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01638: val_loss did not improve from 0.00337\n",
      "Epoch 1639/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1930 - acc: 0.8889 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 0.00337\n",
      "Epoch 1640/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2002 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 0.00337\n",
      "Epoch 1641/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1861 - acc: 0.9111 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 0.00337\n",
      "Epoch 1642/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2123 - acc: 0.9111 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 0.00337\n",
      "Epoch 1643/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2122 - acc: 0.8444 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 0.00337\n",
      "Epoch 1644/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1971 - acc: 0.9111 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 0.00337\n",
      "Epoch 1645/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2108 - acc: 0.8889 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 0.00337\n",
      "Epoch 1646/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1894 - acc: 0.8889 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 0.00337\n",
      "Epoch 1647/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1893 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 0.00337\n",
      "Epoch 1648/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1937 - acc: 0.9111 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 0.00337\n",
      "Epoch 1649/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1900 - acc: 0.9111 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 0.00337\n",
      "Epoch 1650/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1984 - acc: 0.8889 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 0.00337\n",
      "Epoch 1651/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2212 - acc: 0.8889 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 0.00337\n",
      "Epoch 1652/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2070 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01652: val_loss did not improve from 0.00337\n",
      "Epoch 1653/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2070 - acc: 0.8667 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 0.00337\n",
      "Epoch 1654/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1916 - acc: 0.9111 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01654: val_loss did not improve from 0.00337\n",
      "Epoch 1655/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2099 - acc: 0.8889 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 0.00337\n",
      "Epoch 1656/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1913 - acc: 0.9111 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 0.00337\n",
      "Epoch 1657/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2146 - acc: 0.8444 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 0.00337\n",
      "Epoch 1658/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1895 - acc: 0.9111 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 0.00337\n",
      "Epoch 1659/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2025 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 0.00337\n",
      "Epoch 1660/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1971 - acc: 0.8667 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 0.00337\n",
      "Epoch 1661/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1972 - acc: 0.8889 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 0.00337\n",
      "Epoch 1662/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2121 - acc: 0.8889 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 0.00337\n",
      "Epoch 1663/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1852 - acc: 0.9333 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01663: val_loss did not improve from 0.00337\n",
      "Epoch 1664/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1934 - acc: 0.8889 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01664: val_loss did not improve from 0.00337\n",
      "Epoch 1665/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2148 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01665: val_loss did not improve from 0.00337\n",
      "Epoch 1666/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1828 - acc: 0.9333 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01666: val_loss did not improve from 0.00337\n",
      "Epoch 1667/2000\n",
      "45/45 [==============================] - 0s 222us/step - loss: 0.1954 - acc: 0.9111 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01667: val_loss did not improve from 0.00337\n",
      "Epoch 1668/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1963 - acc: 0.9111 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01668: val_loss did not improve from 0.00337\n",
      "Epoch 1669/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1936 - acc: 0.8889 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01669: val_loss did not improve from 0.00337\n",
      "Epoch 1670/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1974 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01670: val_loss did not improve from 0.00337\n",
      "Epoch 1671/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1998 - acc: 0.8667 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01671: val_loss did not improve from 0.00337\n",
      "Epoch 1672/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 288us/step - loss: 0.1892 - acc: 0.8889 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01672: val_loss did not improve from 0.00337\n",
      "Epoch 1673/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1977 - acc: 0.8889 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01673: val_loss did not improve from 0.00337\n",
      "Epoch 1674/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1988 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01674: val_loss did not improve from 0.00337\n",
      "Epoch 1675/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2049 - acc: 0.8667 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01675: val_loss did not improve from 0.00337\n",
      "Epoch 1676/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2237 - acc: 0.8889 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01676: val_loss did not improve from 0.00337\n",
      "Epoch 1677/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2506 - acc: 0.8667 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01677: val_loss improved from 0.00337 to 0.00279, saving model to ./model/vloss0.003_vacc1.000_ep1677.hdf5\n",
      "Epoch 1678/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2151 - acc: 0.8889 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01678: val_loss did not improve from 0.00279\n",
      "Epoch 1679/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2137 - acc: 0.9111 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01679: val_loss did not improve from 0.00279\n",
      "Epoch 1680/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1979 - acc: 0.8889 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01680: val_loss did not improve from 0.00279\n",
      "Epoch 1681/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2070 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01681: val_loss did not improve from 0.00279\n",
      "Epoch 1682/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1908 - acc: 0.9111 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01682: val_loss did not improve from 0.00279\n",
      "Epoch 1683/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1918 - acc: 0.9111 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01683: val_loss did not improve from 0.00279\n",
      "Epoch 1684/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2002 - acc: 0.8444 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01684: val_loss did not improve from 0.00279\n",
      "Epoch 1685/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2067 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01685: val_loss did not improve from 0.00279\n",
      "Epoch 1686/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1855 - acc: 0.9111 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01686: val_loss did not improve from 0.00279\n",
      "Epoch 1687/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2025 - acc: 0.8889 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01687: val_loss did not improve from 0.00279\n",
      "Epoch 1688/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1964 - acc: 0.8889 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01688: val_loss did not improve from 0.00279\n",
      "Epoch 1689/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2032 - acc: 0.8889 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01689: val_loss did not improve from 0.00279\n",
      "Epoch 1690/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2005 - acc: 0.8889 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01690: val_loss did not improve from 0.00279\n",
      "Epoch 1691/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2018 - acc: 0.8889 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01691: val_loss did not improve from 0.00279\n",
      "Epoch 1692/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1895 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01692: val_loss did not improve from 0.00279\n",
      "Epoch 1693/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1798 - acc: 0.9111 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01693: val_loss did not improve from 0.00279\n",
      "Epoch 1694/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1935 - acc: 0.9111 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01694: val_loss did not improve from 0.00279\n",
      "Epoch 1695/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1928 - acc: 0.8889 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01695: val_loss did not improve from 0.00279\n",
      "Epoch 1696/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1887 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01696: val_loss did not improve from 0.00279\n",
      "Epoch 1697/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1939 - acc: 0.8444 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01697: val_loss did not improve from 0.00279\n",
      "Epoch 1698/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1981 - acc: 0.8444 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01698: val_loss did not improve from 0.00279\n",
      "Epoch 1699/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1914 - acc: 0.9111 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01699: val_loss did not improve from 0.00279\n",
      "Epoch 1700/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1945 - acc: 0.9111 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01700: val_loss did not improve from 0.00279\n",
      "Epoch 1701/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2126 - acc: 0.8667 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01701: val_loss did not improve from 0.00279\n",
      "Epoch 1702/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1873 - acc: 0.9111 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01702: val_loss did not improve from 0.00279\n",
      "Epoch 1703/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1944 - acc: 0.8889 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01703: val_loss did not improve from 0.00279\n",
      "Epoch 1704/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2080 - acc: 0.9111 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01704: val_loss did not improve from 0.00279\n",
      "Epoch 1705/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.1899 - acc: 0.9111 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01705: val_loss did not improve from 0.00279\n",
      "Epoch 1706/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2052 - acc: 0.9111 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01706: val_loss did not improve from 0.00279\n",
      "Epoch 1707/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2026 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01707: val_loss did not improve from 0.00279\n",
      "Epoch 1708/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1858 - acc: 0.9111 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01708: val_loss did not improve from 0.00279\n",
      "Epoch 1709/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2053 - acc: 0.8667 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01709: val_loss did not improve from 0.00279\n",
      "Epoch 1710/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2046 - acc: 0.8889 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01710: val_loss did not improve from 0.00279\n",
      "Epoch 1711/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1921 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01711: val_loss did not improve from 0.00279\n",
      "Epoch 1712/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2082 - acc: 0.8889 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01712: val_loss improved from 0.00279 to 0.00229, saving model to ./model/vloss0.002_vacc1.000_ep1712.hdf5\n",
      "Epoch 1713/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.2018 - acc: 0.9111 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01713: val_loss did not improve from 0.00229\n",
      "Epoch 1714/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2014 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01714: val_loss did not improve from 0.00229\n",
      "Epoch 1715/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.2015 - acc: 0.8889 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01715: val_loss did not improve from 0.00229\n",
      "Epoch 1716/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1889 - acc: 0.8667 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01716: val_loss did not improve from 0.00229\n",
      "Epoch 1717/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2026 - acc: 0.8889 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01717: val_loss did not improve from 0.00229\n",
      "Epoch 1718/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1919 - acc: 0.8889 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01718: val_loss did not improve from 0.00229\n",
      "Epoch 1719/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1927 - acc: 0.8889 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01719: val_loss did not improve from 0.00229\n",
      "Epoch 1720/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1913 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01720: val_loss did not improve from 0.00229\n",
      "Epoch 1721/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1934 - acc: 0.8667 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01721: val_loss did not improve from 0.00229\n",
      "Epoch 1722/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1966 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01722: val_loss did not improve from 0.00229\n",
      "Epoch 1723/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2024 - acc: 0.8889 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01723: val_loss did not improve from 0.00229\n",
      "Epoch 1724/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2011 - acc: 0.8889 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01724: val_loss did not improve from 0.00229\n",
      "Epoch 1725/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1879 - acc: 0.9111 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01725: val_loss did not improve from 0.00229\n",
      "Epoch 1726/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1824 - acc: 0.8889 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01726: val_loss did not improve from 0.00229\n",
      "Epoch 1727/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1842 - acc: 0.8889 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01727: val_loss did not improve from 0.00229\n",
      "Epoch 1728/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1923 - acc: 0.9111 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01728: val_loss did not improve from 0.00229\n",
      "Epoch 1729/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1859 - acc: 0.8889 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01729: val_loss did not improve from 0.00229\n",
      "Epoch 1730/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1841 - acc: 0.8889 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01730: val_loss did not improve from 0.00229\n",
      "Epoch 1731/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1993 - acc: 0.9111 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01731: val_loss improved from 0.00229 to 0.00202, saving model to ./model/vloss0.002_vacc1.000_ep1731.hdf5\n",
      "Epoch 1732/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1943 - acc: 0.9111 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01732: val_loss did not improve from 0.00202\n",
      "Epoch 1733/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1777 - acc: 0.8889 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01733: val_loss did not improve from 0.00202\n",
      "Epoch 1734/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2040 - acc: 0.8889 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01734: val_loss did not improve from 0.00202\n",
      "Epoch 1735/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2004 - acc: 0.8889 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01735: val_loss did not improve from 0.00202\n",
      "Epoch 1736/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1790 - acc: 0.8889 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01736: val_loss did not improve from 0.00202\n",
      "Epoch 1737/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1841 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01737: val_loss did not improve from 0.00202\n",
      "Epoch 1738/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2013 - acc: 0.8667 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01738: val_loss did not improve from 0.00202\n",
      "Epoch 1739/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1899 - acc: 0.9111 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01739: val_loss did not improve from 0.00202\n",
      "Epoch 1740/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2005 - acc: 0.8889 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01740: val_loss did not improve from 0.00202\n",
      "Epoch 1741/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2029 - acc: 0.8889 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01741: val_loss did not improve from 0.00202\n",
      "Epoch 1742/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1924 - acc: 0.8889 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01742: val_loss did not improve from 0.00202\n",
      "Epoch 1743/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1960 - acc: 0.8889 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01743: val_loss did not improve from 0.00202\n",
      "Epoch 1744/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1785 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01744: val_loss did not improve from 0.00202\n",
      "Epoch 1745/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1879 - acc: 0.8667 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01745: val_loss did not improve from 0.00202\n",
      "Epoch 1746/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1860 - acc: 0.8667 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01746: val_loss did not improve from 0.00202\n",
      "Epoch 1747/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1828 - acc: 0.9111 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01747: val_loss did not improve from 0.00202\n",
      "Epoch 1748/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1960 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01748: val_loss did not improve from 0.00202\n",
      "Epoch 1749/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2108 - acc: 0.8667 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01749: val_loss did not improve from 0.00202\n",
      "Epoch 1750/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1890 - acc: 0.9111 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01750: val_loss did not improve from 0.00202\n",
      "Epoch 1751/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1961 - acc: 0.8667 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01751: val_loss did not improve from 0.00202\n",
      "Epoch 1752/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1982 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01752: val_loss did not improve from 0.00202\n",
      "Epoch 1753/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1840 - acc: 0.9111 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01753: val_loss did not improve from 0.00202\n",
      "Epoch 1754/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1845 - acc: 0.9111 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01754: val_loss did not improve from 0.00202\n",
      "Epoch 1755/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1937 - acc: 0.8889 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01755: val_loss did not improve from 0.00202\n",
      "Epoch 1756/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.1822 - acc: 0.9111 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01756: val_loss did not improve from 0.00202\n",
      "Epoch 1757/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1844 - acc: 0.9111 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01757: val_loss did not improve from 0.00202\n",
      "Epoch 1758/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1947 - acc: 0.8889 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01758: val_loss did not improve from 0.00202\n",
      "Epoch 1759/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1887 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01759: val_loss did not improve from 0.00202\n",
      "Epoch 1760/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1889 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01760: val_loss did not improve from 0.00202\n",
      "Epoch 1761/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2004 - acc: 0.8667 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01761: val_loss did not improve from 0.00202\n",
      "Epoch 1762/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1962 - acc: 0.8889 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01762: val_loss did not improve from 0.00202\n",
      "Epoch 1763/2000\n",
      "45/45 [==============================] - 0s 554us/step - loss: 0.2129 - acc: 0.8667 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01763: val_loss did not improve from 0.00202\n",
      "Epoch 1764/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1861 - acc: 0.9111 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01764: val_loss did not improve from 0.00202\n",
      "Epoch 1765/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1990 - acc: 0.8667 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01765: val_loss did not improve from 0.00202\n",
      "Epoch 1766/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1901 - acc: 0.8889 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01766: val_loss did not improve from 0.00202\n",
      "Epoch 1767/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.2080 - acc: 0.8444 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01767: val_loss did not improve from 0.00202\n",
      "Epoch 1768/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2124 - acc: 0.8667 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01768: val_loss did not improve from 0.00202\n",
      "Epoch 1769/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.1757 - acc: 0.8667 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01769: val_loss did not improve from 0.00202\n",
      "Epoch 1770/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1920 - acc: 0.9111 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01770: val_loss did not improve from 0.00202\n",
      "Epoch 1771/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1949 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01771: val_loss did not improve from 0.00202\n",
      "Epoch 1772/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1816 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01772: val_loss did not improve from 0.00202\n",
      "Epoch 1773/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1765 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01773: val_loss did not improve from 0.00202\n",
      "Epoch 1774/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.1898 - acc: 0.8889 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01774: val_loss did not improve from 0.00202\n",
      "Epoch 1775/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1842 - acc: 0.8444 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01775: val_loss did not improve from 0.00202\n",
      "Epoch 1776/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1827 - acc: 0.8889 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01776: val_loss did not improve from 0.00202\n",
      "Epoch 1777/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1848 - acc: 0.9111 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01777: val_loss did not improve from 0.00202\n",
      "Epoch 1778/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1864 - acc: 0.8667 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01778: val_loss did not improve from 0.00202\n",
      "Epoch 1779/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1750 - acc: 0.8667 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01779: val_loss improved from 0.00202 to 0.00200, saving model to ./model/vloss0.002_vacc1.000_ep1779.hdf5\n",
      "Epoch 1780/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2098 - acc: 0.9111 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01780: val_loss improved from 0.00200 to 0.00163, saving model to ./model/vloss0.002_vacc1.000_ep1780.hdf5\n",
      "Epoch 1781/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1946 - acc: 0.9111 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01781: val_loss did not improve from 0.00163\n",
      "Epoch 1782/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2054 - acc: 0.9111 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01782: val_loss did not improve from 0.00163\n",
      "Epoch 1783/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2121 - acc: 0.8667 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01783: val_loss did not improve from 0.00163\n",
      "Epoch 1784/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.1857 - acc: 0.9111 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01784: val_loss did not improve from 0.00163\n",
      "Epoch 1785/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1954 - acc: 0.8889 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01785: val_loss did not improve from 0.00163\n",
      "Epoch 1786/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1902 - acc: 0.8889 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01786: val_loss did not improve from 0.00163\n",
      "Epoch 1787/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1937 - acc: 0.8667 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01787: val_loss did not improve from 0.00163\n",
      "Epoch 1788/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1889 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01788: val_loss did not improve from 0.00163\n",
      "Epoch 1789/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2140 - acc: 0.8667 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01789: val_loss did not improve from 0.00163\n",
      "Epoch 1790/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1956 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01790: val_loss did not improve from 0.00163\n",
      "Epoch 1791/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1994 - acc: 0.8667 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01791: val_loss did not improve from 0.00163\n",
      "Epoch 1792/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1984 - acc: 0.8667 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01792: val_loss did not improve from 0.00163\n",
      "Epoch 1793/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1842 - acc: 0.8667 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01793: val_loss did not improve from 0.00163\n",
      "Epoch 1794/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0086 - acc: 1.000 - 0s 310us/step - loss: 0.1837 - acc: 0.8667 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01794: val_loss did not improve from 0.00163\n",
      "Epoch 1795/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1921 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01795: val_loss did not improve from 0.00163\n",
      "Epoch 1796/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1848 - acc: 0.8889 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01796: val_loss did not improve from 0.00163\n",
      "Epoch 1797/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1831 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01797: val_loss did not improve from 0.00163\n",
      "Epoch 1798/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1819 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01798: val_loss did not improve from 0.00163\n",
      "Epoch 1799/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1741 - acc: 0.9111 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01799: val_loss did not improve from 0.00163\n",
      "Epoch 1800/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1904 - acc: 0.8444 - val_loss: 0.0032 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01800: val_loss did not improve from 0.00163\n",
      "Epoch 1801/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1799 - acc: 0.9111 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01801: val_loss did not improve from 0.00163\n",
      "Epoch 1802/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1821 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01802: val_loss did not improve from 0.00163\n",
      "Epoch 1803/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1933 - acc: 0.8889 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01803: val_loss did not improve from 0.00163\n",
      "Epoch 1804/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2029 - acc: 0.8889 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01804: val_loss did not improve from 0.00163\n",
      "Epoch 1805/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1758 - acc: 0.9111 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01805: val_loss did not improve from 0.00163\n",
      "Epoch 1806/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1828 - acc: 0.9111 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01806: val_loss did not improve from 0.00163\n",
      "Epoch 1807/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1865 - acc: 0.9111 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01807: val_loss did not improve from 0.00163\n",
      "Epoch 1808/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1915 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01808: val_loss did not improve from 0.00163\n",
      "Epoch 1809/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1930 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01809: val_loss did not improve from 0.00163\n",
      "Epoch 1810/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1983 - acc: 0.8667 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01810: val_loss did not improve from 0.00163\n",
      "Epoch 1811/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1846 - acc: 0.9111 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01811: val_loss did not improve from 0.00163\n",
      "Epoch 1812/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1819 - acc: 0.8667 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01812: val_loss did not improve from 0.00163\n",
      "Epoch 1813/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1884 - acc: 0.8889 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01813: val_loss did not improve from 0.00163\n",
      "Epoch 1814/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1911 - acc: 0.9111 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01814: val_loss did not improve from 0.00163\n",
      "Epoch 1815/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1867 - acc: 0.9111 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01815: val_loss did not improve from 0.00163\n",
      "Epoch 1816/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1958 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01816: val_loss did not improve from 0.00163\n",
      "Epoch 1817/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.1842 - acc: 0.9111 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01817: val_loss did not improve from 0.00163\n",
      "Epoch 1818/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1843 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01818: val_loss did not improve from 0.00163\n",
      "Epoch 1819/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2024 - acc: 0.8889 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01819: val_loss did not improve from 0.00163\n",
      "Epoch 1820/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2122 - acc: 0.8889 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01820: val_loss did not improve from 0.00163\n",
      "Epoch 1821/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.1748 - acc: 0.9333 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01821: val_loss did not improve from 0.00163\n",
      "Epoch 1822/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2116 - acc: 0.9111 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01822: val_loss did not improve from 0.00163\n",
      "Epoch 1823/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2000 - acc: 0.8667 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01823: val_loss did not improve from 0.00163\n",
      "Epoch 1824/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2057 - acc: 0.8222 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01824: val_loss did not improve from 0.00163\n",
      "Epoch 1825/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.5486 - acc: 0.8444 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01825: val_loss did not improve from 0.00163\n",
      "Epoch 1826/2000\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.7105 - acc: 0.8444 - val_loss: 0.0447 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01826: val_loss did not improve from 0.00163\n",
      "Epoch 1827/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.2956 - acc: 0.8889 - val_loss: 0.1172 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01827: val_loss did not improve from 0.00163\n",
      "Epoch 1828/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.3661 - acc: 0.8444 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01828: val_loss did not improve from 0.00163\n",
      "Epoch 1829/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.2272 - acc: 0.8889 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01829: val_loss did not improve from 0.00163\n",
      "Epoch 1830/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2160 - acc: 0.8889 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01830: val_loss did not improve from 0.00163\n",
      "Epoch 1831/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2036 - acc: 0.9111 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01831: val_loss did not improve from 0.00163\n",
      "Epoch 1832/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1925 - acc: 0.8889 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01832: val_loss did not improve from 0.00163\n",
      "Epoch 1833/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1982 - acc: 0.8444 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01833: val_loss did not improve from 0.00163\n",
      "Epoch 1834/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1906 - acc: 0.8889 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01834: val_loss did not improve from 0.00163\n",
      "Epoch 1835/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1837 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01835: val_loss did not improve from 0.00163\n",
      "Epoch 1836/2000\n",
      "45/45 [==============================] - 0s 333us/step - loss: 0.1873 - acc: 0.8889 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01836: val_loss did not improve from 0.00163\n",
      "Epoch 1837/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1863 - acc: 0.8889 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01837: val_loss did not improve from 0.00163\n",
      "Epoch 1838/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1833 - acc: 0.8889 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01838: val_loss did not improve from 0.00163\n",
      "Epoch 1839/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1799 - acc: 0.9111 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01839: val_loss did not improve from 0.00163\n",
      "Epoch 1840/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1932 - acc: 0.8667 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01840: val_loss did not improve from 0.00163\n",
      "Epoch 1841/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1791 - acc: 0.9111 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01841: val_loss did not improve from 0.00163\n",
      "Epoch 1842/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1881 - acc: 0.9111 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01842: val_loss did not improve from 0.00163\n",
      "Epoch 1843/2000\n",
      "45/45 [==============================] - 0s 532us/step - loss: 0.1922 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01843: val_loss did not improve from 0.00163\n",
      "Epoch 1844/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.1961 - acc: 0.8667 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01844: val_loss did not improve from 0.00163\n",
      "Epoch 1845/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1983 - acc: 0.8889 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01845: val_loss did not improve from 0.00163\n",
      "Epoch 1846/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1914 - acc: 0.8667 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01846: val_loss did not improve from 0.00163\n",
      "Epoch 1847/2000\n",
      "45/45 [==============================] - 0s 222us/step - loss: 0.1836 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01847: val_loss did not improve from 0.00163\n",
      "Epoch 1848/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1822 - acc: 0.9111 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01848: val_loss did not improve from 0.00163\n",
      "Epoch 1849/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1810 - acc: 0.9111 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01849: val_loss did not improve from 0.00163\n",
      "Epoch 1850/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1863 - acc: 0.8889 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01850: val_loss did not improve from 0.00163\n",
      "Epoch 1851/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1736 - acc: 0.9111 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01851: val_loss did not improve from 0.00163\n",
      "Epoch 1852/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1864 - acc: 0.8667 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01852: val_loss did not improve from 0.00163\n",
      "Epoch 1853/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1865 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01853: val_loss did not improve from 0.00163\n",
      "Epoch 1854/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1951 - acc: 0.8667 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01854: val_loss did not improve from 0.00163\n",
      "Epoch 1855/2000\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.1833 - acc: 0.9111 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01855: val_loss did not improve from 0.00163\n",
      "Epoch 1856/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1820 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01856: val_loss did not improve from 0.00163\n",
      "Epoch 1857/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1941 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01857: val_loss did not improve from 0.00163\n",
      "Epoch 1858/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1877 - acc: 0.8667 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01858: val_loss did not improve from 0.00163\n",
      "Epoch 1859/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1816 - acc: 0.9111 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01859: val_loss did not improve from 0.00163\n",
      "Epoch 1860/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1830 - acc: 0.9111 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01860: val_loss did not improve from 0.00163\n",
      "Epoch 1861/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1908 - acc: 0.9111 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01861: val_loss did not improve from 0.00163\n",
      "Epoch 1862/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2006 - acc: 0.8667 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01862: val_loss did not improve from 0.00163\n",
      "Epoch 1863/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1807 - acc: 0.9111 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01863: val_loss did not improve from 0.00163\n",
      "Epoch 1864/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1782 - acc: 0.8667 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01864: val_loss did not improve from 0.00163\n",
      "Epoch 1865/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1769 - acc: 0.9111 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01865: val_loss did not improve from 0.00163\n",
      "Epoch 1866/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1757 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01866: val_loss did not improve from 0.00163\n",
      "Epoch 1867/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1792 - acc: 0.8667 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01867: val_loss did not improve from 0.00163\n",
      "Epoch 1868/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1840 - acc: 0.8444 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01868: val_loss did not improve from 0.00163\n",
      "Epoch 1869/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1740 - acc: 0.8667 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01869: val_loss did not improve from 0.00163\n",
      "Epoch 1870/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1791 - acc: 0.9111 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01870: val_loss did not improve from 0.00163\n",
      "Epoch 1871/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1744 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01871: val_loss did not improve from 0.00163\n",
      "Epoch 1872/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1926 - acc: 0.8667 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01872: val_loss did not improve from 0.00163\n",
      "Epoch 1873/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.1923 - acc: 0.8889 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01873: val_loss did not improve from 0.00163\n",
      "Epoch 1874/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1992 - acc: 0.8889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01874: val_loss did not improve from 0.00163\n",
      "Epoch 1875/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1859 - acc: 0.8667 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01875: val_loss did not improve from 0.00163\n",
      "Epoch 1876/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1866 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01876: val_loss did not improve from 0.00163\n",
      "Epoch 1877/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1835 - acc: 0.8667 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01877: val_loss did not improve from 0.00163\n",
      "Epoch 1878/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1803 - acc: 0.9111 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01878: val_loss did not improve from 0.00163\n",
      "Epoch 1879/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1889 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01879: val_loss did not improve from 0.00163\n",
      "Epoch 1880/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1942 - acc: 0.8889 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01880: val_loss did not improve from 0.00163\n",
      "Epoch 1881/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1762 - acc: 0.9111 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01881: val_loss did not improve from 0.00163\n",
      "Epoch 1882/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1833 - acc: 0.9333 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01882: val_loss did not improve from 0.00163\n",
      "Epoch 1883/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1919 - acc: 0.9111 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01883: val_loss did not improve from 0.00163\n",
      "Epoch 1884/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1937 - acc: 0.8889 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01884: val_loss did not improve from 0.00163\n",
      "Epoch 1885/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1771 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01885: val_loss did not improve from 0.00163\n",
      "Epoch 1886/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1711 - acc: 0.9111 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01886: val_loss did not improve from 0.00163\n",
      "Epoch 1887/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1892 - acc: 0.8889 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01887: val_loss did not improve from 0.00163\n",
      "Epoch 1888/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1868 - acc: 0.8889 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01888: val_loss did not improve from 0.00163\n",
      "Epoch 1889/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1909 - acc: 0.8889 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01889: val_loss did not improve from 0.00163\n",
      "Epoch 1890/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2089 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01890: val_loss did not improve from 0.00163\n",
      "Epoch 1891/2000\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.1868 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01891: val_loss did not improve from 0.00163\n",
      "Epoch 1892/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1743 - acc: 0.9111 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01892: val_loss did not improve from 0.00163\n",
      "Epoch 1893/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1882 - acc: 0.8667 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01893: val_loss did not improve from 0.00163\n",
      "Epoch 1894/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1887 - acc: 0.8667 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01894: val_loss did not improve from 0.00163\n",
      "Epoch 1895/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1867 - acc: 0.8889 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01895: val_loss did not improve from 0.00163\n",
      "Epoch 1896/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1786 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01896: val_loss did not improve from 0.00163\n",
      "Epoch 1897/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.2006 - acc: 0.8667 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01897: val_loss did not improve from 0.00163\n",
      "Epoch 1898/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1916 - acc: 0.8667 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01898: val_loss did not improve from 0.00163\n",
      "Epoch 1899/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1850 - acc: 0.8889 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01899: val_loss did not improve from 0.00163\n",
      "Epoch 1900/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1808 - acc: 0.8667 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01900: val_loss did not improve from 0.00163\n",
      "Epoch 1901/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1835 - acc: 0.8889 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01901: val_loss did not improve from 0.00163\n",
      "Epoch 1902/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.1658 - acc: 1.000 - 0s 266us/step - loss: 0.1801 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01902: val_loss did not improve from 0.00163\n",
      "Epoch 1903/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1759 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01903: val_loss did not improve from 0.00163\n",
      "Epoch 1904/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1843 - acc: 0.8667 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01904: val_loss did not improve from 0.00163\n",
      "Epoch 1905/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1824 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01905: val_loss did not improve from 0.00163\n",
      "Epoch 1906/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2031 - acc: 0.8889 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01906: val_loss did not improve from 0.00163\n",
      "Epoch 1907/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1677 - acc: 0.9111 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01907: val_loss did not improve from 0.00163\n",
      "Epoch 1908/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1765 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01908: val_loss did not improve from 0.00163\n",
      "Epoch 1909/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1746 - acc: 0.9111 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01909: val_loss did not improve from 0.00163\n",
      "Epoch 1910/2000\n",
      "45/45 [==============================] - 0s 244us/step - loss: 0.1900 - acc: 0.8889 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01910: val_loss did not improve from 0.00163\n",
      "Epoch 1911/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1794 - acc: 0.9111 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01911: val_loss did not improve from 0.00163\n",
      "Epoch 1912/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1842 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01912: val_loss did not improve from 0.00163\n",
      "Epoch 1913/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1865 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01913: val_loss did not improve from 0.00163\n",
      "Epoch 1914/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1815 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01914: val_loss did not improve from 0.00163\n",
      "Epoch 1915/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1863 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01915: val_loss did not improve from 0.00163\n",
      "Epoch 1916/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1784 - acc: 0.9111 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01916: val_loss did not improve from 0.00163\n",
      "Epoch 1917/2000\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.2000 - acc: 0.8444 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01917: val_loss did not improve from 0.00163\n",
      "Epoch 1918/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1912 - acc: 0.8667 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01918: val_loss did not improve from 0.00163\n",
      "Epoch 1919/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1924 - acc: 0.9111 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01919: val_loss did not improve from 0.00163\n",
      "Epoch 1920/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2006 - acc: 0.8667 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01920: val_loss did not improve from 0.00163\n",
      "Epoch 1921/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1940 - acc: 0.8889 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01921: val_loss did not improve from 0.00163\n",
      "Epoch 1922/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1918 - acc: 0.9111 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01922: val_loss did not improve from 0.00163\n",
      "Epoch 1923/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1719 - acc: 0.8667 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01923: val_loss did not improve from 0.00163\n",
      "Epoch 1924/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1913 - acc: 0.8889 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01924: val_loss did not improve from 0.00163\n",
      "Epoch 1925/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1790 - acc: 0.9111 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01925: val_loss did not improve from 0.00163\n",
      "Epoch 1926/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1762 - acc: 0.9111 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01926: val_loss did not improve from 0.00163\n",
      "Epoch 1927/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1882 - acc: 0.8889 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01927: val_loss did not improve from 0.00163\n",
      "Epoch 1928/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1727 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01928: val_loss did not improve from 0.00163\n",
      "Epoch 1929/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1853 - acc: 0.8889 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01929: val_loss did not improve from 0.00163\n",
      "Epoch 1930/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 310us/step - loss: 0.1934 - acc: 0.8889 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01930: val_loss did not improve from 0.00163\n",
      "Epoch 1931/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.2096 - acc: 0.8667 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01931: val_loss did not improve from 0.00163\n",
      "Epoch 1932/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1851 - acc: 0.9111 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01932: val_loss did not improve from 0.00163\n",
      "Epoch 1933/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1754 - acc: 0.9111 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01933: val_loss did not improve from 0.00163\n",
      "Epoch 1934/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1770 - acc: 0.8889 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01934: val_loss did not improve from 0.00163\n",
      "Epoch 1935/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1799 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01935: val_loss did not improve from 0.00163\n",
      "Epoch 1936/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1836 - acc: 0.8889 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01936: val_loss did not improve from 0.00163\n",
      "Epoch 1937/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1953 - acc: 0.8444 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01937: val_loss did not improve from 0.00163\n",
      "Epoch 1938/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0858 - acc: 1.000 - 0s 310us/step - loss: 0.1822 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01938: val_loss did not improve from 0.00163\n",
      "Epoch 1939/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1731 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01939: val_loss did not improve from 0.00163\n",
      "Epoch 1940/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1819 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01940: val_loss did not improve from 0.00163\n",
      "Epoch 1941/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1752 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01941: val_loss did not improve from 0.00163\n",
      "Epoch 1942/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1930 - acc: 0.8889 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01942: val_loss did not improve from 0.00163\n",
      "Epoch 1943/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1817 - acc: 0.9111 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01943: val_loss did not improve from 0.00163\n",
      "Epoch 1944/2000\n",
      "45/45 [==============================] - 0s 377us/step - loss: 0.1901 - acc: 0.8889 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01944: val_loss did not improve from 0.00163\n",
      "Epoch 1945/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1820 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01945: val_loss did not improve from 0.00163\n",
      "Epoch 1946/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1863 - acc: 0.8889 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01946: val_loss did not improve from 0.00163\n",
      "Epoch 1947/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1840 - acc: 0.9111 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01947: val_loss did not improve from 0.00163\n",
      "Epoch 1948/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1936 - acc: 0.9111 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01948: val_loss did not improve from 0.00163\n",
      "Epoch 1949/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1822 - acc: 0.8889 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01949: val_loss did not improve from 0.00163\n",
      "Epoch 1950/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1782 - acc: 0.8889 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01950: val_loss did not improve from 0.00163\n",
      "Epoch 1951/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1871 - acc: 0.9111 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01951: val_loss did not improve from 0.00163\n",
      "Epoch 1952/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1955 - acc: 0.8889 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01952: val_loss did not improve from 0.00163\n",
      "Epoch 1953/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1806 - acc: 0.9111 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01953: val_loss did not improve from 0.00163\n",
      "Epoch 1954/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1865 - acc: 0.8667 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01954: val_loss did not improve from 0.00163\n",
      "Epoch 1955/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1749 - acc: 0.9111 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01955: val_loss did not improve from 0.00163\n",
      "Epoch 1956/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1851 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01956: val_loss did not improve from 0.00163\n",
      "Epoch 1957/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1801 - acc: 0.9111 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01957: val_loss did not improve from 0.00163\n",
      "Epoch 1958/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1787 - acc: 0.9111 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01958: val_loss did not improve from 0.00163\n",
      "Epoch 1959/2000\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0841 - acc: 1.000 - 0s 310us/step - loss: 0.1857 - acc: 0.9111 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01959: val_loss did not improve from 0.00163\n",
      "Epoch 1960/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1936 - acc: 0.8667 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01960: val_loss did not improve from 0.00163\n",
      "Epoch 1961/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1820 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01961: val_loss did not improve from 0.00163\n",
      "Epoch 1962/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1797 - acc: 0.8889 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01962: val_loss did not improve from 0.00163\n",
      "Epoch 1963/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1903 - acc: 0.8889 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01963: val_loss did not improve from 0.00163\n",
      "Epoch 1964/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1926 - acc: 0.8667 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01964: val_loss did not improve from 0.00163\n",
      "Epoch 1965/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1964 - acc: 0.8889 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01965: val_loss did not improve from 0.00163\n",
      "Epoch 1966/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1673 - acc: 0.9111 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01966: val_loss did not improve from 0.00163\n",
      "Epoch 1967/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1878 - acc: 0.8889 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01967: val_loss did not improve from 0.00163\n",
      "Epoch 1968/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1933 - acc: 0.9111 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01968: val_loss did not improve from 0.00163\n",
      "Epoch 1969/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1792 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01969: val_loss did not improve from 0.00163\n",
      "Epoch 1970/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1827 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01970: val_loss did not improve from 0.00163\n",
      "Epoch 1971/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1915 - acc: 0.9111 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01971: val_loss did not improve from 0.00163\n",
      "Epoch 1972/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1766 - acc: 0.8889 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01972: val_loss did not improve from 0.00163\n",
      "Epoch 1973/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 266us/step - loss: 0.2152 - acc: 0.8444 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01973: val_loss did not improve from 0.00163\n",
      "Epoch 1974/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1877 - acc: 0.9111 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01974: val_loss did not improve from 0.00163\n",
      "Epoch 1975/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1969 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01975: val_loss did not improve from 0.00163\n",
      "Epoch 1976/2000\n",
      "45/45 [==============================] - 0s 355us/step - loss: 0.1754 - acc: 0.8889 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01976: val_loss did not improve from 0.00163\n",
      "Epoch 1977/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1884 - acc: 0.8889 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01977: val_loss did not improve from 0.00163\n",
      "Epoch 1978/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1811 - acc: 0.9111 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01978: val_loss did not improve from 0.00163\n",
      "Epoch 1979/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1911 - acc: 0.8889 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01979: val_loss did not improve from 0.00163\n",
      "Epoch 1980/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1711 - acc: 0.9111 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01980: val_loss did not improve from 0.00163\n",
      "Epoch 1981/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1898 - acc: 0.8222 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01981: val_loss did not improve from 0.00163\n",
      "Epoch 1982/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1858 - acc: 0.8889 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01982: val_loss did not improve from 0.00163\n",
      "Epoch 1983/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1849 - acc: 0.8889 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01983: val_loss did not improve from 0.00163\n",
      "Epoch 1984/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1896 - acc: 0.9111 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01984: val_loss did not improve from 0.00163\n",
      "Epoch 1985/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1799 - acc: 0.8889 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01985: val_loss did not improve from 0.00163\n",
      "Epoch 1986/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1948 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01986: val_loss did not improve from 0.00163\n",
      "Epoch 1987/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1771 - acc: 0.9111 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01987: val_loss did not improve from 0.00163\n",
      "Epoch 1988/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1714 - acc: 0.8889 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01988: val_loss did not improve from 0.00163\n",
      "Epoch 1989/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1785 - acc: 0.8444 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01989: val_loss did not improve from 0.00163\n",
      "Epoch 1990/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1853 - acc: 0.8889 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01990: val_loss did not improve from 0.00163\n",
      "Epoch 1991/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1861 - acc: 0.8889 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01991: val_loss did not improve from 0.00163\n",
      "Epoch 1992/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1802 - acc: 0.8889 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01992: val_loss did not improve from 0.00163\n",
      "Epoch 1993/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1831 - acc: 0.8889 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01993: val_loss did not improve from 0.00163\n",
      "Epoch 1994/2000\n",
      "45/45 [==============================] - 0s 332us/step - loss: 0.1802 - acc: 0.8889 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01994: val_loss did not improve from 0.00163\n",
      "Epoch 1995/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1756 - acc: 0.8889 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01995: val_loss did not improve from 0.00163\n",
      "Epoch 1996/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1800 - acc: 0.8889 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01996: val_loss did not improve from 0.00163\n",
      "Epoch 1997/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1824 - acc: 0.8889 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01997: val_loss did not improve from 0.00163\n",
      "Epoch 1998/2000\n",
      "45/45 [==============================] - 0s 266us/step - loss: 0.1773 - acc: 0.8889 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01998: val_loss did not improve from 0.00163\n",
      "Epoch 1999/2000\n",
      "45/45 [==============================] - 0s 288us/step - loss: 0.1777 - acc: 0.9111 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "\n",
      "Epoch 01999: val_loss did not improve from 0.00163\n",
      "Epoch 2000/2000\n",
      "45/45 [==============================] - 0s 310us/step - loss: 0.1899 - acc: 0.8667 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 02000: val_loss did not improve from 0.00163\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모델 최적화 훈련진행\n",
    "# - 준비된 훈련계층과 훈련방법에 따라 미니배치(mini-batch) 방식으로 최적화 훈련진행\n",
    "# - 훈련데이터셋을 한번에 투입해 또는 한개 샘플씩 학습을 하지 않고, \n",
    "#   일부(batch_size) 단위샘플로 나누어서 부분적으로 학습을 진행함\n",
    "# - 미니배치를 하나씩 학습모델에 투입해 손실함수값을 줄이기 위한 파라미터(가중치, 편향)를 조율해 나감\n",
    "# - 각 미니배치별 손실함수값을 구하고, 이들의 평균적인 오차와 정확도를 해당 모델의 성능으로 판단함\n",
    "# - 또한 미니배치 단위샘플을 모두 사용하는 일련의 학습과정(에포크: epoch)을 \n",
    "#   한번에 끝내는 것이 아니라 여러차례 반복실시해 모델의 성능을 개선해 나감  \n",
    "\n",
    "history = md.fit(X_train_scaled, y_train_ohe, \n",
    "                 epochs = 2000, batch_size = 5, \n",
    "                 validation_split = 0.1, \n",
    "                 verbose = 1, \n",
    "                 callbacks = [cp, es])\n",
    "# - 훈련셋을 5개 샘플씩 나누어 미니배치를 여러개 만들고 \n",
    "#   이를 모두 활용하는 딥러닝 학습을 총 2000번 반복실시함\n",
    "\n",
    "# - batch_size: 훈련데이터셋중에서 몇개 샘플을 미니배치(mini-batch)로 만들것인지 설정\n",
    "#   적게설정: 빠른 훈련속도이점. 단위샘플셋에 따라 학습편차가 많이 발생. 병렬연산 못하고 하드웨어 자원낭비\n",
    "#   많이설정: 훈련속도가 느려짐. 지역최소값(local minimum)에 빠질 수 있음. 하드웨어 자원에 부하가 많이 발생함)\n",
    "# - epochs: 미니배치를 모두 처리하는 일련의 학습을 몇 번 정도 실시할지 설정\n",
    "#   적게설정: 파라미터조율이 충분하지 않아서 과소적합 가능성\n",
    "#   많이설정: 파라미터조율이 지나쳐 과대적합 가능성\n",
    "\n",
    "# - validation_split = 0.2: 전체데이터중 훈련셋(training) 80%, 검증셋(validation) 20%로 분할하는 비율설정\n",
    "# - callbacks \n",
    "#   [cp] 학습모델별 성능평가파일 생성방법 설정\n",
    "#   [es] 딥러닝 과적합방지용 조기종료 옵션 설정\n",
    "\n",
    "# 신경망 학습을 위한 데이터셋 3가지 유형\n",
    "# - 훈련 데이터(training data): 매개변수(가중치와 편향) 학습\n",
    "# - 검증 데이터(validation data): 하이퍼파라미터 성능 평가\n",
    "# - 시험 데이터(test data): 신경망의 범용 성능 평가\n",
    "\n",
    "# - validation_split = 0.1 대신에 별도 피처&타겟셋을 입력할 수 있음\n",
    "# - validation_data = (X_testing_scaled, y_test_ohe)로 설정하면\n",
    "#   시험셋을 검증용으로 투입한 것으로 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>딥러닝 모델 성능평가</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 학습모델 성능평가 히스토리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 학습실시에 따른 성능평가 히스토리 확보\n",
    "\n",
    "# 훈련셋(training) 학습성능\n",
    "y_loss = history.history['loss'] # 훈련셋 오차\n",
    "y_acc = history.history['acc'] # 훈련셋 정확도\n",
    "\n",
    "# 검증셋(validation) 학습성능\n",
    "y_vloss = history.history['val_loss'] # 검증셋 오차\n",
    "y_vacc = history.history['val_acc'] # 검증셋 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2000\n",
      "<class 'list'> 2000\n",
      "<class 'list'> 2000\n",
      "<class 'list'> 2000\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 학습실시에 따른 성능평가 히스토리 객체형식과 규모\n",
    "print(type(y_loss), len(y_loss))\n",
    "print(type(y_acc), len(y_acc))\n",
    "print(type(y_vloss), len(y_vloss))\n",
    "print(type(y_vacc), len(y_vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋   오차추이: [2.419, 2.286, 2.193, 2.092, 2.013, 1.953, 1.918, 1.892, 1.859, 1.84, 1.816, 1.805, 1.785, 1.769, 1.746, 1.737, 1.716, 1.713, 1.693, 1.684, 1.672, 1.66, 1.649, 1.644, 1.626, 1.619, 1.612, 1.601, 1.591, 1.591, 1.576, 1.571, 1.571, 1.559, 1.563, 1.546, 1.541, 1.539, 1.536, 1.523, 1.522, 1.517, 1.512, 1.501, 1.494, 1.503, 1.488, 1.489, 1.487, 1.479, 1.475, 1.472, 1.47, 1.465, 1.459, 1.453, 1.455, 1.438, 1.447, 1.44, 1.433, 1.431, 1.429, 1.417, 1.42, 1.419, 1.413, 1.409, 1.404, 1.404, 1.395, 1.402, 1.41, 1.392, 1.394, 1.384, 1.377, 1.371, 1.375, 1.368, 1.365, 1.36, 1.364, 1.352, 1.353, 1.351, 1.346, 1.34, 1.35, 1.34, 1.333, 1.33, 1.333, 1.341, 1.32, 1.327, 1.328, 1.323, 1.311, 1.311, 1.305, 1.298, 1.303, 1.295, 1.295, 1.295, 1.284, 1.288, 1.278, 1.272, 1.272, 1.272, 1.27, 1.27, 1.282, 1.256, 1.262, 1.255, 1.249, 1.258, 1.267, 1.248, 1.233, 1.234, 1.229, 1.235, 1.226, 1.234, 1.22, 1.213, 1.227, 1.203, 1.22, 1.221, 1.206, 1.208, 1.197, 1.2, 1.197, 1.199, 1.196, 1.223, 1.182, 1.181, 1.193, 1.179, 1.18, 1.171, 1.163, 1.17, 1.167, 1.173, 1.168, 1.153, 1.163, 1.159, 1.142, 1.137, 1.136, 1.139, 1.143, 1.132, 1.151, 1.125, 1.13, 1.115, 1.123, 1.12, 1.116, 1.111, 1.115, 1.113, 1.109, 1.096, 1.098, 1.087, 1.088, 1.099, 1.084, 1.085, 1.072, 1.085, 1.084, 1.069, 1.063, 1.065, 1.066, 1.059, 1.062, 1.053, 1.059, 1.054, 1.054, 1.047, 1.061, 1.035, 1.038, 1.037, 1.033, 1.018, 1.029, 1.024, 1.011, 1.024, 1.021, 1.016, 1.008, 1.013, 1.009, 1.003, 0.999, 1.009, 1.026, 0.986, 0.995, 0.994, 0.984, 0.977, 0.975, 0.976, 0.973, 0.983, 0.973, 0.958, 0.971, 0.962, 0.976, 0.956, 0.97, 0.969, 0.951, 0.94, 0.949, 0.942, 0.942, 0.96, 0.936, 0.934, 0.935, 0.916, 0.922, 0.934, 0.914, 0.915, 0.913, 0.931, 0.909, 0.903, 0.91, 0.905, 0.909, 0.9, 0.896, 0.914, 0.902, 0.89, 0.886, 0.906, 0.885, 0.883, 0.873, 0.892, 0.865, 0.877, 0.868, 0.871, 0.884, 0.856, 0.858, 0.872, 0.865, 0.856, 0.848, 0.866, 0.853, 0.844, 0.838, 0.839, 0.835, 0.833, 0.859, 0.835, 0.839, 0.835, 0.826, 0.837, 0.827, 0.828, 0.814, 0.819, 0.818, 0.818, 0.823, 0.809, 0.844, 0.814, 0.801, 0.803, 0.796, 0.801, 0.792, 0.797, 0.799, 0.789, 0.792, 0.779, 0.781, 0.79, 0.787, 0.774, 0.791, 0.794, 0.768, 0.787, 0.767, 0.772, 0.764, 0.789, 0.76, 0.77, 0.768, 0.757, 0.764, 0.746, 0.767, 0.754, 0.745, 0.779, 0.759, 0.761, 0.744, 0.761, 0.761, 0.74, 0.745, 0.73, 0.736, 0.73, 0.727, 0.749, 0.734, 0.734, 0.737, 0.728, 0.713, 0.739, 0.719, 0.72, 0.729, 0.716, 0.715, 0.736, 0.722, 0.721, 0.709, 0.707, 0.71, 0.696, 0.713, 0.708, 0.705, 0.701, 0.724, 0.702, 0.709, 0.686, 0.691, 0.692, 0.691, 0.694, 0.692, 0.691, 0.683, 0.691, 0.692, 0.686, 0.677, 0.699, 0.674, 0.678, 0.698, 0.659, 0.677, 0.661, 0.669, 0.656, 0.665, 0.66, 0.647, 0.656, 0.652, 0.646, 0.653, 0.654, 0.661, 0.654, 0.643, 0.658, 0.644, 0.646, 0.65, 0.65, 0.646, 0.639, 0.64, 0.65, 0.634, 0.646, 0.637, 0.645, 0.631, 0.635, 0.626, 0.639, 0.646, 0.621, 0.633, 0.639, 0.643, 0.628, 0.62, 0.637, 0.632, 0.616, 0.616, 0.618, 0.607, 0.608, 0.624, 0.623, 0.606, 0.623, 0.606, 0.6, 0.598, 0.62, 0.616, 0.604, 0.589, 0.615, 0.587, 0.605, 0.601, 0.593, 0.608, 0.584, 0.591, 0.584, 0.589, 0.584, 0.581, 0.589, 0.576, 0.581, 0.594, 0.6, 0.572, 0.593, 0.575, 0.578, 0.568, 0.581, 0.626, 0.59, 0.578, 0.586, 0.568, 0.571, 0.572, 0.561, 0.558, 0.566, 0.559, 0.556, 0.56, 0.578, 0.562, 0.558, 0.558, 0.56, 0.55, 0.553, 0.553, 0.547, 0.563, 0.565, 0.559, 0.547, 0.543, 0.559, 0.541, 0.54, 0.542, 0.54, 0.55, 0.534, 0.556, 0.559, 0.549, 0.532, 0.538, 0.537, 0.532, 0.536, 0.526, 0.544, 0.549, 0.523, 0.541, 0.533, 0.522, 0.525, 0.527, 0.516, 0.518, 0.522, 0.517, 0.515, 0.521, 0.525, 0.523, 0.523, 0.508, 0.515, 0.522, 0.533, 0.534, 0.515, 0.514, 0.499, 0.518, 0.516, 0.504, 0.504, 0.522, 0.49, 0.503, 0.504, 0.513, 0.517, 0.502, 0.493, 0.494, 0.49, 0.496, 0.524, 0.495, 0.507, 0.497, 0.517, 0.496, 0.486, 0.485, 0.488, 0.492, 0.487, 0.494, 0.496, 0.483, 0.484, 0.493, 0.496, 0.483, 0.479, 0.497, 0.506, 0.473, 0.488, 0.479, 0.478, 0.476, 0.474, 0.466, 0.477, 0.468, 0.473, 0.479, 0.468, 0.459, 0.46, 0.489, 0.46, 0.469, 0.481, 0.467, 0.463, 0.465, 0.46, 0.455, 0.455, 0.463, 0.457, 0.465, 0.479, 0.473, 0.45, 0.455, 0.453, 0.47, 0.443, 0.463, 0.451, 0.462, 0.461, 0.449, 0.444, 0.447, 0.449, 0.433, 0.454, 0.44, 0.441, 0.457, 0.436, 0.447, 0.455, 0.453, 0.436, 0.442, 0.438, 0.438, 0.456, 0.45, 0.436, 0.46, 0.437, 0.429, 0.437, 0.433, 0.431, 0.434, 0.428, 0.448, 0.425, 0.424, 0.439, 0.439, 0.417, 0.435, 0.427, 0.422, 0.425, 0.425, 0.432, 0.425, 0.424, 0.424, 0.427, 0.426, 0.412, 0.431, 0.443, 0.406, 0.448, 0.459, 0.403, 0.43, 0.43, 0.433, 0.407, 0.413, 0.405, 0.4, 0.408, 0.412, 0.401, 0.408, 0.415, 0.412, 0.407, 0.398, 0.419, 0.393, 0.407, 0.4, 0.431, 0.388, 0.438, 0.41, 0.408, 0.401, 0.397, 0.403, 0.425, 0.402, 0.431, 0.388, 0.402, 0.4, 0.385, 0.389, 0.39, 0.388, 0.388, 0.392, 0.387, 0.378, 0.398, 0.387, 0.399, 0.417, 0.428, 0.392, 0.399, 0.38, 0.382, 0.381, 0.379, 0.392, 0.383, 0.383, 0.375, 0.385, 0.411, 0.39, 0.384, 0.391, 0.371, 0.388, 0.393, 0.377, 0.373, 0.372, 0.367, 0.371, 0.379, 0.384, 0.378, 0.366, 0.38, 0.369, 0.363, 0.371, 0.371, 0.367, 0.384, 0.367, 0.374, 0.37, 0.413, 0.379, 0.362, 0.362, 0.369, 0.359, 0.371, 0.361, 0.364, 0.369, 0.371, 0.357, 0.36, 0.363, 0.366, 0.363, 0.366, 0.348, 0.381, 0.367, 0.375, 0.367, 0.354, 0.354, 0.35, 0.367, 0.349, 0.358, 0.36, 0.347, 0.354, 0.364, 0.357, 0.354, 0.347, 0.349, 0.374, 0.356, 0.362, 0.36, 0.354, 0.373, 0.364, 0.358, 0.354, 0.343, 0.343, 0.343, 0.334, 0.343, 0.334, 0.339, 0.347, 0.334, 0.367, 0.353, 0.342, 0.337, 0.349, 0.34, 0.337, 0.334, 0.345, 0.345, 0.336, 0.329, 0.337, 0.338, 0.33, 0.341, 0.355, 0.334, 0.363, 0.333, 0.328, 0.336, 0.334, 0.341, 0.345, 0.328, 0.335, 0.357, 0.33, 0.336, 0.348, 0.327, 0.319, 0.321, 0.325, 0.337, 0.334, 0.347, 0.323, 0.327, 0.324, 0.325, 0.325, 0.347, 0.313, 0.346, 0.331, 0.325, 0.328, 0.316, 0.336, 0.325, 0.319, 0.328, 0.311, 0.319, 0.313, 0.335, 0.309, 0.326, 0.326, 0.314, 0.322, 0.319, 0.315, 0.322, 0.318, 0.332, 0.315, 0.323, 0.308, 0.335, 0.306, 0.324, 0.319, 0.334, 0.329, 0.317, 0.339, 0.305, 0.304, 0.306, 0.314, 0.318, 0.317, 0.306, 0.328, 0.314, 0.314, 0.32, 0.308, 0.303, 0.316, 0.324, 0.321, 0.325, 0.297, 0.32, 0.297, 0.315, 0.31, 0.301, 0.299, 0.301, 0.299, 0.306, 0.331, 0.308, 0.294, 0.324, 0.313, 0.326, 0.287, 0.301, 0.328, 0.29, 0.311, 0.309, 0.291, 0.298, 0.317, 0.3, 0.295, 0.296, 0.296, 0.302, 0.296, 0.298, 0.306, 0.31, 0.306, 0.3, 0.294, 0.308, 0.313, 0.299, 0.311, 0.301, 0.296, 0.289, 0.294, 0.286, 0.285, 0.288, 0.282, 0.283, 0.292, 0.297, 0.301, 0.292, 0.293, 0.288, 0.287, 0.281, 0.295, 0.286, 0.296, 0.282, 0.301, 0.289, 0.29, 0.296, 0.286, 0.288, 0.284, 0.313, 0.313, 0.282, 0.286, 0.294, 0.273, 0.303, 0.29, 0.289, 0.284, 0.307, 0.298, 0.283, 0.279, 0.286, 0.286, 0.277, 0.284, 0.29, 0.289, 0.275, 0.277, 0.285, 0.283, 0.28, 0.278, 0.277, 0.281, 0.287, 0.278, 0.288, 0.3, 0.27, 0.29, 0.279, 0.272, 0.295, 0.28, 0.286, 0.276, 0.279, 0.271, 0.269, 0.277, 0.258, 0.288, 0.271, 0.29, 0.267, 0.28, 0.265, 0.273, 0.282, 0.271, 0.275, 0.263, 0.271, 0.265, 0.282, 0.279, 0.274, 0.3, 0.266, 0.272, 0.278, 0.274, 0.278, 0.259, 0.265, 0.263, 0.264, 0.27, 0.268, 0.286, 0.262, 0.275, 0.264, 0.264, 0.27, 0.258, 0.279, 0.26, 0.264, 0.269, 0.284, 0.258, 0.267, 0.273, 0.285, 0.276, 0.264, 0.275, 0.287, 0.257, 0.259, 0.259, 0.275, 0.286, 0.266, 0.257, 0.268, 0.26, 0.265, 0.266, 0.259, 0.257, 0.256, 0.247, 0.252, 0.266, 0.268, 0.252, 0.269, 0.251, 0.264, 0.287, 0.266, 0.263, 0.28, 0.254, 0.255, 0.27, 0.252, 0.28, 0.276, 0.256, 0.255, 0.255, 0.248, 0.27, 0.25, 0.266, 0.259, 0.244, 0.252, 0.253, 0.261, 0.239, 0.255, 0.27, 0.248, 0.255, 0.253, 0.25, 0.252, 0.254, 0.247, 0.259, 0.255, 0.258, 0.247, 0.244, 0.259, 0.258, 0.242, 0.244, 0.255, 0.249, 0.256, 0.242, 0.247, 0.249, 0.239, 0.24, 0.251, 0.239, 0.254, 0.248, 0.251, 0.261, 0.251, 0.245, 0.251, 0.248, 0.245, 0.247, 0.245, 0.26, 0.239, 0.242, 0.25, 0.247, 0.248, 0.239, 0.241, 0.238, 0.242, 0.249, 0.281, 0.243, 0.246, 0.261, 0.273, 0.259, 0.238, 0.256, 0.259, 0.247, 0.247, 0.251, 0.236, 0.238, 0.237, 0.247, 0.251, 0.242, 0.235, 0.281, 0.235, 0.242, 0.24, 0.235, 0.245, 0.242, 0.262, 0.24, 0.244, 0.235, 0.25, 0.235, 0.235, 0.246, 0.25, 0.23, 0.253, 0.238, 0.238, 0.23, 0.235, 0.227, 0.248, 0.233, 0.237, 0.253, 0.237, 0.248, 0.236, 0.253, 0.237, 0.256, 0.23, 0.252, 0.241, 0.236, 0.246, 0.235, 0.237, 0.257, 0.268, 0.217, 0.244, 0.236, 0.263, 0.229, 0.23, 0.23, 0.237, 0.274, 0.238, 0.233, 0.22, 0.243, 0.217, 0.235, 0.224, 0.225, 0.243, 0.218, 0.233, 0.229, 0.233, 0.239, 0.228, 0.238, 0.223, 0.241, 0.242, 0.23, 0.231, 0.233, 0.215, 0.228, 0.24, 0.238, 0.238, 0.221, 0.228, 0.243, 0.263, 0.22, 0.231, 0.239, 0.225, 0.228, 0.235, 0.227, 0.227, 0.221, 0.22, 0.232, 0.231, 0.244, 0.229, 0.229, 0.254, 0.254, 0.232, 0.241, 0.231, 0.254, 0.217, 0.235, 0.24, 0.222, 0.216, 0.231, 0.227, 0.231, 0.243, 0.232, 0.22, 0.225, 0.225, 0.223, 0.226, 0.221, 0.238, 0.232, 0.243, 0.248, 0.219, 0.221, 0.248, 0.216, 0.225, 0.222, 0.242, 0.215, 0.224, 0.217, 0.224, 0.22, 0.223, 0.226, 0.224, 0.238, 0.22, 0.216, 0.226, 0.212, 0.223, 0.222, 0.221, 0.23, 0.214, 0.218, 0.219, 0.237, 0.214, 0.221, 0.227, 0.234, 0.229, 0.227, 0.22, 0.218, 0.223, 0.212, 0.216, 0.215, 0.209, 0.218, 0.212, 0.211, 0.23, 0.24, 0.234, 0.219, 0.22, 0.223, 0.212, 0.236, 0.233, 0.219, 0.231, 0.225, 0.237, 0.235, 0.217, 0.222, 0.226, 0.221, 0.232, 0.212, 0.21, 0.225, 0.232, 0.221, 0.207, 0.216, 0.222, 0.212, 0.225, 0.225, 0.223, 0.219, 0.216, 0.217, 0.229, 0.212, 0.245, 0.21, 0.221, 0.223, 0.228, 0.221, 0.218, 0.22, 0.215, 0.226, 0.226, 0.21, 0.209, 0.217, 0.211, 0.227, 0.218, 0.227, 0.207, 0.224, 0.206, 0.224, 0.201, 0.224, 0.229, 0.218, 0.211, 0.204, 0.203, 0.231, 0.227, 0.209, 0.21, 0.21, 0.229, 0.205, 0.218, 0.21, 0.225, 0.218, 0.215, 0.231, 0.227, 0.215, 0.225, 0.217, 0.23, 0.196, 0.212, 0.213, 0.242, 0.203, 0.216, 0.204, 0.218, 0.209, 0.211, 0.205, 0.221, 0.196, 0.21, 0.208, 0.228, 0.217, 0.213, 0.251, 0.203, 0.205, 0.204, 0.206, 0.219, 0.227, 0.21, 0.216, 0.194, 0.201, 0.241, 0.203, 0.22, 0.208, 0.231, 0.214, 0.213, 0.204, 0.214, 0.199, 0.213, 0.22, 0.214, 0.209, 0.212, 0.217, 0.23, 0.201, 0.211, 0.21, 0.204, 0.208, 0.204, 0.228, 0.201, 0.233, 0.225, 0.228, 0.2, 0.201, 0.204, 0.2, 0.198, 0.214, 0.201, 0.197, 0.197, 0.211, 0.192, 0.225, 0.195, 0.211, 0.209, 0.201, 0.211, 0.216, 0.231, 0.202, 0.196, 0.2, 0.208, 0.208, 0.202, 0.199, 0.225, 0.196, 0.202, 0.203, 0.227, 0.225, 0.204, 0.221, 0.198, 0.206, 0.212, 0.209, 0.207, 0.204, 0.204, 0.212, 0.206, 0.231, 0.199, 0.194, 0.21, 0.211, 0.204, 0.224, 0.202, 0.195, 0.197, 0.2, 0.194, 0.208, 0.197, 0.213, 0.201, 0.198, 0.199, 0.198, 0.192, 0.203, 0.209, 0.203, 0.193, 0.2, 0.195, 0.198, 0.203, 0.217, 0.212, 0.197, 0.197, 0.207, 0.217, 0.195, 0.209, 0.197, 0.196, 0.204, 0.194, 0.194, 0.213, 0.229, 0.199, 0.199, 0.193, 0.207, 0.19, 0.199, 0.203, 0.196, 0.201, 0.205, 0.201, 0.205, 0.192, 0.211, 0.206, 0.197, 0.199, 0.197, 0.199, 0.193, 0.219, 0.194, 0.211, 0.205, 0.207, 0.187, 0.206, 0.193, 0.202, 0.205, 0.205, 0.248, 0.187, 0.198, 0.203, 0.193, 0.19, 0.195, 0.207, 0.22, 0.215, 0.21, 0.199, 0.195, 0.2, 0.208, 0.193, 0.21, 0.217, 0.193, 0.207, 0.199, 0.189, 0.207, 0.195, 0.205, 0.195, 0.202, 0.202, 0.21, 0.217, 0.233, 0.208, 0.212, 0.226, 0.204, 0.206, 0.205, 0.195, 0.203, 0.199, 0.199, 0.214, 0.205, 0.201, 0.204, 0.18, 0.208, 0.215, 0.193, 0.193, 0.2, 0.186, 0.212, 0.212, 0.197, 0.211, 0.189, 0.189, 0.194, 0.19, 0.198, 0.221, 0.207, 0.207, 0.192, 0.21, 0.191, 0.215, 0.189, 0.203, 0.197, 0.197, 0.212, 0.185, 0.193, 0.215, 0.183, 0.195, 0.196, 0.194, 0.197, 0.2, 0.189, 0.198, 0.199, 0.205, 0.224, 0.251, 0.215, 0.214, 0.198, 0.207, 0.191, 0.192, 0.2, 0.207, 0.186, 0.202, 0.196, 0.203, 0.201, 0.202, 0.189, 0.18, 0.193, 0.193, 0.189, 0.194, 0.198, 0.191, 0.195, 0.213, 0.187, 0.194, 0.208, 0.19, 0.205, 0.203, 0.186, 0.205, 0.205, 0.192, 0.208, 0.202, 0.201, 0.202, 0.189, 0.203, 0.192, 0.193, 0.191, 0.193, 0.197, 0.202, 0.201, 0.188, 0.182, 0.184, 0.192, 0.186, 0.184, 0.199, 0.194, 0.178, 0.204, 0.2, 0.179, 0.184, 0.201, 0.19, 0.201, 0.203, 0.192, 0.196, 0.178, 0.188, 0.186, 0.183, 0.196, 0.211, 0.189, 0.196, 0.198, 0.184, 0.185, 0.194, 0.182, 0.184, 0.195, 0.189, 0.189, 0.2, 0.196, 0.213, 0.186, 0.199, 0.19, 0.208, 0.212, 0.176, 0.192, 0.195, 0.182, 0.177, 0.19, 0.184, 0.183, 0.185, 0.186, 0.175, 0.21, 0.195, 0.205, 0.212, 0.186, 0.195, 0.19, 0.194, 0.189, 0.214, 0.196, 0.199, 0.198, 0.184, 0.184, 0.192, 0.185, 0.183, 0.182, 0.174, 0.19, 0.18, 0.182, 0.193, 0.203, 0.176, 0.183, 0.187, 0.192, 0.193, 0.198, 0.185, 0.182, 0.188, 0.191, 0.187, 0.196, 0.184, 0.184, 0.202, 0.212, 0.175, 0.212, 0.2, 0.206, 0.549, 0.71, 0.296, 0.366, 0.227, 0.216, 0.204, 0.192, 0.198, 0.191, 0.184, 0.187, 0.186, 0.183, 0.18, 0.193, 0.179, 0.188, 0.192, 0.196, 0.198, 0.191, 0.184, 0.182, 0.181, 0.186, 0.174, 0.186, 0.187, 0.195, 0.183, 0.182, 0.194, 0.188, 0.182, 0.183, 0.191, 0.201, 0.181, 0.178, 0.177, 0.176, 0.179, 0.184, 0.174, 0.179, 0.174, 0.193, 0.192, 0.199, 0.186, 0.187, 0.183, 0.18, 0.189, 0.194, 0.176, 0.183, 0.192, 0.194, 0.177, 0.171, 0.189, 0.187, 0.191, 0.209, 0.187, 0.174, 0.188, 0.189, 0.187, 0.179, 0.201, 0.192, 0.185, 0.181, 0.184, 0.18, 0.176, 0.184, 0.182, 0.203, 0.168, 0.176, 0.175, 0.19, 0.179, 0.184, 0.186, 0.182, 0.186, 0.178, 0.2, 0.191, 0.192, 0.201, 0.194, 0.192, 0.172, 0.191, 0.179, 0.176, 0.188, 0.173, 0.185, 0.193, 0.21, 0.185, 0.175, 0.177, 0.18, 0.184, 0.195, 0.182, 0.173, 0.182, 0.175, 0.193, 0.182, 0.19, 0.182, 0.186, 0.184, 0.194, 0.182, 0.178, 0.187, 0.195, 0.181, 0.187, 0.175, 0.185, 0.18, 0.179, 0.186, 0.194, 0.182, 0.18, 0.19, 0.193, 0.196, 0.167, 0.188, 0.193, 0.179, 0.183, 0.191, 0.177, 0.215, 0.188, 0.197, 0.175, 0.188, 0.181, 0.191, 0.171, 0.19, 0.186, 0.185, 0.19, 0.18, 0.195, 0.177, 0.171, 0.179, 0.185, 0.186, 0.18, 0.183, 0.18, 0.176, 0.18, 0.182, 0.177, 0.178, 0.19]\n",
      "테스트셋 오차추이: [2.379, 2.246, 2.151, 2.06, 1.991, 1.956, 1.916, 1.894, 1.875, 1.856, 1.842, 1.82, 1.803, 1.79, 1.792, 1.774, 1.782, 1.769, 1.768, 1.737, 1.754, 1.738, 1.727, 1.744, 1.729, 1.697, 1.699, 1.697, 1.686, 1.682, 1.687, 1.681, 1.717, 1.685, 1.688, 1.677, 1.666, 1.661, 1.669, 1.671, 1.67, 1.675, 1.647, 1.649, 1.669, 1.679, 1.649, 1.645, 1.65, 1.645, 1.651, 1.661, 1.646, 1.627, 1.618, 1.605, 1.632, 1.63, 1.649, 1.627, 1.612, 1.616, 1.612, 1.617, 1.625, 1.594, 1.62, 1.619, 1.588, 1.579, 1.617, 1.607, 1.58, 1.571, 1.609, 1.577, 1.578, 1.588, 1.568, 1.59, 1.568, 1.572, 1.549, 1.543, 1.542, 1.576, 1.539, 1.532, 1.538, 1.521, 1.532, 1.557, 1.524, 1.508, 1.504, 1.543, 1.488, 1.487, 1.495, 1.482, 1.505, 1.5, 1.466, 1.47, 1.478, 1.478, 1.462, 1.453, 1.438, 1.426, 1.43, 1.452, 1.437, 1.461, 1.447, 1.402, 1.422, 1.437, 1.409, 1.424, 1.397, 1.404, 1.412, 1.391, 1.354, 1.379, 1.377, 1.371, 1.357, 1.362, 1.367, 1.365, 1.349, 1.34, 1.386, 1.365, 1.303, 1.319, 1.357, 1.301, 1.305, 1.319, 1.308, 1.294, 1.307, 1.283, 1.268, 1.318, 1.289, 1.256, 1.266, 1.247, 1.285, 1.258, 1.27, 1.246, 1.242, 1.248, 1.204, 1.232, 1.242, 1.223, 1.226, 1.202, 1.243, 1.2, 1.202, 1.188, 1.168, 1.218, 1.206, 1.12, 1.172, 1.155, 1.146, 1.142, 1.154, 1.153, 1.147, 1.138, 1.105, 1.116, 1.126, 1.083, 1.101, 1.095, 1.108, 1.105, 1.111, 1.088, 1.078, 1.054, 1.093, 1.082, 1.03, 1.07, 1.071, 1.067, 1.006, 1.017, 1.047, 1.003, 1.023, 0.978, 1.031, 1.026, 0.961, 1.004, 0.996, 0.984, 0.964, 0.996, 0.96, 0.963, 0.943, 0.941, 0.922, 0.954, 0.951, 0.896, 0.925, 0.934, 0.955, 0.897, 0.857, 0.932, 0.933, 0.876, 0.86, 0.923, 0.887, 0.885, 0.866, 0.898, 0.894, 0.89, 0.911, 0.844, 0.855, 0.862, 0.84, 0.874, 0.84, 0.837, 0.866, 0.852, 0.815, 0.808, 0.83, 0.827, 0.823, 0.812, 0.823, 0.826, 0.786, 0.78, 0.797, 0.8, 0.808, 0.787, 0.794, 0.773, 0.805, 0.762, 0.786, 0.763, 0.733, 0.784, 0.81, 0.77, 0.748, 0.744, 0.763, 0.771, 0.736, 0.749, 0.749, 0.746, 0.78, 0.737, 0.72, 0.706, 0.734, 0.772, 0.705, 0.688, 0.673, 0.73, 0.7, 0.695, 0.718, 0.683, 0.739, 0.687, 0.663, 0.733, 0.669, 0.679, 0.712, 0.686, 0.649, 0.665, 0.67, 0.662, 0.684, 0.659, 0.696, 0.662, 0.647, 0.648, 0.644, 0.62, 0.677, 0.658, 0.64, 0.62, 0.647, 0.638, 0.66, 0.602, 0.614, 0.64, 0.612, 0.632, 0.62, 0.646, 0.602, 0.649, 0.549, 0.627, 0.6, 0.57, 0.612, 0.596, 0.588, 0.61, 0.601, 0.579, 0.573, 0.618, 0.536, 0.566, 0.557, 0.543, 0.613, 0.598, 0.534, 0.546, 0.585, 0.539, 0.543, 0.56, 0.563, 0.54, 0.506, 0.528, 0.532, 0.557, 0.529, 0.551, 0.551, 0.521, 0.494, 0.579, 0.541, 0.54, 0.516, 0.479, 0.476, 0.535, 0.556, 0.533, 0.484, 0.5, 0.441, 0.538, 0.482, 0.533, 0.475, 0.507, 0.506, 0.487, 0.476, 0.484, 0.517, 0.499, 0.454, 0.443, 0.503, 0.499, 0.476, 0.48, 0.455, 0.467, 0.466, 0.501, 0.44, 0.498, 0.459, 0.418, 0.431, 0.446, 0.462, 0.456, 0.435, 0.459, 0.429, 0.46, 0.403, 0.43, 0.431, 0.444, 0.441, 0.414, 0.422, 0.482, 0.445, 0.372, 0.472, 0.418, 0.393, 0.414, 0.463, 0.405, 0.446, 0.44, 0.449, 0.451, 0.372, 0.415, 0.437, 0.422, 0.415, 0.366, 0.4, 0.469, 0.393, 0.351, 0.39, 0.454, 0.38, 0.402, 0.419, 0.365, 0.417, 0.341, 0.388, 0.418, 0.368, 0.432, 0.371, 0.388, 0.366, 0.364, 0.327, 0.398, 0.387, 0.346, 0.404, 0.396, 0.346, 0.345, 0.399, 0.344, 0.381, 0.347, 0.397, 0.365, 0.391, 0.336, 0.314, 0.347, 0.349, 0.356, 0.359, 0.288, 0.411, 0.351, 0.341, 0.369, 0.35, 0.326, 0.326, 0.348, 0.32, 0.382, 0.31, 0.318, 0.372, 0.348, 0.306, 0.335, 0.343, 0.309, 0.318, 0.303, 0.352, 0.298, 0.354, 0.303, 0.288, 0.313, 0.317, 0.286, 0.328, 0.33, 0.324, 0.281, 0.329, 0.294, 0.3, 0.295, 0.302, 0.317, 0.298, 0.261, 0.317, 0.315, 0.276, 0.29, 0.265, 0.278, 0.342, 0.334, 0.284, 0.261, 0.341, 0.237, 0.261, 0.315, 0.269, 0.288, 0.244, 0.291, 0.292, 0.242, 0.27, 0.247, 0.278, 0.279, 0.243, 0.291, 0.271, 0.241, 0.261, 0.336, 0.207, 0.228, 0.28, 0.241, 0.299, 0.236, 0.258, 0.285, 0.221, 0.258, 0.244, 0.231, 0.257, 0.206, 0.287, 0.249, 0.236, 0.266, 0.242, 0.256, 0.216, 0.222, 0.285, 0.253, 0.23, 0.237, 0.244, 0.255, 0.244, 0.233, 0.214, 0.229, 0.207, 0.231, 0.223, 0.252, 0.219, 0.221, 0.242, 0.232, 0.213, 0.219, 0.251, 0.211, 0.2, 0.227, 0.205, 0.266, 0.187, 0.224, 0.215, 0.205, 0.185, 0.221, 0.278, 0.191, 0.192, 0.203, 0.237, 0.2, 0.184, 0.18, 0.235, 0.223, 0.195, 0.173, 0.192, 0.175, 0.253, 0.185, 0.194, 0.224, 0.222, 0.197, 0.188, 0.171, 0.242, 0.187, 0.167, 0.19, 0.215, 0.188, 0.194, 0.169, 0.167, 0.198, 0.189, 0.175, 0.177, 0.195, 0.198, 0.173, 0.205, 0.179, 0.151, 0.201, 0.198, 0.156, 0.181, 0.175, 0.169, 0.197, 0.16, 0.179, 0.19, 0.161, 0.212, 0.171, 0.132, 0.203, 0.174, 0.198, 0.148, 0.168, 0.155, 0.17, 0.175, 0.154, 0.188, 0.15, 0.156, 0.146, 0.181, 0.177, 0.142, 0.133, 0.166, 0.18, 0.145, 0.136, 0.196, 0.183, 0.122, 0.156, 0.159, 0.143, 0.144, 0.13, 0.228, 0.149, 0.146, 0.168, 0.136, 0.14, 0.155, 0.153, 0.158, 0.146, 0.148, 0.141, 0.13, 0.137, 0.162, 0.136, 0.149, 0.155, 0.121, 0.156, 0.15, 0.131, 0.124, 0.146, 0.169, 0.117, 0.115, 0.168, 0.158, 0.106, 0.171, 0.138, 0.132, 0.147, 0.154, 0.102, 0.121, 0.137, 0.138, 0.121, 0.123, 0.142, 0.135, 0.109, 0.125, 0.143, 0.118, 0.118, 0.128, 0.124, 0.115, 0.152, 0.118, 0.138, 0.142, 0.095, 0.131, 0.115, 0.133, 0.139, 0.112, 0.121, 0.111, 0.131, 0.104, 0.127, 0.121, 0.104, 0.11, 0.129, 0.118, 0.125, 0.103, 0.112, 0.098, 0.15, 0.1, 0.112, 0.114, 0.11, 0.099, 0.123, 0.115, 0.106, 0.1, 0.105, 0.118, 0.099, 0.111, 0.108, 0.092, 0.124, 0.091, 0.135, 0.092, 0.121, 0.159, 0.073, 0.11, 0.111, 0.12, 0.124, 0.098, 0.095, 0.102, 0.108, 0.101, 0.093, 0.103, 0.124, 0.13, 0.091, 0.088, 0.103, 0.087, 0.095, 0.103, 0.111, 0.085, 0.088, 0.096, 0.098, 0.097, 0.08, 0.117, 0.093, 0.109, 0.091, 0.111, 0.095, 0.084, 0.089, 0.105, 0.083, 0.081, 0.1, 0.098, 0.102, 0.089, 0.095, 0.104, 0.095, 0.087, 0.088, 0.106, 0.075, 0.08, 0.076, 0.096, 0.082, 0.098, 0.092, 0.064, 0.102, 0.114, 0.058, 0.09, 0.089, 0.08, 0.084, 0.098, 0.078, 0.066, 0.092, 0.085, 0.082, 0.106, 0.07, 0.073, 0.085, 0.085, 0.087, 0.073, 0.074, 0.079, 0.083, 0.067, 0.082, 0.106, 0.066, 0.075, 0.099, 0.064, 0.068, 0.104, 0.061, 0.063, 0.11, 0.063, 0.06, 0.071, 0.078, 0.062, 0.086, 0.074, 0.071, 0.065, 0.062, 0.097, 0.074, 0.057, 0.075, 0.087, 0.08, 0.096, 0.07, 0.06, 0.069, 0.076, 0.069, 0.065, 0.068, 0.091, 0.065, 0.054, 0.076, 0.054, 0.069, 0.094, 0.058, 0.044, 0.081, 0.082, 0.056, 0.086, 0.062, 0.078, 0.063, 0.058, 0.046, 0.074, 0.067, 0.055, 0.052, 0.071, 0.062, 0.066, 0.064, 0.05, 0.055, 0.065, 0.068, 0.083, 0.046, 0.059, 0.08, 0.058, 0.055, 0.059, 0.057, 0.057, 0.056, 0.061, 0.052, 0.053, 0.055, 0.098, 0.052, 0.058, 0.059, 0.066, 0.062, 0.044, 0.054, 0.059, 0.042, 0.068, 0.059, 0.056, 0.052, 0.048, 0.05, 0.069, 0.059, 0.037, 0.064, 0.059, 0.051, 0.04, 0.074, 0.072, 0.067, 0.042, 0.048, 0.081, 0.043, 0.046, 0.07, 0.048, 0.043, 0.046, 0.051, 0.047, 0.063, 0.051, 0.046, 0.045, 0.06, 0.045, 0.039, 0.06, 0.054, 0.054, 0.057, 0.037, 0.045, 0.04, 0.055, 0.041, 0.045, 0.079, 0.041, 0.046, 0.056, 0.045, 0.045, 0.052, 0.047, 0.04, 0.047, 0.038, 0.052, 0.042, 0.044, 0.043, 0.058, 0.034, 0.042, 0.054, 0.037, 0.034, 0.043, 0.047, 0.036, 0.039, 0.067, 0.047, 0.032, 0.036, 0.052, 0.042, 0.048, 0.044, 0.041, 0.045, 0.045, 0.039, 0.051, 0.031, 0.043, 0.058, 0.047, 0.032, 0.045, 0.06, 0.039, 0.038, 0.031, 0.06, 0.039, 0.031, 0.052, 0.041, 0.054, 0.03, 0.039, 0.03, 0.058, 0.046, 0.034, 0.03, 0.081, 0.035, 0.034, 0.041, 0.032, 0.039, 0.037, 0.036, 0.046, 0.038, 0.034, 0.03, 0.038, 0.038, 0.041, 0.033, 0.039, 0.029, 0.059, 0.036, 0.035, 0.04, 0.035, 0.04, 0.062, 0.036, 0.026, 0.058, 0.031, 0.036, 0.032, 0.032, 0.023, 0.042, 0.038, 0.032, 0.036, 0.033, 0.039, 0.048, 0.031, 0.027, 0.04, 0.028, 0.033, 0.036, 0.035, 0.033, 0.036, 0.035, 0.024, 0.037, 0.039, 0.028, 0.033, 0.034, 0.027, 0.031, 0.039, 0.037, 0.036, 0.027, 0.024, 0.03, 0.04, 0.029, 0.028, 0.024, 0.036, 0.035, 0.025, 0.032, 0.037, 0.029, 0.026, 0.028, 0.029, 0.033, 0.027, 0.026, 0.028, 0.03, 0.031, 0.027, 0.027, 0.028, 0.035, 0.034, 0.028, 0.025, 0.026, 0.034, 0.025, 0.03, 0.05, 0.019, 0.025, 0.026, 0.035, 0.021, 0.028, 0.044, 0.022, 0.029, 0.033, 0.024, 0.019, 0.032, 0.026, 0.018, 0.037, 0.02, 0.019, 0.03, 0.03, 0.024, 0.03, 0.018, 0.027, 0.023, 0.021, 0.034, 0.023, 0.017, 0.033, 0.026, 0.022, 0.029, 0.033, 0.025, 0.024, 0.023, 0.026, 0.031, 0.018, 0.019, 0.03, 0.018, 0.029, 0.019, 0.029, 0.02, 0.02, 0.027, 0.033, 0.03, 0.023, 0.026, 0.019, 0.024, 0.024, 0.033, 0.019, 0.016, 0.02, 0.028, 0.024, 0.017, 0.025, 0.03, 0.016, 0.036, 0.026, 0.015, 0.013, 0.027, 0.025, 0.017, 0.021, 0.023, 0.019, 0.023, 0.02, 0.018, 0.017, 0.03, 0.029, 0.016, 0.015, 0.031, 0.019, 0.02, 0.018, 0.02, 0.021, 0.02, 0.017, 0.023, 0.019, 0.024, 0.017, 0.038, 0.017, 0.017, 0.019, 0.021, 0.026, 0.016, 0.022, 0.017, 0.021, 0.017, 0.018, 0.017, 0.027, 0.015, 0.015, 0.034, 0.02, 0.019, 0.021, 0.022, 0.028, 0.018, 0.013, 0.025, 0.019, 0.019, 0.017, 0.023, 0.025, 0.013, 0.017, 0.018, 0.016, 0.016, 0.017, 0.027, 0.016, 0.013, 0.019, 0.03, 0.012, 0.018, 0.022, 0.02, 0.013, 0.015, 0.017, 0.018, 0.016, 0.015, 0.018, 0.016, 0.018, 0.015, 0.013, 0.025, 0.016, 0.021, 0.014, 0.017, 0.015, 0.013, 0.018, 0.017, 0.015, 0.015, 0.015, 0.017, 0.014, 0.022, 0.023, 0.012, 0.011, 0.026, 0.017, 0.02, 0.015, 0.016, 0.015, 0.015, 0.013, 0.016, 0.014, 0.014, 0.016, 0.018, 0.018, 0.017, 0.012, 0.015, 0.018, 0.014, 0.016, 0.018, 0.021, 0.024, 0.011, 0.021, 0.015, 0.012, 0.015, 0.023, 0.013, 0.009, 0.014, 0.022, 0.02, 0.012, 0.016, 0.013, 0.013, 0.013, 0.015, 0.01, 0.022, 0.018, 0.012, 0.012, 0.014, 0.014, 0.018, 0.02, 0.011, 0.013, 0.015, 0.008, 0.016, 0.015, 0.011, 0.01, 0.012, 0.021, 0.016, 0.01, 0.01, 0.014, 0.009, 0.017, 0.014, 0.012, 0.011, 0.014, 0.012, 0.015, 0.013, 0.007, 0.018, 0.012, 0.009, 0.012, 0.016, 0.014, 0.013, 0.016, 0.014, 0.011, 0.013, 0.015, 0.017, 0.014, 0.012, 0.009, 0.02, 0.01, 0.013, 0.009, 0.016, 0.014, 0.009, 0.009, 0.009, 0.018, 0.012, 0.009, 0.01, 0.014, 0.009, 0.013, 0.01, 0.014, 0.014, 0.011, 0.012, 0.012, 0.011, 0.012, 0.006, 0.014, 0.014, 0.009, 0.009, 0.017, 0.009, 0.01, 0.011, 0.011, 0.01, 0.013, 0.009, 0.007, 0.009, 0.018, 0.007, 0.006, 0.015, 0.015, 0.009, 0.009, 0.008, 0.013, 0.015, 0.008, 0.012, 0.01, 0.01, 0.014, 0.01, 0.011, 0.011, 0.008, 0.015, 0.009, 0.007, 0.018, 0.009, 0.007, 0.009, 0.009, 0.01, 0.012, 0.007, 0.009, 0.01, 0.008, 0.01, 0.011, 0.011, 0.007, 0.006, 0.008, 0.012, 0.014, 0.006, 0.015, 0.007, 0.01, 0.008, 0.007, 0.011, 0.008, 0.008, 0.014, 0.006, 0.008, 0.008, 0.008, 0.021, 0.012, 0.005, 0.008, 0.011, 0.012, 0.007, 0.01, 0.011, 0.005, 0.007, 0.013, 0.006, 0.007, 0.01, 0.01, 0.012, 0.008, 0.01, 0.013, 0.009, 0.008, 0.007, 0.007, 0.009, 0.01, 0.006, 0.009, 0.01, 0.009, 0.01, 0.008, 0.006, 0.008, 0.007, 0.009, 0.009, 0.006, 0.005, 0.005, 0.012, 0.008, 0.007, 0.007, 0.01, 0.006, 0.009, 0.008, 0.007, 0.006, 0.009, 0.007, 0.007, 0.007, 0.017, 0.005, 0.004, 0.007, 0.009, 0.007, 0.005, 0.007, 0.01, 0.008, 0.004, 0.008, 0.011, 0.006, 0.007, 0.006, 0.008, 0.009, 0.006, 0.006, 0.009, 0.008, 0.006, 0.005, 0.008, 0.006, 0.006, 0.009, 0.007, 0.006, 0.004, 0.005, 0.018, 0.006, 0.005, 0.005, 0.004, 0.006, 0.006, 0.006, 0.006, 0.014, 0.007, 0.005, 0.006, 0.008, 0.006, 0.008, 0.006, 0.007, 0.004, 0.008, 0.005, 0.006, 0.006, 0.008, 0.009, 0.008, 0.005, 0.007, 0.006, 0.01, 0.015, 0.004, 0.005, 0.011, 0.003, 0.004, 0.007, 0.005, 0.004, 0.006, 0.005, 0.008, 0.006, 0.004, 0.005, 0.006, 0.007, 0.004, 0.008, 0.006, 0.004, 0.004, 0.009, 0.005, 0.004, 0.005, 0.006, 0.005, 0.005, 0.004, 0.005, 0.009, 0.004, 0.005, 0.006, 0.004, 0.005, 0.006, 0.005, 0.004, 0.006, 0.005, 0.007, 0.004, 0.006, 0.004, 0.004, 0.006, 0.009, 0.005, 0.004, 0.008, 0.005, 0.006, 0.004, 0.004, 0.008, 0.003, 0.006, 0.008, 0.004, 0.003, 0.005, 0.007, 0.004, 0.003, 0.008, 0.008, 0.005, 0.005, 0.004, 0.004, 0.005, 0.004, 0.003, 0.005, 0.004, 0.006, 0.004, 0.005, 0.005, 0.004, 0.004, 0.008, 0.006, 0.004, 0.003, 0.003, 0.005, 0.003, 0.008, 0.004, 0.002, 0.006, 0.004, 0.003, 0.007, 0.006, 0.005, 0.006, 0.005, 0.005, 0.004, 0.007, 0.003, 0.004, 0.004, 0.004, 0.005, 0.004, 0.003, 0.002, 0.004, 0.009, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.006, 0.002, 0.005, 0.005, 0.005, 0.003, 0.003, 0.003, 0.005, 0.003, 0.004, 0.004, 0.002, 0.003, 0.006, 0.005, 0.003, 0.004, 0.003, 0.004, 0.005, 0.005, 0.003, 0.005, 0.005, 0.003, 0.005, 0.003, 0.004, 0.007, 0.003, 0.004, 0.004, 0.003, 0.003, 0.004, 0.005, 0.004, 0.002, 0.002, 0.006, 0.007, 0.002, 0.003, 0.006, 0.004, 0.004, 0.003, 0.004, 0.003, 0.004, 0.005, 0.004, 0.004, 0.003, 0.003, 0.003, 0.003, 0.004, 0.003, 0.003, 0.003, 0.005, 0.002, 0.002, 0.003, 0.005, 0.004, 0.003, 0.003, 0.006, 0.003, 0.002, 0.003, 0.005, 0.003, 0.003, 0.003, 0.002, 0.007, 0.004, 0.003, 0.004, 0.014, 0.012, 0.045, 0.117, 0.004, 0.014, 0.01, 0.007, 0.006, 0.004, 0.007, 0.005, 0.004, 0.007, 0.006, 0.007, 0.006, 0.005, 0.005, 0.003, 0.005, 0.006, 0.004, 0.005, 0.004, 0.006, 0.005, 0.004, 0.004, 0.003, 0.005, 0.005, 0.003, 0.005, 0.005, 0.004, 0.003, 0.006, 0.003, 0.003, 0.004, 0.004, 0.004, 0.004, 0.003, 0.004, 0.004, 0.004, 0.005, 0.004, 0.005, 0.004, 0.003, 0.004, 0.005, 0.003, 0.005, 0.004, 0.003, 0.003, 0.007, 0.004, 0.003, 0.003, 0.007, 0.007, 0.003, 0.003, 0.004, 0.004, 0.004, 0.005, 0.004, 0.002, 0.004, 0.004, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.004, 0.003, 0.003, 0.002, 0.005, 0.003, 0.003, 0.004, 0.004, 0.004, 0.002, 0.003, 0.003, 0.004, 0.005, 0.002, 0.002, 0.004, 0.007, 0.003, 0.003, 0.003, 0.003, 0.004, 0.003, 0.006, 0.004, 0.002, 0.003, 0.003, 0.004, 0.002, 0.003, 0.003, 0.003, 0.003, 0.002, 0.005, 0.005, 0.003, 0.002, 0.003, 0.004, 0.003, 0.004, 0.004, 0.002, 0.003, 0.006, 0.003, 0.004, 0.003, 0.003, 0.003, 0.004, 0.003, 0.003, 0.004, 0.003, 0.005, 0.003, 0.004, 0.006, 0.004, 0.004, 0.004, 0.003, 0.005, 0.002, 0.004, 0.004, 0.004, 0.003, 0.002, 0.003, 0.004, 0.004, 0.003, 0.004, 0.002, 0.003, 0.003, 0.003, 0.003, 0.002, 0.003, 0.003, 0.003, 0.003, 0.002, 0.003, 0.002, 0.003, 0.003, 0.004]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋   정확도추이: [0.2, 0.244, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 0.356, 0.378, 0.4, 0.378, 0.4, 0.378, 0.4, 0.4, 0.4, 0.4, 0.422, 0.4, 0.4, 0.422, 0.4, 0.378, 0.4, 0.378, 0.378, 0.378, 0.378, 0.4, 0.4, 0.422, 0.444, 0.467, 0.489, 0.422, 0.4, 0.378, 0.4, 0.489, 0.489, 0.422, 0.422, 0.489, 0.489, 0.489, 0.489, 0.467, 0.489, 0.467, 0.511, 0.511, 0.511, 0.422, 0.467, 0.489, 0.489, 0.511, 0.511, 0.511, 0.511, 0.511, 0.511, 0.511, 0.489, 0.467, 0.444, 0.489, 0.489, 0.511, 0.511, 0.444, 0.467, 0.533, 0.556, 0.489, 0.511, 0.556, 0.578, 0.578, 0.556, 0.489, 0.489, 0.578, 0.578, 0.556, 0.533, 0.556, 0.556, 0.556, 0.556, 0.578, 0.533, 0.511, 0.578, 0.556, 0.533, 0.489, 0.556, 0.578, 0.578, 0.533, 0.533, 0.578, 0.578, 0.533, 0.511, 0.556, 0.556, 0.489, 0.556, 0.511, 0.556, 0.578, 0.556, 0.489, 0.533, 0.556, 0.556, 0.533, 0.556, 0.556, 0.556, 0.533, 0.533, 0.556, 0.511, 0.444, 0.511, 0.511, 0.578, 0.578, 0.533, 0.533, 0.556, 0.556, 0.511, 0.6, 0.578, 0.533, 0.489, 0.511, 0.533, 0.578, 0.556, 0.489, 0.489, 0.6, 0.578, 0.556, 0.467, 0.578, 0.578, 0.511, 0.533, 0.533, 0.556, 0.556, 0.511, 0.556, 0.578, 0.533, 0.533, 0.556, 0.622, 0.578, 0.578, 0.556, 0.556, 0.578, 0.556, 0.578, 0.578, 0.556, 0.533, 0.578, 0.6, 0.6, 0.578, 0.556, 0.533, 0.556, 0.578, 0.578, 0.6, 0.6, 0.6, 0.578, 0.6, 0.6, 0.533, 0.6, 0.6, 0.556, 0.578, 0.6, 0.6, 0.578, 0.578, 0.6, 0.6, 0.622, 0.578, 0.578, 0.6, 0.578, 0.578, 0.6, 0.6, 0.6, 0.622, 0.6, 0.622, 0.6, 0.6, 0.622, 0.6, 0.622, 0.578, 0.6, 0.6, 0.622, 0.644, 0.622, 0.622, 0.644, 0.644, 0.556, 0.644, 0.644, 0.644, 0.622, 0.622, 0.6, 0.6, 0.644, 0.644, 0.644, 0.644, 0.622, 0.644, 0.644, 0.644, 0.644, 0.644, 0.622, 0.644, 0.644, 0.644, 0.644, 0.622, 0.644, 0.667, 0.644, 0.622, 0.644, 0.644, 0.667, 0.644, 0.644, 0.622, 0.667, 0.644, 0.667, 0.667, 0.6, 0.644, 0.644, 0.667, 0.644, 0.644, 0.644, 0.644, 0.644, 0.644, 0.667, 0.667, 0.667, 0.667, 0.622, 0.667, 0.689, 0.689, 0.667, 0.644, 0.667, 0.667, 0.667, 0.689, 0.689, 0.644, 0.689, 0.689, 0.644, 0.689, 0.644, 0.644, 0.689, 0.689, 0.689, 0.667, 0.667, 0.711, 0.667, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.667, 0.711, 0.711, 0.711, 0.711, 0.711, 0.667, 0.711, 0.711, 0.711, 0.711, 0.711, 0.733, 0.689, 0.711, 0.711, 0.711, 0.711, 0.689, 0.733, 0.667, 0.711, 0.711, 0.711, 0.711, 0.711, 0.733, 0.711, 0.733, 0.711, 0.733, 0.689, 0.711, 0.711, 0.733, 0.711, 0.711, 0.711, 0.711, 0.756, 0.711, 0.711, 0.711, 0.711, 0.711, 0.689, 0.711, 0.711, 0.733, 0.733, 0.733, 0.756, 0.778, 0.756, 0.756, 0.733, 0.733, 0.733, 0.733, 0.733, 0.756, 0.756, 0.756, 0.711, 0.711, 0.756, 0.733, 0.778, 0.733, 0.733, 0.756, 0.733, 0.711, 0.778, 0.756, 0.733, 0.778, 0.711, 0.756, 0.756, 0.756, 0.756, 0.733, 0.733, 0.756, 0.756, 0.733, 0.756, 0.756, 0.733, 0.778, 0.733, 0.756, 0.733, 0.756, 0.756, 0.756, 0.778, 0.778, 0.778, 0.756, 0.756, 0.756, 0.778, 0.822, 0.756, 0.778, 0.733, 0.756, 0.778, 0.778, 0.778, 0.8, 0.8, 0.778, 0.778, 0.778, 0.778, 0.778, 0.8, 0.756, 0.778, 0.778, 0.778, 0.733, 0.778, 0.8, 0.778, 0.778, 0.756, 0.778, 0.778, 0.778, 0.756, 0.756, 0.756, 0.756, 0.8, 0.756, 0.778, 0.778, 0.8, 0.778, 0.778, 0.8, 0.8, 0.778, 0.778, 0.756, 0.778, 0.778, 0.756, 0.778, 0.8, 0.778, 0.8, 0.8, 0.778, 0.778, 0.778, 0.778, 0.778, 0.778, 0.8, 0.8, 0.756, 0.8, 0.8, 0.756, 0.8, 0.778, 0.8, 0.8, 0.756, 0.822, 0.756, 0.756, 0.778, 0.778, 0.822, 0.822, 0.756, 0.8, 0.822, 0.8, 0.8, 0.8, 0.778, 0.822, 0.8, 0.778, 0.8, 0.8, 0.822, 0.8, 0.8, 0.8, 0.844, 0.8, 0.822, 0.8, 0.8, 0.8, 0.8, 0.8, 0.822, 0.844, 0.778, 0.8, 0.778, 0.822, 0.844, 0.822, 0.822, 0.8, 0.8, 0.822, 0.844, 0.844, 0.756, 0.8, 0.822, 0.8, 0.822, 0.8, 0.822, 0.8, 0.778, 0.8, 0.778, 0.822, 0.822, 0.778, 0.822, 0.822, 0.822, 0.822, 0.844, 0.8, 0.8, 0.822, 0.844, 0.822, 0.8, 0.8, 0.822, 0.778, 0.822, 0.822, 0.822, 0.778, 0.844, 0.822, 0.8, 0.822, 0.844, 0.778, 0.822, 0.822, 0.867, 0.822, 0.778, 0.844, 0.844, 0.8, 0.844, 0.8, 0.822, 0.822, 0.8, 0.822, 0.844, 0.867, 0.8, 0.822, 0.822, 0.8, 0.8, 0.844, 0.822, 0.822, 0.778, 0.8, 0.844, 0.867, 0.867, 0.822, 0.8, 0.844, 0.822, 0.844, 0.867, 0.844, 0.822, 0.844, 0.867, 0.822, 0.844, 0.822, 0.822, 0.822, 0.778, 0.822, 0.822, 0.822, 0.844, 0.867, 0.822, 0.8, 0.822, 0.8, 0.844, 0.844, 0.844, 0.844, 0.844, 0.822, 0.822, 0.844, 0.844, 0.822, 0.778, 0.867, 0.844, 0.822, 0.8, 0.844, 0.867, 0.844, 0.867, 0.844, 0.822, 0.822, 0.778, 0.844, 0.844, 0.822, 0.822, 0.867, 0.822, 0.844, 0.844, 0.822, 0.844, 0.867, 0.822, 0.822, 0.822, 0.867, 0.8, 0.778, 0.822, 0.822, 0.822, 0.822, 0.822, 0.844, 0.867, 0.844, 0.844, 0.867, 0.867, 0.844, 0.844, 0.867, 0.867, 0.822, 0.8, 0.867, 0.822, 0.822, 0.778, 0.867, 0.844, 0.844, 0.844, 0.844, 0.822, 0.844, 0.844, 0.844, 0.867, 0.844, 0.844, 0.844, 0.867, 0.822, 0.822, 0.844, 0.867, 0.822, 0.822, 0.8, 0.822, 0.844, 0.867, 0.844, 0.844, 0.822, 0.867, 0.867, 0.822, 0.844, 0.822, 0.822, 0.844, 0.822, 0.844, 0.867, 0.867, 0.844, 0.822, 0.822, 0.844, 0.822, 0.822, 0.867, 0.822, 0.844, 0.867, 0.844, 0.867, 0.844, 0.867, 0.844, 0.822, 0.844, 0.844, 0.8, 0.844, 0.822, 0.822, 0.822, 0.867, 0.844, 0.822, 0.822, 0.867, 0.844, 0.844, 0.867, 0.844, 0.867, 0.844, 0.822, 0.822, 0.844, 0.867, 0.844, 0.844, 0.8, 0.822, 0.822, 0.822, 0.867, 0.822, 0.844, 0.844, 0.844, 0.822, 0.867, 0.844, 0.867, 0.867, 0.8, 0.822, 0.822, 0.844, 0.867, 0.822, 0.867, 0.844, 0.867, 0.822, 0.8, 0.844, 0.867, 0.867, 0.844, 0.822, 0.867, 0.867, 0.822, 0.822, 0.844, 0.867, 0.822, 0.867, 0.822, 0.822, 0.844, 0.867, 0.844, 0.822, 0.844, 0.822, 0.867, 0.844, 0.822, 0.844, 0.8, 0.867, 0.844, 0.822, 0.844, 0.822, 0.867, 0.867, 0.844, 0.822, 0.822, 0.822, 0.822, 0.822, 0.867, 0.867, 0.867, 0.844, 0.844, 0.822, 0.844, 0.822, 0.844, 0.844, 0.867, 0.867, 0.867, 0.844, 0.889, 0.844, 0.867, 0.867, 0.844, 0.844, 0.844, 0.844, 0.822, 0.822, 0.844, 0.867, 0.867, 0.844, 0.844, 0.867, 0.844, 0.867, 0.867, 0.8, 0.867, 0.844, 0.867, 0.889, 0.867, 0.844, 0.867, 0.844, 0.867, 0.844, 0.844, 0.867, 0.822, 0.867, 0.867, 0.822, 0.867, 0.867, 0.867, 0.844, 0.844, 0.844, 0.867, 0.867, 0.867, 0.867, 0.822, 0.822, 0.822, 0.844, 0.867, 0.844, 0.867, 0.8, 0.8, 0.867, 0.867, 0.889, 0.822, 0.844, 0.8, 0.867, 0.867, 0.844, 0.867, 0.867, 0.867, 0.867, 0.822, 0.867, 0.867, 0.867, 0.867, 0.844, 0.822, 0.844, 0.844, 0.844, 0.867, 0.867, 0.889, 0.822, 0.844, 0.844, 0.822, 0.867, 0.844, 0.844, 0.844, 0.867, 0.8, 0.844, 0.844, 0.867, 0.844, 0.867, 0.867, 0.867, 0.844, 0.889, 0.844, 0.867, 0.8, 0.889, 0.844, 0.867, 0.867, 0.889, 0.822, 0.889, 0.867, 0.889, 0.844, 0.867, 0.844, 0.822, 0.844, 0.889, 0.867, 0.867, 0.822, 0.867, 0.867, 0.889, 0.867, 0.867, 0.889, 0.867, 0.867, 0.867, 0.844, 0.867, 0.844, 0.822, 0.867, 0.867, 0.889, 0.844, 0.889, 0.844, 0.867, 0.889, 0.867, 0.867, 0.867, 0.867, 0.889, 0.844, 0.867, 0.867, 0.844, 0.889, 0.844, 0.889, 0.889, 0.844, 0.867, 0.867, 0.867, 0.867, 0.867, 0.889, 0.889, 0.889, 0.844, 0.889, 0.867, 0.867, 0.889, 0.867, 0.889, 0.867, 0.844, 0.867, 0.867, 0.867, 0.889, 0.867, 0.867, 0.889, 0.844, 0.867, 0.844, 0.889, 0.844, 0.867, 0.867, 0.889, 0.844, 0.889, 0.867, 0.844, 0.844, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.867, 0.867, 0.844, 0.911, 0.867, 0.844, 0.822, 0.844, 0.889, 0.867, 0.844, 0.844, 0.889, 0.889, 0.867, 0.844, 0.889, 0.867, 0.8, 0.867, 0.867, 0.867, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.867, 0.889, 0.844, 0.889, 0.867, 0.844, 0.911, 0.867, 0.822, 0.889, 0.867, 0.867, 0.889, 0.867, 0.867, 0.933, 0.889, 0.867, 0.889, 0.889, 0.844, 0.867, 0.867, 0.867, 0.889, 0.889, 0.844, 0.911, 0.867, 0.822, 0.889, 0.867, 0.867, 0.889, 0.867, 0.889, 0.867, 0.867, 0.867, 0.844, 0.867, 0.889, 0.867, 0.867, 0.889, 0.889, 0.844, 0.889, 0.867, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.889, 0.844, 0.889, 0.867, 0.822, 0.844, 0.867, 0.889, 0.867, 0.867, 0.844, 0.867, 0.889, 0.889, 0.889, 0.867, 0.867, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.844, 0.889, 0.889, 0.911, 0.844, 0.889, 0.867, 0.867, 0.889, 0.844, 0.844, 0.889, 0.867, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.822, 0.889, 0.889, 0.889, 0.889, 0.867, 0.867, 0.867, 0.867, 0.889, 0.911, 0.867, 0.911, 0.911, 0.889, 0.844, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.867, 0.889, 0.867, 0.867, 0.844, 0.889, 0.867, 0.889, 0.867, 0.867, 0.889, 0.844, 0.867, 0.889, 0.844, 0.867, 0.889, 0.889, 0.889, 0.867, 0.867, 0.889, 0.889, 0.911, 0.867, 0.844, 0.889, 0.911, 0.889, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.911, 0.889, 0.889, 0.911, 0.889, 0.844, 0.844, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.867, 0.889, 0.867, 0.867, 0.889, 0.867, 0.844, 0.911, 0.867, 0.867, 0.911, 0.911, 0.889, 0.844, 0.911, 0.911, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.867, 0.889, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.911, 0.889, 0.889, 0.911, 0.911, 0.911, 0.889, 0.889, 0.867, 0.911, 0.889, 0.911, 0.889, 0.889, 0.867, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.867, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.911, 0.911, 0.911, 0.867, 0.889, 0.889, 0.911, 0.889, 0.911, 0.889, 0.867, 0.889, 0.911, 0.911, 0.911, 0.867, 0.911, 0.889, 0.911, 0.867, 0.889, 0.867, 0.889, 0.911, 0.889, 0.867, 0.844, 0.911, 0.889, 0.911, 0.911, 0.911, 0.867, 0.889, 0.867, 0.889, 0.911, 0.867, 0.911, 0.889, 0.911, 0.889, 0.889, 0.911, 0.867, 0.889, 0.867, 0.911, 0.911, 0.889, 0.867, 0.911, 0.889, 0.889, 0.867, 0.889, 0.889, 0.911, 0.911, 0.889, 0.867, 0.911, 0.889, 0.844, 0.889, 0.911, 0.911, 0.889, 0.844, 0.844, 0.889, 0.889, 0.889, 0.867, 0.911, 0.867, 0.933, 0.889, 0.889, 0.911, 0.889, 0.844, 0.867, 0.867, 0.867, 0.889, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.911, 0.867, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.8, 0.889, 0.911, 0.867, 0.911, 0.911, 0.911, 0.911, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.867, 0.867, 0.911, 0.889, 0.867, 0.911, 0.889, 0.911, 0.911, 0.911, 0.889, 0.867, 0.867, 0.911, 0.911, 0.867, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.844, 0.867, 0.911, 0.911, 0.911, 0.911, 0.889, 0.911, 0.889, 0.889, 0.911, 0.911, 0.889, 0.911, 0.889, 0.911, 0.889, 0.911, 0.867, 0.867, 0.867, 0.822, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.933, 0.889, 0.911, 0.889, 0.889, 0.867, 0.889, 0.911, 0.889, 0.889, 0.889, 0.867, 0.889, 0.911, 0.911, 0.911, 0.867, 0.889, 0.911, 0.911, 0.911, 0.911, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.867, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.911, 0.911, 0.911, 0.867, 0.867, 0.911, 0.889, 0.889, 0.844, 0.911, 0.911, 0.911, 0.889, 0.867, 0.867, 0.911, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.911, 0.867, 0.889, 0.889, 0.911, 0.911, 0.911, 0.867, 0.889, 0.911, 0.911, 0.911, 0.889, 0.911, 0.933, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.889, 0.911, 0.844, 0.911, 0.911, 0.889, 0.911, 0.911, 0.867, 0.889, 0.867, 0.911, 0.911, 0.911, 0.911, 0.889, 0.867, 0.911, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.911, 0.867, 0.889, 0.889, 0.867, 0.911, 0.889, 0.889, 0.911, 0.889, 0.911, 0.911, 0.867, 0.911, 0.889, 0.889, 0.889, 0.889, 0.911, 0.867, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.911, 0.911, 0.844, 0.911, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.867, 0.911, 0.889, 0.911, 0.844, 0.911, 0.889, 0.867, 0.889, 0.889, 0.933, 0.889, 0.889, 0.933, 0.911, 0.911, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.867, 0.889, 0.867, 0.889, 0.911, 0.889, 0.889, 0.911, 0.911, 0.844, 0.889, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.911, 0.911, 0.889, 0.889, 0.844, 0.844, 0.911, 0.911, 0.867, 0.911, 0.889, 0.911, 0.911, 0.911, 0.889, 0.911, 0.867, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.867, 0.889, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.867, 0.911, 0.889, 0.889, 0.889, 0.889, 0.911, 0.867, 0.867, 0.911, 0.889, 0.867, 0.911, 0.867, 0.889, 0.911, 0.911, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.867, 0.889, 0.867, 0.911, 0.867, 0.889, 0.844, 0.867, 0.867, 0.911, 0.889, 0.889, 0.889, 0.889, 0.844, 0.889, 0.911, 0.867, 0.867, 0.911, 0.911, 0.911, 0.867, 0.911, 0.889, 0.889, 0.867, 0.889, 0.867, 0.889, 0.867, 0.867, 0.867, 0.867, 0.889, 0.889, 0.889, 0.889, 0.911, 0.844, 0.911, 0.889, 0.889, 0.889, 0.911, 0.911, 0.911, 0.911, 0.889, 0.867, 0.911, 0.867, 0.889, 0.911, 0.911, 0.889, 0.911, 0.889, 0.889, 0.889, 0.933, 0.911, 0.867, 0.822, 0.844, 0.844, 0.889, 0.844, 0.889, 0.889, 0.911, 0.889, 0.844, 0.889, 0.911, 0.889, 0.889, 0.889, 0.911, 0.867, 0.911, 0.911, 0.889, 0.867, 0.889, 0.867, 0.911, 0.911, 0.911, 0.889, 0.911, 0.867, 0.889, 0.867, 0.911, 0.889, 0.889, 0.867, 0.911, 0.911, 0.911, 0.867, 0.911, 0.867, 0.911, 0.911, 0.867, 0.844, 0.867, 0.911, 0.889, 0.867, 0.889, 0.889, 0.867, 0.889, 0.867, 0.911, 0.889, 0.889, 0.911, 0.933, 0.911, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.867, 0.867, 0.889, 0.889, 0.867, 0.867, 0.889, 0.867, 0.889, 0.889, 0.889, 0.867, 0.889, 0.889, 0.911, 0.889, 0.911, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.911, 0.844, 0.867, 0.911, 0.867, 0.889, 0.911, 0.867, 0.889, 0.911, 0.911, 0.889, 0.889, 0.889, 0.889, 0.867, 0.911, 0.911, 0.889, 0.889, 0.889, 0.844, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.889, 0.889, 0.889, 0.911, 0.911, 0.889, 0.889, 0.911, 0.889, 0.911, 0.867, 0.911, 0.911, 0.911, 0.911, 0.911, 0.867, 0.889, 0.889, 0.889, 0.867, 0.889, 0.911, 0.889, 0.911, 0.911, 0.889, 0.911, 0.889, 0.844, 0.911, 0.889, 0.889, 0.889, 0.911, 0.889, 0.911, 0.822, 0.889, 0.889, 0.911, 0.889, 0.889, 0.911, 0.889, 0.844, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.889, 0.911, 0.867]\n",
      "테스트셋 정확도추이: [0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.4, 0.6, 0.6, 0.6, 0.6, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.6, 0.6, 0.4, 0.6, 0.4, 0.4, 0.6, 0.4, 0.4, 0.6, 0.4, 0.6, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6, 0.6, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6, 0.6, 0.6, 0.8, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('훈련셋   오차추이:', [round(i, 3) for i in y_loss])\n",
    "print('테스트셋 오차추이:', [round(i, 3) for i in y_vloss])\n",
    "\n",
    "print()\n",
    "\n",
    "print('훈련셋   정확도추이:', [round(i, 3) for i in y_acc])\n",
    "print('테스트셋 정확도추이:', [round(i, 3) for i in y_vacc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 학습모델 성능평가 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 1997, 1998, 1999])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x축 설정\n",
    "# - epochs = 200에 의해서 딥러닝 학습모델이 200개 만들어저 진행됨에 따라\n",
    "#   학습오차, 학습정확도, 검증오차, 검증정확도가 모두 200개 값이 생성됨\n",
    "# - 그런데, callbacks = [cp, es] 파라미터 설정으로 \n",
    "#   개선된 베스트모델&과적합방지 학습모델에 대해서만 제한적으로  \n",
    "#   학습오차, 학습정확도, 검증오차, 검증정확도가 기록됨  \n",
    "#   [cp] 학습모델별 성능평가파일 생성시 \n",
    "#        이전파일보다 성능이 개선된 베스트모델을 저장옵션\n",
    "#   [es] 딥러닝 과적합방지용 조기종료 옵션 \n",
    "x_len = np.arange(len(y_acc))\n",
    "x_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝학습 훈련셋과 검증셋 오차 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859c9b0438>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 딥러닝학습 훈련셋과 검증셋 오차 추이\n",
    "plt.plot(x_len, y_loss, 'co--', label = '훈련셋 오차')\n",
    "plt.plot(x_len, y_vloss, 'mo--', label = '검증셋 오차')\n",
    "plt.title('딥러닝학습 훈련셋과 검증셋 오차 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('오차(loss)', size = 12)\n",
    "plt.legend()\n",
    "\n",
    "# 마커설정 옵션 'co--', 'mo--'\n",
    "# --> color = 'cyan', marker = 'o', linestyle = '--' 로 풀어서 코딩가능\n",
    "# --> color = 'magenta', marker = 'o', linestyle = '--' 로 풀어서 코딩가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝학습 훈련셋과 검증셋 정확도 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18593c5ab70>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 딥러닝학습 훈련셋과 검증셋 정확도 추이\n",
    "plt.plot(x_len, y_acc, 'co--', label = '훈련셋 정확도')\n",
    "plt.plot(x_len, y_vacc, 'mo--', label = '검증셋 정확도')\n",
    "plt.title('딥러닝학습 훈련셋과 검증셋 정확도 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('정확도(accuracy)', size = 12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련셋 딥러닝학습 오차과 정확도 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859ca7aba8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련셋 딥러닝학습 오차와 정확도 추이\n",
    "plt.plot(x_len, y_acc, 'bo--', label = '훈련셋 정확도')\n",
    "plt.plot(x_len, y_loss, 'ro--', label = '훈련셋 오차')\n",
    "plt.title('훈련셋 딥러닝학습 오차과 정확도 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('red: 오차 ----- blue: 정확도', size = 12)\n",
    "plt.legend()\n",
    "\n",
    "# 마커설정 옵션 'bo--', 'ro--'\n",
    "# --> color = 'blue', marker = 'o', linestyle = '--' 의미임\n",
    "# --> color = 'red', marker = 'o', linestyle = '--' 의미임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트셋 딥러닝학습 오차과 정확도 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859caea7f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋 딥러닝학습 오차과 정확도 추이\n",
    "plt.plot(x_len, y_vacc, 'bo--', label = '테스트셋 정확도')\n",
    "plt.plot(x_len, y_vloss, 'ro--', label = '테스트셋 오차')\n",
    "plt.title('테스트셋 딥러닝학습 오차과 정확도 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('red: 오차 ----- blue: 정확도', size = 12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련셋 정확도와 검증셋 오차 추이\n",
    "* 훈련셋의 정확도는 에포크(epoch)가 실행될 수록 좋아짐\n",
    "* 반면에 검증셋의 오차는 에포크가 실행될 수록 더이상 좋아지지 않음\n",
    "* 따라서 에포크(epoch) 회수를 통해 적정한 딥러닝 모델학습 지정이 필요함 \n",
    "* 학습이 진행될수록 훈련셋의 정확도는 올라가지만, \n",
    "<br>과적합으로 인해 검증셋의 학습결과는 점점 나빠지게 됨\n",
    "<img src = './images/train_validation.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859cb523c8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈렷셋 딥러닝학습 정확도 추이\n",
    "plt.plot(x_len, y_acc, color = 'blue', \n",
    "         marker = 'o', linestyle = '--', label = '훈련셋 정확도')\n",
    "plt.title('딥러닝학습 훈련셋 정확도 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('blue: 정확도', size = 12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859dba6cf8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증셋 딥러닝학습 오차 추이\n",
    "plt.plot(x_len, y_vloss, color = 'red', \n",
    "         marker = 'o', linestyle = '--', label = '검증셋 오차')\n",
    "plt.title('딥러닝학습 검증셋 오차 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('red: 오차', size = 12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1859dbfedd8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련셋 정확도와 검증셋 오차 추이\n",
    "plt.plot(x_len, y_acc, color = 'blue', marker = 'o', \n",
    "         linestyle = '--', label = '훈련셋 정확도')\n",
    "plt.plot(x_len, y_vloss, color = 'red', marker = 'o', \n",
    "         linestyle = '--', label = '검증셋 오차')\n",
    "plt.title('딥러닝학습 훈련셋 정확도와 검증셋 오차 추이')\n",
    "plt.xlabel('epoch', size = 12)\n",
    "plt.ylabel('red: 오차 ----- blue: 정확도', size = 12)\n",
    "plt.legend()\n",
    "\n",
    "# 마커설정 옵션: color = 'red', marker = 'o', linestyle = '--'\n",
    "# --> 'ro--'로 축약해서 설정가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련셋(train) 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 100us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14946425557136536, 0.9200000023841858]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련셋을 활용한 딥러닝모델의 성능평가 결과\n",
    "train_perf = md.evaluate(X_train_scaled, y_train_ohe)\n",
    "train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 모델성능 종합: [0.149, 0.92]\n",
      "훈련셋 모델성능       오차(loss): 0.149\n",
      "훈련셋 모델성능 정확도(accuracy): 0.920\n"
     ]
    }
   ],
   "source": [
    "print('훈련셋 모델성능 종합:', [round(i, 3) for i in train_perf])\n",
    "print(f'훈련셋 모델성능       오차(loss): {train_perf[0]:0.3f}')\n",
    "print(f'훈련셋 모델성능 정확도(accuracy): {train_perf[1]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>딥러닝 모델 예측하기</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한 스텝(윈도우)씩 실제 음표값을 투입해 음표를 예측\n",
    "* 실제 음표 4개를 입력해서 그 다음 음표 1개를 예측하는 과정을 반복하는 방법\n",
    "* 준비된 윈도우 길이만큼의 음표 4개 조합이 계속적으로 추가 입력되어 그 다음 음표를 예측하는 과정을 반복하게됨\n",
    "* 최초로 투입되는 4개 음표 이후에, 이어지는 전체 노래의 음표들을 모두 예측하지만,\n",
    "<br>각 음표 예측을 위해 실제 음표 4개 조합이 계속 추가 투입되어 활용됨\n",
    "<img src = './images/music_onestep.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표 예측 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음표시퀀스 출력결과 저장용 리스트객체 생성\n",
    "seq_out = ['g8', 'e8', 'e4', 'f8']\n",
    "# - 최초 투입되는 4개 음표는 미리 지정해 놓음\n",
    "# - 이 4개 음표 이후로 예측되는 음표들이 추가저장될 예정임\n",
    "\n",
    "# 최대 예측 음표개수 정의\n",
    "pred_count = 50\n",
    "# - 최초 투입되는 4개 음표 이후로 50개 정도의 음표를 예측할 수 있도록 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.21930145e-13, 0.00000000e+00, 1.73314207e-17, 1.02312027e-10,\n",
       "        3.99712823e-24, 3.34738239e-14, 1.43257286e-13, 5.12586064e-16,\n",
       "        9.94953215e-01, 5.04675694e-03, 4.35769856e-14, 0.00000000e+00],\n",
       "       [7.23221504e-13, 9.98334467e-01, 2.78844873e-05, 1.74295463e-12,\n",
       "        6.52133512e-17, 5.29939265e-13, 1.61388611e-12, 1.40056291e-26,\n",
       "        2.45241954e-05, 1.61324802e-03, 1.45483356e-11, 9.40051734e-15],\n",
       "       [4.38977222e-20, 0.00000000e+00, 0.00000000e+00, 1.02477499e-29,\n",
       "        0.00000000e+00, 3.34974570e-19, 9.04116084e-22, 9.99986053e-01,\n",
       "        7.05594640e-14, 5.81548102e-20, 1.39217536e-05, 0.00000000e+00],\n",
       "       [2.38411148e-11, 0.00000000e+00, 2.31751788e-11, 1.13504236e-14,\n",
       "        1.19477125e-29, 4.44804079e-13, 9.93684475e-12, 3.10457250e-11,\n",
       "        3.02034914e-01, 6.97965086e-01, 1.27726443e-12, 0.00000000e+00],\n",
       "       [2.29738384e-09, 1.32522677e-04, 3.03682216e-14, 2.49211553e-07,\n",
       "        1.04185298e-14, 1.51056290e-09, 2.38803555e-09, 8.33150501e-16,\n",
       "        1.41304702e-01, 8.54960322e-01, 8.33434667e-13, 3.60226771e-03],\n",
       "       [3.74896816e-17, 8.02488477e-24, 2.23129276e-35, 3.66238807e-25,\n",
       "        4.22161469e-23, 2.66753733e-18, 1.01392863e-18, 4.01343715e-12,\n",
       "        3.02586966e-04, 3.76957746e-16, 9.99304056e-01, 3.93374386e-04],\n",
       "       [2.32825369e-13, 1.87739158e-10, 2.04750654e-16, 2.82923516e-04,\n",
       "        3.64593961e-06, 8.37391137e-13, 2.93557496e-13, 2.69512619e-31,\n",
       "        2.23649200e-04, 9.97344963e-04, 1.76298212e-08, 9.98492360e-01],\n",
       "       [2.19998241e-14, 2.32614032e-11, 1.85958672e-15, 8.01882823e-04,\n",
       "        1.95811852e-03, 1.30917277e-13, 3.21986463e-14, 3.82164492e-35,\n",
       "        9.86216473e-05, 5.49024204e-04, 7.00850791e-08, 9.96592224e-01],\n",
       "       [1.01286025e-13, 8.13152496e-08, 4.64523073e-11, 4.59118273e-05,\n",
       "        9.87416863e-01, 7.93948837e-13, 2.12275577e-13, 4.25829940e-37,\n",
       "        8.42045760e-04, 1.02196131e-02, 6.74564144e-05, 1.40791701e-03],\n",
       "       [1.26925392e-21, 2.00007747e-23, 3.08441673e-33, 0.00000000e+00,\n",
       "        1.00499003e-28, 5.07498732e-21, 7.17059770e-21, 1.10634691e-33,\n",
       "        1.58419471e-14, 1.29972577e-05, 3.29732931e-13, 9.99987006e-01],\n",
       "       [5.43891375e-13, 0.00000000e+00, 5.83096460e-09, 5.08368236e-10,\n",
       "        1.22005038e-15, 2.12961661e-13, 1.54840591e-13, 1.29114827e-24,\n",
       "        1.67293437e-02, 9.83270586e-01, 1.10716645e-08, 9.66623111e-33],\n",
       "       [2.67698190e-11, 6.67037151e-04, 7.53933145e-03, 3.91029135e-14,\n",
       "        1.39469491e-12, 3.96609204e-11, 6.40952083e-11, 1.37313759e-27,\n",
       "        1.00349998e-02, 9.81755078e-01, 3.62540050e-06, 1.42355431e-12],\n",
       "       [5.62259519e-18, 2.62437852e-21, 1.58400621e-29, 4.47193411e-12,\n",
       "        3.13305126e-19, 1.62193161e-18, 1.52999578e-17, 2.03953678e-37,\n",
       "        3.29744694e-07, 9.99999642e-01, 2.40780402e-15, 8.75142936e-20],\n",
       "       [3.76266336e-12, 2.55571020e-12, 1.51611003e-03, 1.89547167e-23,\n",
       "        4.20378371e-10, 1.11179000e-11, 7.60249936e-12, 3.98064513e-26,\n",
       "        3.31796880e-04, 7.89541344e-04, 9.97362554e-01, 2.47497373e-15],\n",
       "       [4.37557268e-10, 5.52690498e-11, 1.09401640e-06, 1.88962696e-03,\n",
       "        3.69000167e-01, 1.65682923e-09, 3.87944232e-10, 3.58199974e-27,\n",
       "        5.31735957e-01, 6.93885013e-02, 2.77902894e-02, 1.94330889e-04],\n",
       "       [1.05757966e-13, 9.98065293e-01, 3.19522369e-05, 6.47038325e-21,\n",
       "        9.67281699e-10, 2.21523632e-13, 6.82995706e-13, 8.38427811e-32,\n",
       "        8.19368381e-07, 5.14077023e-04, 1.38773338e-03, 4.27417365e-14],\n",
       "       [1.30344408e-17, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.10207851e-38, 5.28103834e-17, 4.16863072e-19, 9.99996424e-01,\n",
       "        2.93155767e-06, 2.31714908e-12, 5.83376618e-07, 9.62404238e-28],\n",
       "       [2.38411148e-11, 0.00000000e+00, 2.31751788e-11, 1.13504236e-14,\n",
       "        1.19477125e-29, 4.44804079e-13, 9.93684475e-12, 3.10457250e-11,\n",
       "        3.02034914e-01, 6.97965086e-01, 1.27726443e-12, 0.00000000e+00],\n",
       "       [6.56165920e-12, 3.59888053e-09, 8.61659000e-19, 6.53572618e-09,\n",
       "        2.99707812e-16, 5.68141627e-12, 5.48487688e-12, 2.45489070e-18,\n",
       "        8.90208932e-04, 4.79538832e-03, 5.16905244e-16, 9.94314373e-01],\n",
       "       [2.74649221e-24, 2.20222912e-29, 0.00000000e+00, 1.73272584e-32,\n",
       "        2.82652247e-28, 1.73643089e-25, 4.39286409e-26, 1.06012219e-20,\n",
       "        3.50992013e-11, 6.77206243e-24, 1.03115678e-06, 9.99998927e-01],\n",
       "       [3.50636437e-14, 2.27021758e-07, 5.60053594e-18, 5.95568163e-05,\n",
       "        3.17211925e-05, 1.40044335e-13, 8.70728389e-14, 8.35347982e-38,\n",
       "        2.82636494e-04, 9.94220018e-01, 1.11980745e-08, 5.40585117e-03],\n",
       "       [5.58851807e-14, 2.13167979e-03, 3.20607505e-08, 1.99514578e-18,\n",
       "        4.31622283e-07, 1.48920589e-13, 5.40574313e-13, 1.82083027e-36,\n",
       "        4.56139514e-06, 9.97020423e-01, 8.42752401e-04, 6.84255473e-12],\n",
       "       [3.31837841e-15, 1.63008594e-14, 9.95920897e-01, 1.71999071e-29,\n",
       "        2.51013078e-13, 8.52188244e-15, 1.16447447e-14, 1.62884329e-32,\n",
       "        8.67844676e-08, 1.53382527e-04, 3.92555725e-03, 2.25063484e-23],\n",
       "       [7.21076692e-15, 7.65600446e-35, 2.09567372e-33, 0.00000000e+00,\n",
       "        3.89883962e-33, 2.18497468e-14, 1.58206283e-15, 2.20562220e-06,\n",
       "        9.99905467e-01, 7.97149914e-05, 1.25263641e-05, 2.66545629e-13],\n",
       "       [5.11049120e-11, 0.00000000e+00, 3.75707312e-11, 5.36519416e-08,\n",
       "        2.14686564e-22, 3.50448099e-12, 2.87111966e-11, 3.57680228e-14,\n",
       "        9.30274189e-01, 6.97258636e-02, 2.07709674e-12, 0.00000000e+00],\n",
       "       [4.50490223e-09, 2.97828490e-04, 4.46533505e-10, 2.32190018e-08,\n",
       "        1.43764124e-13, 3.89370358e-09, 5.44274581e-09, 3.00409532e-17,\n",
       "        7.43339598e-01, 2.56362170e-01, 2.63071520e-09, 4.11581993e-07],\n",
       "       [9.56640041e-13, 1.50322358e-17, 3.03316091e-29, 2.81633983e-14,\n",
       "        4.39927840e-19, 1.89120610e-13, 2.59563394e-13, 4.85416814e-17,\n",
       "        9.99539495e-01, 1.05897998e-04, 3.54576565e-04, 6.51882437e-11],\n",
       "       [1.17351249e-08, 9.44718431e-06, 5.19800342e-05, 1.76516754e-04,\n",
       "        9.78168828e-05, 1.36511007e-08, 1.72423942e-08, 2.32831829e-22,\n",
       "        5.04818797e-01, 4.80574757e-01, 1.42707089e-02, 1.45944909e-08],\n",
       "       [1.17351249e-08, 9.44718431e-06, 5.19800342e-05, 1.76516754e-04,\n",
       "        9.78168828e-05, 1.36511007e-08, 1.72423942e-08, 2.32831829e-22,\n",
       "        5.04818797e-01, 4.80574757e-01, 1.42707089e-02, 1.45944909e-08],\n",
       "       [3.96607781e-11, 3.40729181e-12, 9.78443881e-10, 9.83629644e-01,\n",
       "        4.72178857e-04, 7.25082910e-11, 2.85446354e-11, 1.21099640e-25,\n",
       "        1.51726957e-02, 7.14596070e-04, 9.06194327e-06, 1.84439148e-06],\n",
       "       [9.18108580e-13, 3.94389572e-13, 4.40557871e-24, 1.65667650e-32,\n",
       "        1.06877968e-22, 1.81170226e-12, 2.27788621e-12, 1.89072858e-17,\n",
       "        1.14732415e-04, 9.99794424e-01, 1.39604083e-06, 8.93390315e-05],\n",
       "       [1.92927809e-11, 3.82889964e-35, 8.28858232e-11, 1.04187042e-04,\n",
       "        7.55672701e-17, 3.29743394e-12, 6.49667820e-12, 6.12819083e-19,\n",
       "        1.18239895e-02, 9.88071799e-01, 3.64964829e-12, 3.38607681e-27],\n",
       "       [1.22131183e-09, 2.79584696e-04, 4.47656788e-07, 7.12360077e-07,\n",
       "        1.10885218e-10, 1.75053305e-09, 1.48826174e-09, 6.66455391e-22,\n",
       "        1.20242126e-01, 8.79463792e-01, 1.26043433e-08, 1.32970245e-05],\n",
       "       [1.51184941e-15, 2.49615660e-16, 3.83283357e-31, 1.10992222e-11,\n",
       "        1.65095802e-19, 4.35651015e-16, 2.37245606e-15, 5.03505185e-31,\n",
       "        2.09993785e-04, 9.99790013e-01, 8.46063725e-13, 2.42715434e-13],\n",
       "       [1.14142207e-09, 1.45605054e-06, 6.72315597e-04, 1.44069001e-09,\n",
       "        1.91873789e-03, 2.79770496e-09, 2.00838479e-09, 3.58457933e-26,\n",
       "        1.10241741e-01, 3.29901457e-01, 5.57264268e-01, 4.64828318e-08],\n",
       "       [1.14142207e-09, 1.45605054e-06, 6.72315597e-04, 1.44069001e-09,\n",
       "        1.91873789e-03, 2.79770496e-09, 2.00838479e-09, 3.58457933e-26,\n",
       "        1.10241741e-01, 3.29901457e-01, 5.57264268e-01, 4.64828318e-08],\n",
       "       [4.37557268e-10, 5.52690498e-11, 1.09401640e-06, 1.88962696e-03,\n",
       "        3.69000167e-01, 1.65682923e-09, 3.87944232e-10, 3.58199974e-27,\n",
       "        5.31735957e-01, 6.93885013e-02, 2.77902894e-02, 1.94330889e-04],\n",
       "       [6.55764834e-17, 9.61848128e-14, 1.37083505e-24, 0.00000000e+00,\n",
       "        6.61470676e-24, 1.36103424e-16, 3.86012158e-16, 5.20813924e-28,\n",
       "        6.13294970e-11, 1.16020301e-03, 3.36196515e-10, 9.98839796e-01],\n",
       "       [1.45725066e-12, 6.33398284e-38, 3.03778704e-11, 3.86855536e-05,\n",
       "        8.66757973e-14, 6.52582318e-13, 3.72193487e-13, 7.80440209e-24,\n",
       "        5.64307161e-03, 9.94318306e-01, 5.51599773e-11, 2.32345058e-22],\n",
       "       [2.41554931e-12, 4.22906876e-03, 9.81116295e-01, 3.69862231e-11,\n",
       "        6.59488653e-12, 3.64020879e-12, 5.60983291e-12, 3.47220794e-30,\n",
       "        1.65752383e-04, 1.44884642e-02, 4.30810758e-07, 1.10237569e-14],\n",
       "       [2.49126566e-12, 1.68595846e-26, 0.00000000e+00, 2.72993553e-21,\n",
       "        6.51417546e-26, 1.92196137e-11, 2.95484042e-13, 4.84375123e-05,\n",
       "        1.00383613e-06, 2.96186772e-06, 9.99947548e-01, 1.54789503e-27],\n",
       "       [3.21930145e-13, 0.00000000e+00, 1.73314207e-17, 1.02312027e-10,\n",
       "        3.99712823e-24, 3.34738239e-14, 1.43257286e-13, 5.12586064e-16,\n",
       "        9.94953215e-01, 5.04675694e-03, 4.35769856e-14, 0.00000000e+00],\n",
       "       [7.23221504e-13, 9.98334467e-01, 2.78844873e-05, 1.74295463e-12,\n",
       "        6.52133512e-17, 5.29939265e-13, 1.61388611e-12, 1.40056291e-26,\n",
       "        2.45241954e-05, 1.61324802e-03, 1.45483356e-11, 9.40051734e-15],\n",
       "       [4.38977222e-20, 0.00000000e+00, 0.00000000e+00, 1.02477499e-29,\n",
       "        0.00000000e+00, 3.34974570e-19, 9.04116084e-22, 9.99986053e-01,\n",
       "        7.05594640e-14, 5.81548102e-20, 1.39217536e-05, 0.00000000e+00],\n",
       "       [2.38411148e-11, 0.00000000e+00, 2.31751788e-11, 1.13504236e-14,\n",
       "        1.19477125e-29, 4.44804079e-13, 9.93684475e-12, 3.10457250e-11,\n",
       "        3.02034914e-01, 6.97965086e-01, 1.27726443e-12, 0.00000000e+00],\n",
       "       [6.56165920e-12, 3.59888053e-09, 8.61659000e-19, 6.53572618e-09,\n",
       "        2.99707812e-16, 5.68141627e-12, 5.48487688e-12, 2.45489070e-18,\n",
       "        8.90208932e-04, 4.79538832e-03, 5.16905244e-16, 9.94314373e-01],\n",
       "       [2.74649221e-24, 2.20222912e-29, 0.00000000e+00, 1.73272584e-32,\n",
       "        2.82652247e-28, 1.73643089e-25, 4.39286409e-26, 1.06012219e-20,\n",
       "        3.50992013e-11, 6.77206243e-24, 1.03115678e-06, 9.99998927e-01],\n",
       "       [3.50636437e-14, 2.27021758e-07, 5.60053594e-18, 5.95568163e-05,\n",
       "        3.17211925e-05, 1.40044335e-13, 8.70728389e-14, 8.35347982e-38,\n",
       "        2.82636494e-04, 9.94220018e-01, 1.11980745e-08, 5.40585117e-03],\n",
       "       [5.58851807e-14, 2.13167770e-03, 3.20606866e-08, 1.99514578e-18,\n",
       "        4.31621885e-07, 1.48920305e-13, 5.40574313e-13, 1.82083027e-36,\n",
       "        4.56139514e-06, 9.97020423e-01, 8.42751586e-04, 6.84251527e-12],\n",
       "       [3.31837841e-15, 1.63008594e-14, 9.95920897e-01, 1.71999071e-29,\n",
       "        2.51013078e-13, 8.52188244e-15, 1.16447447e-14, 1.62884329e-32,\n",
       "        8.67843113e-08, 1.53382527e-04, 3.92555539e-03, 2.25065187e-23]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음표 예측실시\n",
    "pred_out = md.predict(X_train_scaled)\n",
    "pred_out\n",
    "# - 4개 윈도우 길이에 해당하는 음표를 투입해 이후로 이어지는 음표를 반복예측실시함\n",
    "# - 4개 윈도우 길이 음표를 입력할 때마다 원핫인코딩방식으로 12개 음표 인덱스번호에 대한 확률값이 예측됨 \n",
    "# - 이 확률값 중에서 가장 수치가 높은 레이블이 1로 기록되어 음표 인덱스번호로 기록됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한 스텝(윈도우)씩 실제 음표값을 투입해 전체노래를 예측:\n",
      " ['g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'e8', 'f8', 'g8', 'g8', 'g4', 'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'f4', 'e8', 'e8', 'e8', 'e8', 'f8', 'f8', 'd8', 'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4']\n"
     ]
    }
   ],
   "source": [
    "for i in range(pred_count): \n",
    "    # - 앞서 입력한대로 50번정도 for구문을 통해 반복 예측실시함\n",
    "    \n",
    "    idx = np.argmax(pred_out[i])\n",
    "    # - 4개 음표 인덱스번호가 입력되어 12개 음표레이블에 대한 확률값이 나타나고\n",
    "    #   각 확률값 수치중에서 가장 숫자가 큰 위치를 argmax()로 확인해 정수레이블로 만듬 \n",
    "    \n",
    "    seq_out.append(idx2code[idx])\n",
    "    # - 가장 확률값 수치가 큰 위치를 이용해 음표인덱스2음표코드 사전을 이용해 음표로 변환시킴 \n",
    "    \n",
    "print('한 스텝(윈도우)씩 실제 음표값을 투입해 전체노래를 예측:\\n', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표 예측 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교용 리스트객체 생성\n",
    "seq_compare = []\n",
    "\n",
    "# for반복구문을 이용한 음표 하나하나 비교\n",
    "for i in range(len(seq)):\n",
    "    if seq[i] == seq_out[i]:\n",
    "        seq_compare.append('일치')\n",
    "    else:\n",
    "        seq_compare.append('불일치')\n",
    "\n",
    "seq_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원곡 음표</th>\n",
       "      <th>예측 음표</th>\n",
       "      <th>비교 결과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e4</td>\n",
       "      <td>e4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d4</td>\n",
       "      <td>d4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c8</td>\n",
       "      <td>c8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d8</td>\n",
       "      <td>e8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>g4</td>\n",
       "      <td>g4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>d4</td>\n",
       "      <td>d4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>c8</td>\n",
       "      <td>c8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>e4</td>\n",
       "      <td>e4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>f4</td>\n",
       "      <td>f4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>e8</td>\n",
       "      <td>f8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>g4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>e4</td>\n",
       "      <td>e4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>d4</td>\n",
       "      <td>d4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>c8</td>\n",
       "      <td>c8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>e4</td>\n",
       "      <td>e4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   원곡 음표 예측 음표 비교 결과\n",
       "0     g8    g8    일치\n",
       "1     e8    e8    일치\n",
       "2     e4    e4    일치\n",
       "3     f8    f8    일치\n",
       "4     d8    d8    일치\n",
       "5     d4    d4    일치\n",
       "6     c8    c8    일치\n",
       "7     d8    e8   불일치\n",
       "8     e8    e8    일치\n",
       "9     f8    f8    일치\n",
       "10    g8    g8    일치\n",
       "11    g8    g8    일치\n",
       "12    g4    g4    일치\n",
       "13    g8    g8    일치\n",
       "14    e8    e8    일치\n",
       "15    e8    e8    일치\n",
       "16    e8    e8    일치\n",
       "17    f8    f8    일치\n",
       "18    d8    d8    일치\n",
       "19    d4    d4    일치\n",
       "20    c8    c8    일치\n",
       "21    e8    e8    일치\n",
       "22    g8    g8    일치\n",
       "23    g8    g8    일치\n",
       "24    e8    e8    일치\n",
       "25    e8    e8    일치\n",
       "26    e4    e4    일치\n",
       "27    d8    d8    일치\n",
       "28    d8    d8    일치\n",
       "29    d8    d8    일치\n",
       "30    d8    d8    일치\n",
       "31    d8    d8    일치\n",
       "32    e8    d8   불일치\n",
       "33    f4    f4    일치\n",
       "34    e8    e8    일치\n",
       "35    e8    e8    일치\n",
       "36    e8    e8    일치\n",
       "37    e8    e8    일치\n",
       "38    e8    f8   불일치\n",
       "39    f8    f8    일치\n",
       "40    g4    d8   불일치\n",
       "41    g8    g8    일치\n",
       "42    e8    e8    일치\n",
       "43    e4    e4    일치\n",
       "44    f8    f8    일치\n",
       "45    d8    d8    일치\n",
       "46    d4    d4    일치\n",
       "47    c8    c8    일치\n",
       "48    e8    e8    일치\n",
       "49    g8    g8    일치\n",
       "50    g8    g8    일치\n",
       "51    e8    e8    일치\n",
       "52    e8    e8    일치\n",
       "53    e4    e4    일치"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교테이블 생성\n",
    "result = pd.DataFrame({'원곡 음표': seq,\n",
    "                       '예측 음표': seq_out, \n",
    "                       '비교 결과': seq_compare})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['일치', '불일치'])\n",
      "dict_values([50, 4])\n",
      "dict_items([('일치', 50), ('불일치', 4)])\n",
      "\n",
      "레이블: 일치, 빈도수: 50, 비율: 0.926\n",
      "레이블: 불일치, 빈도수: 4, 비율: 0.074\n"
     ]
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교결과\n",
    "result_labelcount = {k: v for k, v in zip(result['비교 결과'].unique(), result['비교 결과'].value_counts())}\n",
    "print(type(result_labelcount))\n",
    "print(result_labelcount.keys())\n",
    "print(result_labelcount.values())\n",
    "print(result_labelcount.items())\n",
    "print()\n",
    "\n",
    "for k, v in result_labelcount.items(): \n",
    "    pct = v / sum(result_labelcount.values()) \n",
    "    print(f\"레이블: {k:}, 빈도수: {v}, 비율: {pct:0.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최초 4개 음표를 투입해 나머지 노래전체를 예측\n",
    "* 최초 음표 4개만을 입력해서 그 다음 음표들을 계속 예측해 나가는 방법\n",
    "* 실제 윈도우 길이에 해당하는 음표 4개가 계속 추가입력되는 것이 아니라, \n",
    "<br>예측된 음표값이 입력값으로 활용되어 예측이 이어지는 구조임\n",
    "* 예측된 음표값이 틀리게 되면 이후 음정과 박자가 이상하게 표시되는 예측오류 누적가능성이 큼\n",
    "<img src = './images/music_fullstep.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표 예측 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음표시퀀스 출력결과 저장용 리스트객체 생성\n",
    "seq_out = ['g8', 'e8', 'e4', 'f8']\n",
    "# - 최초 투입되는 4개 음표는 미리 지정해 놓음\n",
    "# - 이 4개 음표 이후로 예측되는 음표들이 추가저장될 예정임\n",
    "\n",
    "# 최대 예측 음표개수 정의\n",
    "pred_count = 50\n",
    "# - 최초 투입되는 4개 음표 이후로 50개 정도의 음표를 예측할 수 있도록 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 9, 2, 10]\n",
      "[1.         0.81818182 0.18181818 0.90909091]\n"
     ]
    }
   ],
   "source": [
    "# 최초 투입되는 4개음표를 음표 인덱스번호로 변환해서 투입함\n",
    "seq_in = [code2idx[i] for i in seq_out]\n",
    "print(seq_in)\n",
    "\n",
    "seq_in_scaled = np.array(seq_in) / float(np.max(X_train))\n",
    "print(seq_in_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 4개 음표를 투입해 나머지 노래전체를 예측:\n",
      " ['g8', 'e8', 'e4', 'f8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8']\n"
     ]
    }
   ],
   "source": [
    "# 음표 예측실시\n",
    "for i in range(pred_count):\n",
    "    # - 앞서 입력한대로 50번정도 for구문을 통해 반복 예측실시함\n",
    "    \n",
    "    sample_in = np.reshape(seq_in_scaled, (1, 4))\n",
    "    # - 4개 음표를 입력피처셋으로 만들기 위한 넘파이 배열 재구조화\n",
    "    \n",
    "    pred_out = md.predict(sample_in)\n",
    "    # - 4개 윈도우 길이에 해당하는 음표를 투입해 이후로 이어지는 음표를 반복예측실시함\n",
    "    # - 4개 윈도우 길이 음표를 입력할 때마다 원핫인코딩방식으로 12개 음표 인덱스번호에 대한 확률값이 예측됨 \n",
    "    # - 이 확률값 중에서 가장 수치가 높은 레이블이 1로 기록되어 음표 인덱스번호로 기록됨\n",
    "\n",
    "    idx = np.argmax(pred_out)\n",
    "    # - 4개 음표 인덱스번호가 입력되어 12개 음표레이블에 대한 확률값이 나타나고\n",
    "    #   각 확률값 수치중에서 가장 숫자가 큰 위치를 argmax()로 확인해 정수레이블로 만듬 \n",
    "\n",
    "    seq_out.append(idx2code[idx])\n",
    "    # - 가장 확률값 수치가 큰 위치를 이용해 음표인덱스2음표코드 사전을 이용해 음표로 변환시킴\n",
    "    \n",
    "    seq_in.append(idx / float(np.max(X_train)))\n",
    "    # - 예측한 정수 레이블을 스케일링실시해 다시 음표예측용 입력윈도우에 추가반영함\n",
    "\n",
    "    seq_in.pop(0)\n",
    "    # - 바로 위 코드에서 새로 예측된 음표예측용 정수 레이블이 추가되었으므로 \n",
    "    #   기존에 있던 음표예측욕 정수 레이블을 한개 제거시킴\n",
    "    \n",
    "print('최초 4개 음표를 투입해 나머지 노래전체를 예측:\\n', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 음표 예측 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치',\n",
       " '불일치']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교용 리스트객체 생성\n",
    "seq_compare = []\n",
    "\n",
    "# for반복구문을 이용한 음표 하나하나 비교\n",
    "for i in range(len(seq)):\n",
    "    if seq[i] == seq_out[i]:\n",
    "        seq_compare.append('일치')\n",
    "    else:\n",
    "        seq_compare.append('불일치')\n",
    "\n",
    "seq_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원곡 음표</th>\n",
       "      <th>예측 음표</th>\n",
       "      <th>비교 결과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g8</td>\n",
       "      <td>g8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8</td>\n",
       "      <td>e8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e4</td>\n",
       "      <td>e4</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8</td>\n",
       "      <td>f8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>g4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>f8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>d4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>c8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>e4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>f4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>f8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>g4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>e4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>f8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>d8</td>\n",
       "      <td>d8</td>\n",
       "      <td>일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>d4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>c8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>g8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>e8</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>e4</td>\n",
       "      <td>d8</td>\n",
       "      <td>불일치</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   원곡 음표 예측 음표 비교 결과\n",
       "0     g8    g8    일치\n",
       "1     e8    e8    일치\n",
       "2     e4    e4    일치\n",
       "3     f8    f8    일치\n",
       "4     d8    d8    일치\n",
       "5     d4    d8   불일치\n",
       "6     c8    d8   불일치\n",
       "7     d8    d8    일치\n",
       "8     e8    d8   불일치\n",
       "9     f8    d8   불일치\n",
       "10    g8    d8   불일치\n",
       "11    g8    d8   불일치\n",
       "12    g4    d8   불일치\n",
       "13    g8    d8   불일치\n",
       "14    e8    d8   불일치\n",
       "15    e8    d8   불일치\n",
       "16    e8    d8   불일치\n",
       "17    f8    d8   불일치\n",
       "18    d8    d8    일치\n",
       "19    d4    d8   불일치\n",
       "20    c8    d8   불일치\n",
       "21    e8    d8   불일치\n",
       "22    g8    d8   불일치\n",
       "23    g8    d8   불일치\n",
       "24    e8    d8   불일치\n",
       "25    e8    d8   불일치\n",
       "26    e4    d8   불일치\n",
       "27    d8    d8    일치\n",
       "28    d8    d8    일치\n",
       "29    d8    d8    일치\n",
       "30    d8    d8    일치\n",
       "31    d8    d8    일치\n",
       "32    e8    d8   불일치\n",
       "33    f4    d8   불일치\n",
       "34    e8    d8   불일치\n",
       "35    e8    d8   불일치\n",
       "36    e8    d8   불일치\n",
       "37    e8    d8   불일치\n",
       "38    e8    d8   불일치\n",
       "39    f8    d8   불일치\n",
       "40    g4    d8   불일치\n",
       "41    g8    d8   불일치\n",
       "42    e8    d8   불일치\n",
       "43    e4    d8   불일치\n",
       "44    f8    d8   불일치\n",
       "45    d8    d8    일치\n",
       "46    d4    d8   불일치\n",
       "47    c8    d8   불일치\n",
       "48    e8    d8   불일치\n",
       "49    g8    d8   불일치\n",
       "50    g8    d8   불일치\n",
       "51    e8    d8   불일치\n",
       "52    e8    d8   불일치\n",
       "53    e4    d8   불일치"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교테이블 생성\n",
    "result = pd.DataFrame({'원곡 음표': seq,\n",
    "                       '예측 음표': seq_out, \n",
    "                       '비교 결과': seq_compare})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['일치', '불일치'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['비교 결과'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "불일치    41\n",
       "일치     13\n",
       "Name: 비교 결과, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['비교 결과'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['일치', '불일치'])\n",
      "dict_values([13, 41])\n",
      "dict_items([('일치', 13), ('불일치', 41)])\n",
      "\n",
      "레이블: 일치, 빈도수: 13, 비율: 0.241\n",
      "레이블: 불일치, 빈도수: 41, 비율: 0.759\n"
     ]
    }
   ],
   "source": [
    "# 실제 노래음표와 예측음표간 비교결과\n",
    "result_labelcount = {k: v for k, v in zip(result['비교 결과'].unique(), result['비교 결과'].value_counts(ascending = True))}\n",
    "print(type(result_labelcount))\n",
    "print(result_labelcount.keys())\n",
    "print(result_labelcount.values())\n",
    "print(result_labelcount.items())\n",
    "print()\n",
    "\n",
    "for k, v in result_labelcount.items(): \n",
    "    pct = v / sum(result_labelcount.values()) \n",
    "    print(f\"레이블: {k:}, 빈도수: {v}, 비율: {pct:0.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
